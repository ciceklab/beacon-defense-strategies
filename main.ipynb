{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device to cpu or cuda\n",
    "device = torch.device('cpu')\n",
    "\n",
    "if(torch.cuda.is_available()):\n",
    "    device = torch.device('cuda:4')\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    print(\"Device set to : cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Arguments\n",
    "parser = argparse.ArgumentParser(description='Actor Critic')\n",
    "\n",
    "parser.add_argument('--data', default=\"/mnt/kerem/CEU\", type=str, help='Dataset Path')\n",
    "parser.add_argument('--epochs', default=64, type=int, metavar='N', help='Number of epochs for training agent.')\n",
    "parser.add_argument('--episodes', default=10000, type=int, metavar='N', help='Number of episodes for training agent.')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.005, type=float, metavar='LR', help='initial learning rate', dest='lr')\n",
    "parser.add_argument('--wd', default=0.0001, type=float, help='Weight decay for training optimizer')\n",
    "parser.add_argument('--seed', default=3, type=int, help='Seed for reproducibility')\n",
    "parser.add_argument('--model-name', default=\"PPO\", type=str, help='Model name for saving model.')\n",
    "parser.add_argument('--gamma', default=0.99, type=float, metavar='N', help='The discount factor as mentioned in the previous section')\n",
    "parser.add_argument('--val_freq', default=50, type=int, metavar='N', help='Validation frequencies')\n",
    "\n",
    "# Model\n",
    "parser.add_argument(\"--latent1\", default=256, required=False, help=\"Latent Space Size for first layer of network.\")\n",
    "parser.add_argument(\"--latent2\", default=256, required=False, help=\"Latent Space Size for second layer of network.\")\n",
    "\n",
    "# Env Properties\n",
    "parser.add_argument('--control_size', default=20, type=int, help='Beacon and Attacker Control group size')\n",
    "parser.add_argument('--gene_size', default=100, type=int, help='States gene size')\n",
    "parser.add_argument('--beacon_size', default=60, type=int, help='Beacon population size')\n",
    "parser.add_argument('--victim_prob', default=0.8, type=float, help='Victim inside beacon or not!')\n",
    "parser.add_argument('--pop_reset_freq', default=10, type=int, help='Reset Population Frequency (Epochs)')\n",
    "parser.add_argument('--max_queries', default=10, type=int, help='Maximum queries per episode')\n",
    "\n",
    "\n",
    "parser.add_argument(\"--state_dim\", default=(4,), required=False, help=\"State Dimension\")\n",
    "parser.add_argument(\"--n-actions\", default=1, required=False, help=\"Actions Count for each state\")\n",
    "\n",
    "\n",
    "# utils\n",
    "parser.add_argument('--resume', default=\"\", type=str, metavar='PATH', help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--save-dir', default='./results', type=str, metavar='PATH', help='path to cache (default: none)')\n",
    "\n",
    "# args = parser.parse_args()  # running in command line\n",
    "args = parser.parse_args('')  # running in ipynb\n",
    "\n",
    "# set command line arguments here when running in ipynb\n",
    "if args.save_dir == '':\n",
    "    args.save_dir = \"./\"\n",
    "\n",
    "args.results_dir = args.save_dir\n",
    "\n",
    "if not os.path.exists(args.results_dir):\n",
    "      os.makedirs(args.results_dir)\n",
    "\n",
    "args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â CEU Beacon - it contains 164 people in total which we will divide into groups to experiment\n",
    "beacon = pd.read_csv(os.path.join(args.data, \"Beacon_164.txt\"), index_col=0, delim_whitespace=True)\n",
    "# Reference genome, i.e. the genome that has no SNPs, all major allele pairs for each position\n",
    "reference = pickle.load(open(os.path.join(args.data, \"reference.pickle\"),\"rb\"))\n",
    "# Binary representation of the beacon; 0: no SNP (i.e. no mutation) 1: SNP (i.e. mutation)\n",
    "binary = np.logical_and(beacon.values != reference, beacon.values != \"NN\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table that contains MAF (minor allele frequency) values for each position. \n",
    "maf = pd.read_csv(os.path.join(args.data, \"MAF.txt\"), index_col=0, delim_whitespace=True)\n",
    "maf.rename(columns = {'referenceAllele':'major', 'referenceAlleleFrequency':'major_freq', \n",
    "                      'otherAllele':'minor', 'otherAlleleFrequency':'minor_freq'}, inplace = True)\n",
    "maf[\"maf\"] = np.round(maf[\"maf\"].values, 3)\n",
    "# Same variable with sorted maf values\n",
    "sorted_maf = maf.sort_values(by='maf')\n",
    "# Extracting column to an array for future use\n",
    "maf_values = maf[\"maf\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4029840, 164), (4029840, 1), (4029840, 164), (4029840,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beacon.shape, reference.shape, binary.shape, maf_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_continuous_action_space = True                \n",
    "\n",
    "action_std = 0.4                    # starting std for action distribution (Multivariate Normal)\n",
    "action_std_decay_rate = 0.05        # linearly decay action_std (action_std = action_std - action_std_decay_rate)\n",
    "min_action_std = 0.1                # minimum action_std (stop decay after action_std <= min_action_std)\n",
    "action_std_decay_freq = int(2.5e5)\n",
    "\n",
    "################ PPO hyperparameters ################\n",
    "K_epochs = 64           # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003       # learning rate for actor network\n",
    "lr_critic = 0.001       # learning rate for critic network\n",
    "\n",
    "random_seed = 0         # set random seed if required (0 = no random seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to : NVIDIA GeForce RTX 2080 Ti\n",
      "Victim is inside the Beacon!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sobhan/Beacons/environment.py:106: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1705951428005/work/torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  self.attacker_state = torch.tensor([self.victim, self.mafs, [0]*len(self.victim)], dtype=torch.float32).transpose(0, 1)\n",
      "/mnt/sobhan/cpi/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "Started training at (GMT) :  2024-04-14 13:26:41\n",
      "============================================================================================\n",
      "current logging run number for  :  2\n",
      "logging at : ./results/PPO__log_2.csv\n",
      "save checkpoint path : ./results/weights/PPO_2.pth\n",
      "Reseting the Populations\n",
      "Victim is inside the Beacon!\n",
      "Episode:  0\n",
      "lrt:  tensor(0.)\n",
      "lrt:  tensor(0.)\n",
      "lrt:  tensor(-9.5395e+15)\n",
      "lrt:  tensor(-2.3519e+18)\n",
      "lrt:  tensor(-2.3519e+18)\n",
      "lrt:  tensor(-2.3519e+18)\n",
      "lrt:  tensor(-2.3519e+18)\n",
      "lrt:  tensor(-2.3519e+18)\n",
      "lrt:  tensor(-2.3519e+18)\n",
      "lrt:  tensor(-2.3519e+18)\n",
      "Episode:  1\n",
      "lrt:  tensor(0.)\n",
      "lrt:  tensor(-60036872.)\n",
      "lrt:  tensor(-60051392.)\n",
      "lrt:  tensor(-61072748.)\n",
      "lrt:  tensor(-47357208.)\n",
      "lrt:  tensor(-47357208.)\n",
      "lrt:  tensor(-47357208.)\n",
      "lrt:  tensor(-8.0223e+10)\n",
      "lrt:  tensor(-8.0223e+10)\n",
      "lrt:  tensor(-8.0226e+10)\n",
      "Episode:  2\n",
      "lrt:  tensor(-6.2540e+09)\n",
      "lrt:  tensor(-6.2541e+09)\n",
      "lrt:  tensor(-6.2543e+09)\n",
      "lrt:  tensor(-6.2558e+09)\n",
      "lrt:  tensor(-6.2578e+09)\n",
      "lrt:  tensor(-6.2578e+09)\n",
      "lrt:  tensor(-6.2596e+09)\n",
      "lrt:  tensor(-6.2596e+09)\n",
      "lrt:  tensor(-6.2599e+09)\n",
      "lrt:  tensor(-6.2599e+09)\n",
      "Episode:  3\n",
      "lrt:  tensor(-3.4381e+16)\n",
      "lrt:  tensor(-3.4381e+16)\n",
      "lrt:  tensor(-3.4381e+16)\n",
      "lrt:  tensor(-5.5301e+16)\n",
      "lrt:  tensor(-5.5301e+16)\n",
      "lrt:  tensor(-5.5301e+16)\n",
      "lrt:  tensor(-5.5301e+16)\n",
      "lrt:  tensor(-5.5301e+16)\n",
      "lrt:  tensor(-5.5301e+16)\n",
      "lrt:  tensor(-5.5301e+16)\n",
      "Episode:  4\n",
      "lrt:  tensor(-3555.9290)\n",
      "lrt:  tensor(-5.6383e+10)\n",
      "lrt:  tensor(-5.6383e+10)\n",
      "lrt:  tensor(-1.0375e+34)\n",
      "lrt:  tensor(-1.0375e+34)\n",
      "lrt:  tensor(-1.0375e+34)\n",
      "lrt:  tensor(-1.0375e+34)\n",
      "lrt:  tensor(-1.0375e+34)\n",
      "lrt:  tensor(-1.0375e+34)\n",
      "lrt:  tensor(-1.0375e+34)\n",
      "Episode:  5\n",
      "lrt:  tensor(-2632531.7500)\n",
      "lrt:  tensor(-2.6315e+11)\n",
      "lrt:  tensor(-5.2061e+20)\n",
      "lrt:  tensor(-5.2061e+20)\n",
      "lrt:  tensor(-5.2061e+20)\n",
      "lrt:  tensor(-1.5543e+29)\n",
      "lrt:  tensor(-1.5543e+29)\n",
      "lrt:  tensor(-1.5543e+29)\n",
      "lrt:  tensor(-1.5544e+29)\n",
      "lrt:  tensor(-1.5544e+29)\n",
      "updating the agent\n",
      "Episode : 5 \t\t Timestep : 60 \t\t Average Reward : tensor([12.8171])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_2.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:06\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode:  6\n",
      "lrt:  tensor(-26.7410)\n",
      "lrt:  tensor(-5.7170e+20)\n",
      "lrt:  tensor(-5.7170e+20)\n",
      "lrt:  tensor(-5.7173e+20)\n",
      "lrt:  tensor(-5.7173e+20)\n",
      "lrt:  tensor(-5.7173e+20)\n",
      "lrt:  tensor(-8.6241e+20)\n",
      "lrt:  tensor(-8.6241e+20)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     train(args, env, ppo_agent)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# initialize a PPO agent\u001b[39;00m\n\u001b[1;32m     13\u001b[0m ppo_agent \u001b[38;5;241m=\u001b[39m PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mppo_agent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/sobhan/Beacons/engine.py:72\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args, env, ppo_agent)\u001b[0m\n\u001b[1;32m     70\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(state)\n\u001b[1;32m     71\u001b[0m action \u001b[38;5;241m=\u001b[39m ppo_agent\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[0;32m---> 72\u001b[0m state, reward, done, rewards \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# saving reward and is_terminals\u001b[39;00m\n\u001b[1;32m     75\u001b[0m ppo_agent\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "File \u001b[0;32m/mnt/sobhan/Beacons/environment.py:88\u001b[0m, in \u001b[0;36mBeaconEnv.step\u001b[0;34m(self, beacon_action)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Calculate the lrt for individuals in the beacon and find the min \u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maltered_probs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m beacon_action\n\u001b[0;32m---> 88\u001b[0m preward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calc_beacon_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     89\u001b[0m ureward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maltered_probs\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlrt: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calc_beacon_reward())\n",
      "File \u001b[0;32m/mnt/sobhan/Beacons/environment.py:123\u001b[0m, in \u001b[0;36mBeaconEnv._calc_beacon_reward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m lrt_values \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, individual \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeacon_state):\n\u001b[0;32m--> 123\u001b[0m     lrt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_lrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeacon_state\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     lrt_values\u001b[38;5;241m.\u001b[39mappend(lrt)\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m# print(\"lrt_values\", lrt_values)\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/sobhan/Beacons/environment.py:179\u001b[0m, in \u001b[0;36mBeaconEnv.calculate_lrt\u001b[0;34m(self, ind, error)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Genome == 1\u001b[39;00m\n\u001b[1;32m    178\u001b[0m log1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(DN_i \u001b[38;5;241m/\u001b[39m (error \u001b[38;5;241m*\u001b[39m DN_i_1))\n\u001b[0;32m--> 179\u001b[0m log2 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDN_i_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDN_i\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mDN_i\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDN_i_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Genome == 0\u001b[39;00m\n\u001b[1;32m    182\u001b[0m log3 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(DN_i \u001b[38;5;241m/\u001b[39m ((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m error) \u001b[38;5;241m*\u001b[39m DN_i_1))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from environment import BeaconEnv\n",
    "from ppo import PPO\n",
    "from engine import train\n",
    "\n",
    "def main():\n",
    "    env = BeaconEnv(args, beacon, maf_values, binary)\n",
    "    state_dim = args.beacon_size * args.gene_size * 4\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "    # initialize a PPO agent\n",
    "    ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "\n",
    "    train(args, env, ppo_agent)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
