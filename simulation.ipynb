{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproducibility(seed: int):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "reproducibility(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to : NVIDIA TITAN RTX\n"
     ]
    }
   ],
   "source": [
    "# set device to cpu or cuda\n",
    "device = torch.device('cpu')\n",
    "\n",
    "if(torch.cuda.is_available()):\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    print(\"Device set to : cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def args_create():\n",
    "    # @title Arguments\n",
    "    parser = argparse.ArgumentParser(description='Actor Critic')\n",
    "\n",
    "    parser.add_argument('--data', default=\"/mnt/kerem/CEU\", type=str, help='Dataset Path')\n",
    "    parser.add_argument('--epochs', default=64, type=int, metavar='N', help='Number of epochs for training agent.')\n",
    "    parser.add_argument('--episodes', default=9, type=int, metavar='N', help='Number of episodes for training agent.')\n",
    "    parser.add_argument('--lr', '--learning-rate', default=0.005, type=float, metavar='LR', help='initial learning rate', dest='lr')\n",
    "    parser.add_argument('--wd', default=0.0001, type=float, help='Weight decay for training optimizer')\n",
    "    parser.add_argument('--seed', default=3, type=int, help='Seed for reproducibility')\n",
    "    parser.add_argument('--model-name', default=\"PPO\", type=str, help='Model name for saving model.')\n",
    "    parser.add_argument('--gamma', default=0.99, type=float, metavar='N', help='The discount factor as mentioned in the previous section')\n",
    "\n",
    "    # Model\n",
    "    parser.add_argument(\"--latent1\", default=256, required=False, help=\"Latent Space Size for first layer of network.\")\n",
    "    parser.add_argument(\"--latent2\", default=256, required=False, help=\"Latent Space Size for second layer of network.\")\n",
    "\n",
    "    # Env Properties\n",
    "    parser.add_argument('--a_control_size', default=50, type=int, help='Attack Control group size')\n",
    "    parser.add_argument('--b_control_size', default=50, type=int, help='Beacon Control group size')\n",
    "    parser.add_argument('--gene_size', default=100, type=int, help='States gene size')\n",
    "    parser.add_argument('--beacon_size', default=10, type=int, help='Beacon population size')\n",
    "    parser.add_argument('--victim_prob', default=1, type=float, help='Victim inside beacon or not!')\n",
    "    parser.add_argument('--max_queries', default=5, type=int, help='Maximum queries per episode')\n",
    "\n",
    "\n",
    "    parser.add_argument('--attacker_type', default=\"agent\", choices=[\"random\", \"optimal\", \"agent\"], type=str, help='Type of the attacker')\n",
    "    parser.add_argument('--beacon_type', default=\"truth\", choices=[\"random\", \"agent\", \"truth\"], type=str, help='Type of the beacon')\n",
    "\n",
    "\n",
    "    parser.add_argument('--pop_reset_freq', default=100000000, type=int, help='Reset Population Frequency (Epochs)')\n",
    "    parser.add_argument('--plot-freq', default=1, type=int, metavar='N', help='Plot Frequencies')\n",
    "    parser.add_argument('--val-freq', default=20, type=int, metavar='N', help='Validation frequencies')\n",
    "    parser.add_argument('--control-lrts', default=None, type=str, help='Control groups LRTS path')\n",
    "\n",
    "    # utils\n",
    "    parser.add_argument('--resume', default=\"\", type=str, metavar='PATH', help='path to latest checkpoint (default: none)')\n",
    "    parser.add_argument('--results-dir', default='./results/simulation', type=str, metavar='PATH', help='path to cache (default: none)')\n",
    "\n",
    "    # args = parser.parse_args()  # running in command line\n",
    "    args = parser.parse_args('')  # running in ipynb\n",
    "\n",
    "    args.results_dir = os.path.join(args.results_dir, \"run\"+str(len(os.listdir(args.results_dir))))\n",
    "    os.makedirs(args.results_dir)\n",
    "    os.makedirs(args.results_dir+\"/logs\")\n",
    "    os.makedirs(args.results_dir+\"/rewards\")\n",
    "    os.makedirs(args.results_dir+\"/indrewards\")\n",
    "    os.makedirs(args.results_dir+\"/actions\")\n",
    "    os.makedirs(args.results_dir+\"/pvalues\")\n",
    "\n",
    "    args.device = device\n",
    "\n",
    "    print(args)\n",
    "    return args\n",
    "\n",
    "# args = args_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CEU Beacon - it contains 164 people in total which we will divide into groups to experiment\n",
    "beacon = pd.read_csv(os.path.join(\"/mnt/kerem/CEU\", \"Beacon_164.txt\"), index_col=0, delim_whitespace=True)\n",
    "# Reference genome, i.e. the genome that has no SNPs, all major allele pairs for each position\n",
    "reference = pickle.load(open(os.path.join(\"/mnt/kerem/CEU\", \"reference.pickle\"),\"rb\"))\n",
    "# Binary representation of the beacon; 0: no SNP (i.e. no mutation) 1: SNP (i.e. mutation)\n",
    "binary = np.logical_and(beacon.values != reference, beacon.values != \"NN\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164, 4029840)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Table that contains MAF (minor allele frequency) values for each position. \n",
    "maf = pd.read_csv(os.path.join(\"/mnt/kerem/CEU\", \"MAF.txt\"), index_col=0, delim_whitespace=True)\n",
    "maf.rename(columns = {'referenceAllele':'major', 'referenceAlleleFrequency':'major_freq', \n",
    "                      'otherAllele':'minor', 'otherAlleleFrequency':'minor_freq'}, inplace = True)\n",
    "maf[\"maf\"] = np.round(maf[\"maf\"].values, 3)\n",
    "# Same variable with sorted maf values\n",
    "sorted_maf = maf.sort_values(by='maf')\n",
    "# Extracting column to an array for future use\n",
    "maf_values = maf[\"maf\"].values\n",
    "\n",
    "binary = binary.T\n",
    "binary.shape #(164, 4029840)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4029840, 164), (4029840, 1), (164, 4029840), (4029840,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beacon.shape, reference.shape, binary.shape, maf_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Namespace(data='/mnt/kerem/CEU', epochs=64, episodes=9, lr=0.005, wd=0.0001, seed=3, model_name='PPO', gamma=0.99, latent1=256, latent2=256, a_control_size=50, b_control_size=50, gene_size=100, beacon_size=10, victim_prob=1, max_queries=5, attacker_type='agent', beacon_type='truth', pop_reset_freq=100000000, plot_freq=1, val_freq=20, control_lrts=None, resume='', results_dir='./results/simulation/run4', device=device(type='cuda', index=0))\n",
      "./results/simulation/run4/logs/beacon_log.csv\n",
      "./results/simulation/run4/logs/beacon_control_log.csv\n",
      "./results/simulation/run4/logs/attacker_log.csv\n",
      "./results/simulation/run4/logs/attacker_control_log.csv\n",
      "==================================⬇️⬇️⬇️⬇️Episode: 1⬇️⬇️⬇️⬇️==============================\n",
      "-------------------------\n",
      "Attacker Action:  7\n",
      "Attacker Action:  0\n",
      "⛔⛔⛔ Attacker Identified the VICTIM ⛔⛔⛔\n",
      "Victim: 0 \t Current Episode Reward : 39\n",
      "==================================⬇️⬇️⬇️⬇️Episode: 2⬇️⬇️⬇️⬇️==============================\n",
      "-------------------------\n",
      "Attacker Action:  0\n",
      "Attacker Action:  3\n",
      "⛔⛔⛔ Attacker Identified the VICTIM ⛔⛔⛔\n",
      "Victim: 1 \t Current Episode Reward : 39\n",
      "==================================⬇️⬇️⬇️⬇️Episode: 3⬇️⬇️⬇️⬇️==============================\n",
      "-------------------------\n",
      "Attacker Action:  0\n",
      "⛔⛔⛔ Attacker Identified the VICTIM ⛔⛔⛔\n",
      "Victim: 2 \t Current Episode Reward : 39\n",
      "==================================⬇️⬇️⬇️⬇️Episode: 4⬇️⬇️⬇️⬇️==============================\n",
      "-------------------------\n",
      "Attacker Action:  2\n",
      "Attacker Action:  1\n",
      "⛔⛔⛔ Attacker Identified the VICTIM ⛔⛔⛔\n",
      "Victim: 3 \t Current Episode Reward : 39\n",
      "==================================⬇️⬇️⬇️⬇️Episode: 5⬇️⬇️⬇️⬇️==============================\n",
      "-------------------------\n",
      "Attacker Action:  0\n",
      "⛔⛔⛔ Attacker Identified the VICTIM ⛔⛔⛔\n",
      "Victim: 4 \t Current Episode Reward : 39\n",
      "==================================⬇️⬇️⬇️⬇️Episode: 6⬇️⬇️⬇️⬇️==============================\n",
      "-------------------------\n",
      "Attacker Action:  7\n",
      "Attacker Action:  5\n",
      "Attacker Action:  0\n",
      "⛔⛔⛔ Attacker Identified the VICTIM ⛔⛔⛔\n",
      "Victim: 5 \t Current Episode Reward : 39\n",
      "==================================⬇️⬇️⬇️⬇️Episode: 7⬇️⬇️⬇️⬇️==============================\n",
      "-------------------------\n",
      "Attacker Action:  0\n",
      "⛔⛔⛔ Attacker Identified the VICTIM ⛔⛔⛔\n",
      "Victim: 6 \t Current Episode Reward : 39\n",
      "==================================⬇️⬇️⬇️⬇️Episode: 8⬇️⬇️⬇️⬇️==============================\n",
      "-------------------------\n",
      "Attacker Action:  0\n",
      "⛔⛔⛔ Attacker Identified the VICTIM ⛔⛔⛔\n",
      "Victim: 7 \t Current Episode Reward : 39\n",
      "==================================⬇️⬇️⬇️⬇️Episode: 9⬇️⬇️⬇️⬇️==============================\n",
      "-------------------------\n",
      "Attacker Action:  1\n",
      "Attacker Action:  0\n",
      "⛔⛔⛔ Attacker Identified the VICTIM ⛔⛔⛔\n",
      "Victim: 8 \t Current Episode Reward : 39\n",
      "==================================⬇️⬇️⬇️⬇️Episode: 10⬇️⬇️⬇️⬇️==============================\n",
      "-------------------------\n",
      "Attacker Action:  0\n",
      "Attacker Action:  1\n",
      "⛔⛔⛔ Attacker Identified the VICTIM ⛔⛔⛔\n",
      "Victim: 9 \t Current Episode Reward : 39\n",
      "==================================⬇️⬇️⬇️⬇️Episode: 11⬇️⬇️⬇️⬇️==============================\n",
      "-------------------------\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 11 is out of bounds for axis 0 with size 11",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVictim: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Current Episode Reward : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(env\u001b[38;5;241m.\u001b[39mvictim_id, rewards[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m---> 34\u001b[0m         \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, args\u001b[38;5;241m.\u001b[39mmax_queries\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[0;32m/mnt/sobhan/Beacons/env.py:72\u001b[0m, in \u001b[0;36mEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==================================⬇️⬇️⬇️⬇️Episode: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m⬇️⬇️⬇️⬇️==============================\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# print(\"Reseting the Populations\")\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Randomly set populations and genes\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m beacon_case, beacon_control, attack_control, victim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvictim_id, mafs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_populations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maltered_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/mnt/sobhan/Beacons/env.py:203\u001b[0m, in \u001b[0;36mEnv.get_populations\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# victim_ind = np.random.randint(1, beacon_size)\u001b[39;00m\n\u001b[1;32m    201\u001b[0m victim_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \n\u001b[0;32m--> 203\u001b[0m victim \u001b[38;5;241m=\u001b[39m \u001b[43mbeacon_case\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvictim_ind\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvictim_prob:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# print(\"Victim is inside the Beacon!\")\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# print(f\"Victim is the {victim_ind-1}th person\")\u001b[39;00m\n\u001b[1;32m    208\u001b[0m     beacon_case \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdelete(beacon_case, \u001b[38;5;241m0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 11 is out of bounds for axis 0 with size 11"
     ]
    }
   ],
   "source": [
    "from ppo import PPO\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from env import Env\n",
    "\n",
    "args=args_create()\n",
    "env = Env(args, beacon, maf_values, binary)\n",
    " \n",
    "################ PPO hyperparameters ################\n",
    "K_epochs = 300         # update policy for K epochs\n",
    "eps_clip = 0.1           # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0001      # learning rate for actor network\n",
    "lr_critic = 0.0001        # learning rate for critic network\n",
    "\n",
    "i_episode = 0\n",
    "\n",
    "if args.attacker_type == \"agent\":\n",
    "    attacker_state_dim = 400\n",
    "    attacker_action_dim = 10\n",
    "\n",
    "    attacker_agent = PPO(attacker_state_dim, attacker_action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, False, None)\n",
    "    attacker_agent.load(\"/mnt/sobhan/Beacons/agents/attacker1/PPO_0.pth\")\n",
    "\n",
    "if args.attacker_type == \"agent\":\n",
    "    while i_episode <= args.episodes:\n",
    "        for t in range(1, args.max_queries+1):\n",
    "            _, rewards, done, _  = env.step(attacker_agent=attacker_agent)\n",
    "            if done:\n",
    "                break\n",
    "        print(\"Victim: {} \\t Current Episode Reward : {}\".format(env.victim_id, rewards[1]))\n",
    "        env.reset()\n",
    "\n",
    "else:\n",
    "    for t in range(1, args.max_queries+1):\n",
    "        _, rewards, done, _  = env.step()\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    has_continuous_action_space = True                \n",
    "\n",
    "action_std = 0.4             # starting std for action distribution (Multivariate Normal)\n",
    "action_std_decay_rate = 0.0025       # linearly decay action_std (action_std = action_std - action_std_decay_rate)\n",
    "min_action_std = 0.05                # minimum action_std (stop decay after action_std <= min_action_std)\n",
    "action_std_decay_freq = int(2.5e5)\n",
    "\n",
    "################ PPO hyperparameters ################\n",
    "K_epochs = 200          # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003       # learning rate for actor network\n",
    "lr_critic = 0.001       # learning rate for critic network\n",
    "\n",
    "random_seed = 0      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ppo import PPO\n",
    "from environment import BeaconEnv\n",
    "\n",
    "args = args_create()\n",
    "\n",
    "print(\"\\n=============================================\\n\")\n",
    "print(\"Start Simulation Using Optimal Attacker\")\n",
    "\n",
    "state_dim = 7\n",
    "action_dim = env.action_space.shape[0]\n",
    "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "ppo_agent.load(\"/mnt/sobhan/Beaconsv2/results/train/run14/weights/PPO_1.pth\")\n",
    "\n",
    "\n",
    "env = BeaconEnv(args, beacon, maf_values, binary)\n",
    "state = env.reset()[1]\n",
    "total_reward = 0\n",
    "current_ureward = 0\n",
    "current_preward = 0\n",
    "privacy_rewards = []\n",
    "utility_rewards = []\n",
    "total_rewards = []\n",
    "lrt_values_list = []\n",
    "\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    # print(\"\\n=============================================\\n\")\n",
    "\n",
    "    # state = torch.flatten(state)\n",
    "    with torch.no_grad():\n",
    "        action, _, _ = ppo_agent.policy_old.act(torch.as_tensor(state).float().to(args.device))\n",
    "\n",
    "    # print(\"Beacon Action: \", action)\n",
    "\n",
    "    action = action.squeeze().item()\n",
    "    state, reward, done, rewards = env.step([action])\n",
    "\n",
    "    total_reward += reward\n",
    "    current_preward += rewards[0]\n",
    "    current_ureward += rewards[1]\n",
    "\n",
    "    # print(\"Current Privacy Reward: {}\\nCurrent Utility Reward: {}\\nThis Episode Reward: {}\\nTotal Reward: {}\".format(rewards[0], rewards[1], reward, total_reward))\n",
    "    total_rewards.append(total_reward)\n",
    "    utility_rewards.append(current_ureward)\n",
    "    privacy_rewards.append(current_preward)\n",
    "    # lrt_values_list.append(lrt_values)\n",
    "\n",
    "# plot_lists(utility_rewards, args.results_dir, 'utilities', 0)\n",
    "print(f\"Validation completed, total reward\")\n",
    "# print(\"\\n=============================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
