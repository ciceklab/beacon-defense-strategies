{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproducibility(seed: int):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "reproducibility(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to : NVIDIA TITAN RTX\n"
     ]
    }
   ],
   "source": [
    "# set device to cpu or cuda\n",
    "device = torch.device('cpu')\n",
    "\n",
    "if(torch.cuda.is_available()):\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    print(\"Device set to : cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def args_create():\n",
    "    # @title Arguments\n",
    "    parser = argparse.ArgumentParser(description='Actor Critic')\n",
    "\n",
    "    parser.add_argument('--data', default=\"/mnt/kerem/CEU\", type=str, help='Dataset Path')\n",
    "    parser.add_argument('--episodes', default=9, type=int, metavar='N', help='Number of episodes for training agent.')\n",
    "    parser.add_argument('--seed', default=3, type=int, help='Seed for reproducibility')\n",
    "\n",
    "    # Env Properties\n",
    "    parser.add_argument('--a_control_size', default=50, type=int, help='Attack Control group size')\n",
    "    parser.add_argument('--b_control_size', default=50, type=int, help='Beacon Control group size')\n",
    "    parser.add_argument('--gene_size', default=100, type=int, help='States gene size')\n",
    "    parser.add_argument('--beacon_size', default=10, type=int, help='Beacon population size')\n",
    "    parser.add_argument('--victim_prob', default=1, type=float, help='Victim inside beacon or not!')\n",
    "    parser.add_argument('--max_queries', default=5, type=int, help='Maximum queries per episode')\n",
    "\n",
    "\n",
    "    parser.add_argument('--attacker_type', default=\"optimal\", choices=[\"random\", \"optimal\", \"agent\"], type=str, help='Type of the attacker')\n",
    "    parser.add_argument('--beacon_type', default=\"agent\", choices=[\"random\", \"agent\", \"truth\"], type=str, help='Type of the beacon')\n",
    "\n",
    "\n",
    "    parser.add_argument('--pop_reset_freq', default=100000000, type=int, help='Reset Population Frequency (Epochs)')\n",
    "    parser.add_argument('--plot-freq', default=1, type=int, metavar='N', help='Plot Frequencies')\n",
    "\n",
    "    # utils\n",
    "    parser.add_argument('--resume', default=\"\", type=str, metavar='PATH', help='path to latest checkpoint (default: none)')\n",
    "    parser.add_argument('--results-dir', default='./results/simulation', type=str, metavar='PATH', help='path to cache (default: none)')\n",
    "\n",
    "    # args = parser.parse_args()  # running in command line\n",
    "    args = parser.parse_args('')  # running in ipynb\n",
    "\n",
    "    args.results_dir = os.path.join(args.results_dir, \"run\"+str(len(os.listdir(args.results_dir))))\n",
    "    os.makedirs(args.results_dir)\n",
    "    os.makedirs(args.results_dir+\"/logs\")\n",
    "    os.makedirs(args.results_dir+\"/rewards\")\n",
    "    os.makedirs(args.results_dir+\"/indrewards\")\n",
    "    os.makedirs(args.results_dir+\"/actions\")\n",
    "    os.makedirs(args.results_dir+\"/pvalues\")\n",
    "\n",
    "    args.device = device\n",
    "\n",
    "    print(args)\n",
    "    return args\n",
    "\n",
    "# args = args_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CEU Beacon - it contains 164 people in total which we will divide into groups to experiment\n",
    "beacon = pd.read_csv(os.path.join(\"/mnt/kerem/CEU\", \"Beacon_164.txt\"), index_col=0, delim_whitespace=True)\n",
    "# Reference genome, i.e. the genome that has no SNPs, all major allele pairs for each position\n",
    "reference = pickle.load(open(os.path.join(\"/mnt/kerem/CEU\", \"reference.pickle\"),\"rb\"))\n",
    "# Binary representation of the beacon; 0: no SNP (i.e. no mutation) 1: SNP (i.e. mutation)\n",
    "binary = np.logical_and(beacon.values != reference, beacon.values != \"NN\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164, 4029840)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Table that contains MAF (minor allele frequency) values for each position. \n",
    "maf = pd.read_csv(os.path.join(\"/mnt/kerem/CEU\", \"MAF.txt\"), index_col=0, delim_whitespace=True)\n",
    "maf.rename(columns = {'referenceAllele':'major', 'referenceAlleleFrequency':'major_freq', \n",
    "                      'otherAllele':'minor', 'otherAlleleFrequency':'minor_freq'}, inplace = True)\n",
    "maf[\"maf\"] = np.round(maf[\"maf\"].values, 3)\n",
    "# Same variable with sorted maf values\n",
    "sorted_maf = maf.sort_values(by='maf')\n",
    "# Extracting column to an array for future use\n",
    "maf_values = maf[\"maf\"].values\n",
    "\n",
    "binary = binary.T\n",
    "binary.shape #(164, 4029840)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4029840, 164), (4029840, 1), (164, 4029840), (4029840,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beacon.shape, reference.shape, binary.shape, maf_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to : NVIDIA TITAN RTX\n",
      "Namespace(data='/mnt/kerem/CEU', episodes=9, seed=3, a_control_size=50, b_control_size=50, gene_size=100, beacon_size=10, victim_prob=1, max_queries=5, attacker_type='optimal', beacon_type='agent', pop_reset_freq=100000000, plot_freq=1, resume='', results_dir='./results/simulation/run3', device=device(type='cuda', index=0))\n",
      "./results/simulation/run3/logs/beacon_log.csv\n",
      "./results/simulation/run3/logs/beacon_control_log.csv\n",
      "./results/simulation/run3/logs/attacker_log.csv\n",
      "./results/simulation/run3/logs/attacker_control_log.csv\n",
      "==================================⬇️⬇️⬇️⬇️Episode: 1⬇️⬇️⬇️⬇️==============================\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sobhan/cpi/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVictim's LRT: 0.0\n",
      "\tThreshold LRT: 0.0\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 14 with MAF: 0.009 and SNP: 1.0 and LRT: 6.889673789677839\n",
      "Beacon Action: 0\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: tensor([0.])  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.889673709869385\n",
      "\tThreshold LRT: 0.0\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 86 with MAF: 0.075 and SNP: 1.0 and LRT: -0.23585350801794114\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.653820514678955\n",
      "\tThreshold LRT: -0.23585349321365356\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 66 with MAF: 0.098 and SNP: 1.0 and LRT: 6.701473761143111\n",
      "Beacon Action: 0\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 13.355294227600098\n",
      "\tThreshold LRT: -0.23585349321365356\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 90 with MAF: 0.205 and SNP: 1.0 and LRT: -0.010206487205471682\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 13.345088005065918\n",
      "\tThreshold LRT: -0.24605998396873474\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 96 with MAF: 0.27 and SNP: 1.0 and LRT: -0.0018452006389120257\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "✅✅✅ Attacker Could NOT Indentify the VICTIM ✅✅✅\n",
      "\tVictim's LRT: 13.343242645263672\n",
      "\tThreshold LRT: -0.24605998396873474\n",
      "\tVictim's Pvalue: 1.0\n",
      "Victim: 0 \t Current Episode Reward : 2.0\n",
      "==================================⬇️⬇️⬇️⬇️Episode: 2⬇️⬇️⬇️⬇️==============================\n",
      "-------------------------\n",
      "\tVictim's LRT: 0.0\n",
      "\tThreshold LRT: 0.0\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 61 with MAF: 0.067 and SNP: 0.0 and LRT: -0.0\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 0.07999999821186066\n",
      "\tVictim's LRT: -0.28715941309928894\n",
      "\tThreshold LRT: -0.28715941309928894\n",
      "\tVictim's Pvalue: 0.07999999821186066\n",
      "Attacker Action: Position 33 with MAF: 0.133 and SNP: 0.0 and LRT: 0.0\n",
      "Beacon Action: 0\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.335163116455078\n",
      "\tThreshold LRT: -0.28715941309928894\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 89 with MAF: 0.192 and SNP: 0.0 and LRT: 0.0\n",
      "Beacon Action: 0\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 12.816532135009766\n",
      "\tThreshold LRT: -0.28715941309928894\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 90 with MAF: 0.205 and SNP: 1.0 and LRT: -0.010206487205471682\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 12.806325912475586\n",
      "\tThreshold LRT: -0.2973659038543701\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 10 with MAF: 0.223 and SNP: 0.0 and LRT: 0.0\n",
      "Beacon Action: 0\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "✅✅✅ Attacker Could NOT Indentify the VICTIM ✅✅✅\n",
      "\tVictim's LRT: 19.20945167541504\n",
      "\tThreshold LRT: -0.2973659038543701\n",
      "\tVictim's Pvalue: 1.0\n",
      "Victim: 1 \t Current Episode Reward : 2.0\n",
      "==================================⬇️⬇️⬇️⬇️Episode: 3⬇️⬇️⬇️⬇️==============================\n",
      "-------------------------\n",
      "\tVictim's LRT: 0.0\n",
      "\tThreshold LRT: 0.0\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 57 with MAF: 0.033 and SNP: 0.0 and LRT: 0.0\n",
      "Beacon Action: 0\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: tensor([0.])  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.840641498565674\n",
      "\tThreshold LRT: 0.0\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 49 with MAF: 0.142 and SNP: 0.0 and LRT: -0.0\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.7928314208984375\n",
      "\tThreshold LRT: -0.047810524702072144\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 97 with MAF: 0.173 and SNP: 0.0 and LRT: -0.0\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.770216464996338\n",
      "\tThreshold LRT: -0.047810524702072144\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 89 with MAF: 0.192 and SNP: 0.0 and LRT: 0.0\n",
      "Beacon Action: 0\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 13.251585006713867\n",
      "\tThreshold LRT: -0.047810524702072144\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 10 with MAF: 0.223 and SNP: 0.0 and LRT: -0.0\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "✅✅✅ Attacker Could NOT Indentify the VICTIM ✅✅✅\n",
      "\tVictim's LRT: 13.245141983032227\n",
      "\tThreshold LRT: -0.054253749549388885\n",
      "\tVictim's Pvalue: 1.0\n",
      "Victim: 2 \t Current Episode Reward : 2.0\n",
      "==================================⬇️⬇️⬇️⬇️Episode: 4⬇️⬇️⬇️⬇️==============================\n",
      "-------------------------\n",
      "\tVictim's LRT: 0.0\n",
      "\tThreshold LRT: 0.0\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 31 with MAF: 0.15 and SNP: 0.0 and LRT: -0.0\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 0.18000000715255737\n",
      "\tVictim's LRT: -0.039477020502090454\n",
      "\tThreshold LRT: -0.039477020502090454\n",
      "\tVictim's Pvalue: 0.18000000715255737\n",
      "Attacker Action: Position 9 with MAF: 0.167 and SNP: 0.0 and LRT: 0.0\n",
      "Beacon Action: 0\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.502834796905518\n",
      "\tThreshold LRT: -0.039477020502090454\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 64 with MAF: 0.233 and SNP: 0.0 and LRT: -0.0\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.497866153717041\n",
      "\tThreshold LRT: -0.04444580897688866\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 77 with MAF: 0.271 and SNP: 0.0 and LRT: -0.0\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.496070861816406\n",
      "\tThreshold LRT: -0.04624105617403984\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 92 with MAF: 0.375 and SNP: 0.0 and LRT: -0.0\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "✅✅✅ Attacker Could NOT Indentify the VICTIM ✅✅✅\n",
      "\tVictim's LRT: 6.495988368988037\n",
      "\tThreshold LRT: -0.046323563903570175\n",
      "\tVictim's Pvalue: 1.0\n",
      "Victim: 3 \t Current Episode Reward : 2.0\n",
      "==================================⬇️⬇️⬇️⬇️Episode: 5⬇️⬇️⬇️⬇️==============================\n",
      "-------------------------\n",
      "\tVictim's LRT: 0.0\n",
      "\tThreshold LRT: 0.0\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 58 with MAF: 0.049 and SNP: 0.0 and LRT: 0.0\n",
      "Beacon Action: 0\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: tensor([0.])  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.807272911071777\n",
      "\tThreshold LRT: 0.0\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 24 with MAF: 0.115 and SNP: 0.0 and LRT: -0.0\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.716507911682129\n",
      "\tThreshold LRT: -0.09076513350009918\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 33 with MAF: 0.133 and SNP: 0.0 and LRT: -0.0\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.657264709472656\n",
      "\tThreshold LRT: -0.15000827610492706\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 78 with MAF: 0.14 and SNP: 0.0 and LRT: 0.0\n",
      "Beacon Action: 0\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 13.263374328613281\n",
      "\tThreshold LRT: -0.2001563310623169\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 8 with MAF: 0.146 and SNP: 0.0 and LRT: 0.0\n",
      "Beacon Action: 0\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "✅✅✅ Attacker Could NOT Indentify the VICTIM ✅✅✅\n",
      "\tVictim's LRT: 19.85548210144043\n",
      "\tThreshold LRT: -0.2001563310623169\n",
      "\tVictim's Pvalue: 1.0\n",
      "Victim: 4 \t Current Episode Reward : 2.0\n",
      "==================================⬇️⬇️⬇️⬇️Episode: 6⬇️⬇️⬇️⬇️==============================\n",
      "-------------------------\n",
      "\tVictim's LRT: 0.0\n",
      "\tThreshold LRT: 0.0\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 66 with MAF: 0.098 and SNP: 1.0 and LRT: -0.13577309003032134\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 0.23999999463558197\n",
      "\tVictim's LRT: -0.13577309250831604\n",
      "\tThreshold LRT: -0.13577309250831604\n",
      "\tVictim's Pvalue: 0.23999999463558197\n",
      "Attacker Action: Position 90 with MAF: 0.205 and SNP: 1.0 and LRT: 6.448928950326526\n",
      "Beacon Action: 0\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.313155651092529\n",
      "\tThreshold LRT: -0.14597958326339722\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 44 with MAF: 0.239 and SNP: 0.0 and LRT: 0.0\n",
      "Beacon Action: 0\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 12.674667358398438\n",
      "\tThreshold LRT: -0.14597958326339722\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 71 with MAF: 0.243 and SNP: 0.0 and LRT: 0.0\n",
      "Beacon Action: 0\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 19.025638580322266\n",
      "\tThreshold LRT: -0.1497989445924759\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 22 with MAF: 0.257 and SNP: 0.0 and LRT: -0.0\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "✅✅✅ Attacker Could NOT Indentify the VICTIM ✅✅✅\n",
      "\tVictim's LRT: 19.02301025390625\n",
      "\tThreshold LRT: -0.1524265557527542\n",
      "\tVictim's Pvalue: 1.0\n",
      "Victim: 5 \t Current Episode Reward : 2.0\n",
      "==================================⬇️⬇️⬇️⬇️Episode: 7⬇️⬇️⬇️⬇️==============================\n",
      "-------------------------\n",
      "\tVictim's LRT: 0.0\n",
      "\tThreshold LRT: 0.0\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 57 with MAF: 0.033 and SNP: 0.0 and LRT: 0.0\n",
      "Beacon Action: 0\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: tensor([0.])  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.840641498565674\n",
      "\tThreshold LRT: 0.0\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 66 with MAF: 0.098 and SNP: 1.0 and LRT: -0.13577309003032134\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.704868793487549\n",
      "\tThreshold LRT: -0.13577309250831604\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 99 with MAF: 0.111 and SNP: 0.0 and LRT: 0.0\n",
      "Beacon Action: 0\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 13.377307891845703\n",
      "\tThreshold LRT: -0.2355484813451767\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 18 with MAF: 0.128 and SNP: 0.0 and LRT: -0.0\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 13.310596466064453\n",
      "\tThreshold LRT: -0.30225998163223267\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 31 with MAF: 0.15 and SNP: 0.0 and LRT: -0.0\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "✅✅✅ Attacker Could NOT Indentify the VICTIM ✅✅✅\n",
      "\tVictim's LRT: 13.271119117736816\n",
      "\tThreshold LRT: -0.30225998163223267\n",
      "\tVictim's Pvalue: 1.0\n",
      "Victim: 6 \t Current Episode Reward : 2.0\n",
      "==================================⬇️⬇️⬇️⬇️Episode: 8⬇️⬇️⬇️⬇️==============================\n",
      "-------------------------\n",
      "\tVictim's LRT: 0.0\n",
      "\tThreshold LRT: 0.0\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 17 with MAF: 0.025 and SNP: 0.0 and LRT: 0.0\n",
      "Beacon Action: 0\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: tensor([0.])  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.857119560241699\n",
      "\tThreshold LRT: 0.0\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 31 with MAF: 0.15 and SNP: 0.0 and LRT: -0.0\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.817642688751221\n",
      "\tThreshold LRT: -0.039477020502090454\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 9 with MAF: 0.167 and SNP: 0.0 and LRT: 0.0\n",
      "Beacon Action: 0\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 13.359954833984375\n",
      "\tThreshold LRT: -0.039477020502090454\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 76 with MAF: 0.173 and SNP: 0.0 and LRT: -0.0\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 13.337340354919434\n",
      "\tThreshold LRT: -0.06209170073270798\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 97 with MAF: 0.173 and SNP: 0.0 and LRT: 0.0\n",
      "Beacon Action: 0\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "✅✅✅ Attacker Could NOT Indentify the VICTIM ✅✅✅\n",
      "\tVictim's LRT: 19.86519432067871\n",
      "\tThreshold LRT: -0.06565658003091812\n",
      "\tVictim's Pvalue: 1.0\n",
      "Victim: 7 \t Current Episode Reward : 2.0\n",
      "==================================⬇️⬇️⬇️⬇️Episode: 9⬇️⬇️⬇️⬇️==============================\n",
      "-------------------------\n",
      "\tVictim's LRT: 0.0\n",
      "\tThreshold LRT: 0.0\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 41 with MAF: 0.071 and SNP: 0.0 and LRT: 0.0\n",
      "Beacon Action: 0\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: tensor([0.])  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.760462284088135\n",
      "\tThreshold LRT: -0.260128915309906\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 18 with MAF: 0.128 and SNP: 0.0 and LRT: -0.0\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.693750858306885\n",
      "\tThreshold LRT: -0.260128915309906\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 22 with MAF: 0.257 and SNP: 0.0 and LRT: -0.0\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.691123008728027\n",
      "\tThreshold LRT: -0.2627565264701843\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 37 with MAF: 0.305 and SNP: 1.0 and LRT: -0.0006901548179367722\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.690433025360107\n",
      "\tThreshold LRT: -0.2627565264701843\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 95 with MAF: 0.35 and SNP: 0.0 and LRT: 0.0\n",
      "Beacon Action: 0\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "✅✅✅ Attacker Could NOT Indentify the VICTIM ✅✅✅\n",
      "\tVictim's LRT: 12.73662281036377\n",
      "\tThreshold LRT: -0.26293736696243286\n",
      "\tVictim's Pvalue: 1.0\n",
      "Victim: 8 \t Current Episode Reward : 2.0\n",
      "==================================⬇️⬇️⬇️⬇️Episode: 10⬇️⬇️⬇️⬇️==============================\n",
      "-------------------------\n",
      "\tVictim's LRT: 0.0\n",
      "\tThreshold LRT: 0.0\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 47 with MAF: 0.115 and SNP: 0.0 and LRT: -0.0\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 0.23999999463558197\n",
      "\tVictim's LRT: -0.09076513350009918\n",
      "\tThreshold LRT: -0.09076513350009918\n",
      "\tVictim's Pvalue: 0.23999999463558197\n",
      "Attacker Action: Position 18 with MAF: 0.128 and SNP: 0.0 and LRT: 0.0\n",
      "Beacon Action: 0\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.543058395385742\n",
      "\tThreshold LRT: -0.09076513350009918\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 90 with MAF: 0.205 and SNP: 1.0 and LRT: -0.010206487205471682\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.5328521728515625\n",
      "\tThreshold LRT: -0.10097161680459976\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 44 with MAF: 0.239 and SNP: 0.0 and LRT: -0.0\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "\tVictim's LRT: 6.52860689163208\n",
      "\tThreshold LRT: -0.10521651804447174\n",
      "\tVictim's Pvalue: 1.0\n",
      "Attacker Action: Position 22 with MAF: 0.257 and SNP: 0.0 and LRT: -0.0\n",
      "Beacon Action: 1\n",
      "-----------------------------------------------------------------\n",
      "Beacon utility reward: 1.0  privacy reward: 1.0\n",
      "✅✅✅ Attacker Could NOT Indentify the VICTIM ✅✅✅\n",
      "\tVictim's LRT: 6.525979518890381\n",
      "\tThreshold LRT: -0.10784412920475006\n",
      "\tVictim's Pvalue: 1.0\n",
      "Victim: 9 \t Current Episode Reward : 2.0\n",
      "==================================⬇️⬇️⬇️⬇️Episode: 11⬇️⬇️⬇️⬇️==============================\n",
      "-------------------------\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 11 is out of bounds for axis 0 with size 11",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVictim: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Current Episode Reward : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(env\u001b[38;5;241m.\u001b[39mvictim_id, rewards[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m---> 53\u001b[0m         \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m         i_episode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/sobhan/Beacons/env.py:72\u001b[0m, in \u001b[0;36mEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==================================⬇️⬇️⬇️⬇️Episode: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m⬇️⬇️⬇️⬇️==============================\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# print(\"Reseting the Populations\")\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Randomly set populations and genes\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m beacon_case, beacon_control, attack_control, victim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvictim_id, mafs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_populations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maltered_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/mnt/sobhan/Beacons/env.py:204\u001b[0m, in \u001b[0;36mEnv.get_populations\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# victim_ind = np.random.randint(1, beacon_size)\u001b[39;00m\n\u001b[1;32m    202\u001b[0m victim_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \n\u001b[0;32m--> 204\u001b[0m victim \u001b[38;5;241m=\u001b[39m \u001b[43mbeacon_case\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvictim_ind\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvictim_prob:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# print(\"Victim is inside the Beacon!\")\u001b[39;00m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# print(f\"Victim is the {victim_ind-1}th person\")\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     beacon_case \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdelete(beacon_case, \u001b[38;5;241m0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 11 is out of bounds for axis 0 with size 11"
     ]
    }
   ],
   "source": [
    "from ppo import PPO\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from env import Env\n",
    "\n",
    "args=args_create()\n",
    "env = Env(args, beacon, maf_values, binary)\n",
    " \n",
    "################ PPO hyperparameters ################\n",
    "K_epochs = 300         # update policy for K epochs\n",
    "eps_clip = 0.1           # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0001      # learning rate for actor network\n",
    "lr_critic = 0.0001        # learning rate for critic network\n",
    "\n",
    "i_episode = 0\n",
    "\n",
    "if args.attacker_type == \"agent\":\n",
    "    attacker_state_dim = 400\n",
    "    attacker_action_dim = 10\n",
    "\n",
    "    attacker_agent = PPO(attacker_state_dim, attacker_action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, False, None)\n",
    "    attacker_agent.load(\"/mnt/sobhan/Beacons/agents/attacker1/PPO_0.pth\")\n",
    "\n",
    "if args.beacon_type == \"agent\":\n",
    "    state_dim = 9\n",
    "    action_dim = 2\n",
    "\n",
    "    # action_std = 0.4\n",
    "\n",
    "    beacon_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, False, None)\n",
    "    beacon_agent.load(\"/mnt/sobhan/Beacons/results/train/run4/weights/PPO_0.pth\")\n",
    "\n",
    "if args.attacker_type == \"agent\":\n",
    "    while i_episode <= args.episodes:\n",
    "        for t in range(1, args.max_queries+1):\n",
    "            _, rewards, done, _  = env.step(attacker_agent=attacker_agent)\n",
    "            if done:\n",
    "                break\n",
    "        print(\"Victim: {} \\t Current Episode Reward : {}\".format(env.victim_id, rewards[1]))\n",
    "        env.reset()\n",
    "        i_episode += 1\n",
    "\n",
    "elif args.beacon_type == \"agent\":\n",
    "    while i_episode <= args.episodes:\n",
    "        for t in range(1, args.max_queries+1):\n",
    "            _, rewards, done, _  = env.step(beacon_agent=beacon_agent)\n",
    "            if done:\n",
    "                break\n",
    "        print(\"Victim: {} \\t Current Episode Reward : {}\".format(env.victim_id, rewards[0]))\n",
    "        env.reset()\n",
    "        i_episode += 1\n",
    "\n",
    "else:\n",
    "    for t in range(1, args.max_queries+1):\n",
    "        _, rewards, done, _  = env.step()\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    has_continuous_action_space = True                \n",
    "\n",
    "action_std = 0.4             # starting std for action distribution (Multivariate Normal)\n",
    "action_std_decay_rate = 0.0025       # linearly decay action_std (action_std = action_std - action_std_decay_rate)\n",
    "min_action_std = 0.05                # minimum action_std (stop decay after action_std <= min_action_std)\n",
    "action_std_decay_freq = int(2.5e5)\n",
    "\n",
    "################ PPO hyperparameters ################\n",
    "K_epochs = 200          # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003       # learning rate for actor network\n",
    "lr_critic = 0.001       # learning rate for critic network\n",
    "\n",
    "random_seed = 0      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ppo import PPO\n",
    "from environment import BeaconEnv\n",
    "\n",
    "args = args_create()\n",
    "\n",
    "print(\"\\n=============================================\\n\")\n",
    "print(\"Start Simulation Using Optimal Attacker\")\n",
    "\n",
    "state_dim = 7\n",
    "action_dim = env.action_space.shape[0]\n",
    "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "ppo_agent.load(\"/mnt/sobhan/Beaconsv2/results/train/run14/weights/PPO_1.pth\")\n",
    "\n",
    "\n",
    "env = BeaconEnv(args, beacon, maf_values, binary)\n",
    "state = env.reset()[1]\n",
    "total_reward = 0\n",
    "current_ureward = 0\n",
    "current_preward = 0\n",
    "privacy_rewards = []\n",
    "utility_rewards = []\n",
    "total_rewards = []\n",
    "lrt_values_list = []\n",
    "\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    # print(\"\\n=============================================\\n\")\n",
    "\n",
    "    # state = torch.flatten(state)\n",
    "    with torch.no_grad():\n",
    "        action, _, _ = ppo_agent.policy_old.act(torch.as_tensor(state).float().to(args.device))\n",
    "\n",
    "    # print(\"Beacon Action: \", action)\n",
    "\n",
    "    action = action.squeeze().item()\n",
    "    state, reward, done, rewards = env.step([action])\n",
    "\n",
    "    total_reward += reward\n",
    "    current_preward += rewards[0]\n",
    "    current_ureward += rewards[1]\n",
    "\n",
    "    # print(\"Current Privacy Reward: {}\\nCurrent Utility Reward: {}\\nThis Episode Reward: {}\\nTotal Reward: {}\".format(rewards[0], rewards[1], reward, total_reward))\n",
    "    total_rewards.append(total_reward)\n",
    "    utility_rewards.append(current_ureward)\n",
    "    privacy_rewards.append(current_preward)\n",
    "    # lrt_values_list.append(lrt_values)\n",
    "\n",
    "# plot_lists(utility_rewards, args.results_dir, 'utilities', 0)\n",
    "print(f\"Validation completed, total reward\")\n",
    "# print(\"\\n=============================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
