{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Training environment: CartPole-v1\n",
      "Started training at (GMT): 2024-06-17 12:10:07\n",
      "Episode : 1 \t\t Timestep : 20 \t\t Average Reward : 16.0\n",
      "Episode : 2 \t\t Timestep : 50 \t\t Average Reward : 27.0\n",
      "Episode : 3 \t\t Timestep : 70 \t\t Average Reward : 24.0\n",
      "Episode : 4 \t\t Timestep : 90 \t\t Average Reward : 20.0\n",
      "Episode : 5 \t\t Timestep : 110 \t\t Average Reward : 14.0\n",
      "Episode : 6 \t\t Timestep : 120 \t\t Average Reward : 15.0\n",
      "Episode : 7 \t\t Timestep : 140 \t\t Average Reward : 16.0\n",
      "Episode : 8 \t\t Timestep : 150 \t\t Average Reward : 14.0\n",
      "Episode : 9 \t\t Timestep : 180 \t\t Average Reward : 29.0\n",
      "Episode : 10 \t\t Timestep : 230 \t\t Average Reward : 48.0\n",
      "Episode : 11 \t\t Timestep : 240 \t\t Average Reward : 15.0\n",
      "Episode : 12 \t\t Timestep : 270 \t\t Average Reward : 30.0\n",
      "Episode : 13 \t\t Timestep : 300 \t\t Average Reward : 23.0\n",
      "Episode : 14 \t\t Timestep : 340 \t\t Average Reward : 40.0\n",
      "Episode : 15 \t\t Timestep : 350 \t\t Average Reward : 16.0\n",
      "Episode : 16 \t\t Timestep : 360 \t\t Average Reward : 11.0\n",
      "Episode : 17 \t\t Timestep : 390 \t\t Average Reward : 23.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m current_ep_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_step \u001b[38;5;241m%\u001b[39m max_ep_len \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[43mppo_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_step \u001b[38;5;241m%\u001b[39m log_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m print_running_episodes \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     72\u001b[0m     log_f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi_episode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_ep_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/mnt/sobhan/Beaconsv3/ppo.py:271\u001b[0m, in \u001b[0;36mPPO.update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Optimize policy for K epochs\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK_epochs):\n\u001b[1;32m    269\u001b[0m \n\u001b[1;32m    270\u001b[0m     \u001b[38;5;66;03m# Evaluating old actions and values\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m     logprobs, state_values, dist_entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# match state_values tensor dimensions with rewards tensor\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     state_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(state_values)\n",
      "File \u001b[0;32m/mnt/sobhan/Beaconsv3/ppo.py:141\u001b[0m, in \u001b[0;36mActorCritic.evaluate\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     action_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(state)\n\u001b[0;32m--> 141\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m action_logprobs \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(action)\n\u001b[1;32m    144\u001b[0m dist_entropy \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mentropy()\n",
      "File \u001b[0;32m/mnt/sobhan/cpi/lib/python3.12/site-packages/torch/distributions/categorical.py:70\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     67\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[1;32m     69\u001b[0m )\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/sobhan/cpi/lib/python3.12/site-packages/torch/distributions/distribution.py:67\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     65\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[1;32m     66\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[0;32m---> 67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m     68\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m             )\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gym\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from ppo import PPO\n",
    "import os\n",
    "\n",
    "env_name = \"CartPole-v1\"\n",
    "has_continuous_action_space = False\n",
    "\n",
    "max_ep_len = 400\n",
    "max_training_timesteps = 100000\n",
    "\n",
    "print_freq = max_ep_len * 2\n",
    "log_freq = 10\n",
    "save_model_freq = int(1e5)\n",
    "\n",
    "action_std = None\n",
    "K_epochs = 200\n",
    "eps_clip = 0.1\n",
    "gamma = 0.99\n",
    "\n",
    "lr_actor = 0.0001  \n",
    "lr_critic = 0.0001 \n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n if not has_continuous_action_space else env.action_space.shape[0]\n",
    "\n",
    "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "\n",
    "log_dir = \"PPO_logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_f_name = log_dir + '/' + env_name + \"_PPO_log_\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".csv\"\n",
    "\n",
    "print(f\"Training environment: {env_name}\")\n",
    "\n",
    "log_f = open(log_f_name, \"w+\")\n",
    "log_f.write('episode,timestep,reward\\n')\n",
    "\n",
    "start_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT):\", start_time)\n",
    "\n",
    "time_step = 0\n",
    "i_episode = 0\n",
    "\n",
    "\n",
    "# printing and logging variables\n",
    "print_running_reward = 0\n",
    "print_running_episodes = 0\n",
    "\n",
    "while time_step <= max_training_timesteps:\n",
    "    state = env.reset()[0]\n",
    "    current_ep_reward = 0\n",
    "\n",
    "    for t in range(1, max_ep_len + 1):\n",
    "        action = ppo_agent.select_action(state)\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "        time_step += 1\n",
    "        current_ep_reward += reward\n",
    "\n",
    "        if time_step % max_ep_len * 3 == 0:\n",
    "            ppo_agent.update()\n",
    "\n",
    "        if time_step % log_freq == 0 and print_running_episodes > 0:\n",
    "            log_f.write(f'{i_episode},{time_step},{current_ep_reward}\\n')\n",
    "            # print average reward till last episode\n",
    "            print_avg_reward = print_running_reward / print_running_episodes\n",
    "            print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
    "\n",
    "            print_running_reward = 0\n",
    "            print_running_episodes = 0\n",
    "\n",
    "\n",
    "        if time_step % save_model_freq == 0:\n",
    "            ppo_agent.save(f\"PPO_{env_name}.pth\")\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print_running_reward += current_ep_reward\n",
    "    print_running_episodes += 1\n",
    "\n",
    "    i_episode += 1\n",
    "\n",
    "log_f.close()\n",
    "end_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Finished training at (GMT):\", end_time)\n",
    "print(\"Total training time:\", end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 8, 2, 1000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "float(env.action_space.high[0]), env.observation_space.shape[0], env.action_space.shape[0], env._max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed)\n\u001b[1;32m     13\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mreproducibility\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36mreproducibility\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreproducibility\u001b[39m(seed: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(seed)\n\u001b[1;32m      5\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m      6\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39mdeterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "def reproducibility(seed: int):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "reproducibility(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Device set to : NVIDIA TITAN RTX\n",
      "Device set to : NVIDIA TITAN RTX\n",
      "Training environment: LunarLanderContinuous-v2\n",
      "Started training at (GMT): 2024-07-14 18:07:19\n",
      "Episode : 1 \t\t Timestep : 90 \t\t Reward: -140 \t\t Average Reward : -132.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sobhan/cpi/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 2 \t\t Timestep : 100 \t\t Reward: -131 \t\t Average Reward : -9.31\n",
      "Episode : 3 \t\t Timestep : 110 \t\t Reward: -139 \t\t Average Reward : -17.3\n",
      "Episode : 4 \t\t Timestep : 120 \t\t Reward: -119 \t\t Average Reward : -19.83\n",
      "Episode : 5 \t\t Timestep : 130 \t\t Reward: -88 \t\t Average Reward : -15.13\n",
      "Episode : 6 \t\t Timestep : 140 \t\t Reward: -178 \t\t Average Reward : -18.51\n",
      "Episode : 7 \t\t Timestep : 150 \t\t Reward: -155 \t\t Average Reward : -20.11\n",
      "Episode : 8 \t\t Timestep : 160 \t\t Reward: -138 \t\t Average Reward : -16.98\n",
      "Episode : 9 \t\t Timestep : 170 \t\t Reward: -144 \t\t Average Reward : -11.7\n",
      "Episode : 10 \t\t Timestep : 180 \t\t Reward: -127 \t\t Average Reward : -12.46\n",
      "Episode : 11 \t\t Timestep : 190 \t\t Reward: -179 \t\t Average Reward : -12.11\n",
      "Episode : 12 \t\t Timestep : 200 \t\t Reward: -139 \t\t Average Reward : -17.04\n",
      "Episode : 13 \t\t Timestep : 210 \t\t Reward: -162 \t\t Average Reward : -16.04\n",
      "Episode : 14 \t\t Timestep : 220 \t\t Reward: -122 \t\t Average Reward : -17.43\n",
      "Episode : 15 \t\t Timestep : 230 \t\t Reward: -89 \t\t Average Reward : -11.31\n",
      "Episode : 16 \t\t Timestep : 240 \t\t Reward: -202 \t\t Average Reward : -19.27\n",
      "Episode : 17 \t\t Timestep : 250 \t\t Reward: -115 \t\t Average Reward : -17.78\n",
      "Episode : 18 \t\t Timestep : 260 \t\t Reward: -148 \t\t Average Reward : -13.82\n",
      "Episode : 19 \t\t Timestep : 270 \t\t Reward: -106 \t\t Average Reward : -14.22\n",
      "Episode : 20 \t\t Timestep : 280 \t\t Reward: -159 \t\t Average Reward : -8.3\n",
      "Episode : 21 \t\t Timestep : 290 \t\t Reward: -126 \t\t Average Reward : -11.87\n",
      "Episode : 22 \t\t Timestep : 300 \t\t Reward: -136 \t\t Average Reward : -16.32\n",
      "Episode : 23 \t\t Timestep : 310 \t\t Reward: -142 \t\t Average Reward : -17.45\n",
      "Episode : 24 \t\t Timestep : 320 \t\t Reward: -136 \t\t Average Reward : -14.13\n",
      "Episode : 25 \t\t Timestep : 330 \t\t Reward: -121 \t\t Average Reward : -25.16\n",
      "Episode : 26 \t\t Timestep : 340 \t\t Reward: -157 \t\t Average Reward : -14.76\n",
      "Episode : 27 \t\t Timestep : 350 \t\t Reward: -117 \t\t Average Reward : -17.17\n",
      "Episode : 28 \t\t Timestep : 360 \t\t Reward: -129 \t\t Average Reward : -11.99\n",
      "Episode : 29 \t\t Timestep : 370 \t\t Reward: -139 \t\t Average Reward : -13.84\n",
      "Episode : 30 \t\t Timestep : 380 \t\t Reward: -127 \t\t Average Reward : -15.42\n",
      "Episode : 31 \t\t Timestep : 390 \t\t Reward: -119 \t\t Average Reward : -11.76\n",
      "Episode : 32 \t\t Timestep : 400 \t\t Reward: -142 \t\t Average Reward : -17.02\n",
      "Episode : 33 \t\t Timestep : 410 \t\t Reward: -132 \t\t Average Reward : -16.65\n",
      "Episode : 34 \t\t Timestep : 420 \t\t Reward: -155 \t\t Average Reward : -18.3\n",
      "Episode : 35 \t\t Timestep : 430 \t\t Reward: -103 \t\t Average Reward : -16.41\n",
      "Episode : 36 \t\t Timestep : 440 \t\t Reward: -143 \t\t Average Reward : -20.33\n",
      "Episode : 37 \t\t Timestep : 450 \t\t Reward: -145 \t\t Average Reward : -19.14\n",
      "Episode : 38 \t\t Timestep : 460 \t\t Reward: -83 \t\t Average Reward : -16.77\n",
      "Episode : 39 \t\t Timestep : 470 \t\t Reward: -132 \t\t Average Reward : -17.29\n",
      "Episode : 40 \t\t Timestep : 480 \t\t Reward: -142 \t\t Average Reward : -16.23\n",
      "Episode : 41 \t\t Timestep : 490 \t\t Reward: -125 \t\t Average Reward : -16.26\n",
      "Episode : 42 \t\t Timestep : 500 \t\t Reward: -150 \t\t Average Reward : -11.93\n",
      "Episode : 43 \t\t Timestep : 510 \t\t Reward: -198 \t\t Average Reward : -19.33\n",
      "Episode : 44 \t\t Timestep : 520 \t\t Reward: -158 \t\t Average Reward : -22.41\n",
      "Episode : 45 \t\t Timestep : 530 \t\t Reward: -136 \t\t Average Reward : -18.22\n",
      "Episode : 46 \t\t Timestep : 540 \t\t Reward: -122 \t\t Average Reward : -19.42\n",
      "Episode : 47 \t\t Timestep : 550 \t\t Reward: -117 \t\t Average Reward : -14.57\n",
      "Episode : 48 \t\t Timestep : 560 \t\t Reward: -144 \t\t Average Reward : -17.56\n",
      "Episode : 49 \t\t Timestep : 570 \t\t Reward: -66 \t\t Average Reward : -22.62\n",
      "Episode : 50 \t\t Timestep : 580 \t\t Reward: -125 \t\t Average Reward : -5.03\n",
      "Episode : 51 \t\t Timestep : 590 \t\t Reward: -140 \t\t Average Reward : -18.92\n",
      "Episode : 52 \t\t Timestep : 600 \t\t Reward: -152 \t\t Average Reward : -12.91\n",
      "Episode : 53 \t\t Timestep : 610 \t\t Reward: -148 \t\t Average Reward : -16.79\n",
      "Episode : 54 \t\t Timestep : 620 \t\t Reward: -166 \t\t Average Reward : -18.58\n",
      "Episode : 55 \t\t Timestep : 630 \t\t Reward: -120 \t\t Average Reward : -20.26\n",
      "Episode : 56 \t\t Timestep : 640 \t\t Reward: -76 \t\t Average Reward : -19.25\n",
      "Episode : 57 \t\t Timestep : 650 \t\t Reward: -134 \t\t Average Reward : -10.77\n",
      "Episode : 58 \t\t Timestep : 660 \t\t Reward: -145 \t\t Average Reward : -17.73\n",
      "Episode : 59 \t\t Timestep : 670 \t\t Reward: -136 \t\t Average Reward : -9.92\n",
      "Episode : 60 \t\t Timestep : 680 \t\t Reward: -127 \t\t Average Reward : -0.11\n",
      "Episode : 61 \t\t Timestep : 690 \t\t Reward: -110 \t\t Average Reward : -17.87\n",
      "Episode : 62 \t\t Timestep : 700 \t\t Reward: -130 \t\t Average Reward : -15.14\n",
      "Episode : 63 \t\t Timestep : 710 \t\t Reward: -150 \t\t Average Reward : -18.5\n",
      "Episode : 64 \t\t Timestep : 720 \t\t Reward: -121 \t\t Average Reward : -21.11\n",
      "Episode : 65 \t\t Timestep : 730 \t\t Reward: -86 \t\t Average Reward : -16.88\n",
      "Episode : 66 \t\t Timestep : 740 \t\t Reward: -173 \t\t Average Reward : -17.88\n",
      "Episode : 67 \t\t Timestep : 750 \t\t Reward: -131 \t\t Average Reward : -19.51\n",
      "Episode : 68 \t\t Timestep : 760 \t\t Reward: -73 \t\t Average Reward : -9.96\n",
      "Episode : 69 \t\t Timestep : 770 \t\t Reward: -139 \t\t Average Reward : -17.97\n",
      "Episode : 70 \t\t Timestep : 780 \t\t Reward: -137 \t\t Average Reward : -18.06\n",
      "Episode : 71 \t\t Timestep : 790 \t\t Reward: -124 \t\t Average Reward : -9.97\n",
      "Episode : 72 \t\t Timestep : 800 \t\t Reward: -136 \t\t Average Reward : -18.04\n",
      "Episode : 73 \t\t Timestep : 810 \t\t Reward: -137 \t\t Average Reward : -26.26\n",
      "Episode : 74 \t\t Timestep : 820 \t\t Reward: -139 \t\t Average Reward : -11.25\n",
      "Episode : 75 \t\t Timestep : 830 \t\t Reward: -148 \t\t Average Reward : 2.88\n",
      "Episode : 76 \t\t Timestep : 840 \t\t Reward: -113 \t\t Average Reward : -14.91\n",
      "Episode : 77 \t\t Timestep : 850 \t\t Reward: -146 \t\t Average Reward : -19.27\n",
      "Episode : 78 \t\t Timestep : 860 \t\t Reward: -95 \t\t Average Reward : -17.08\n",
      "Episode : 79 \t\t Timestep : 870 \t\t Reward: -100 \t\t Average Reward : -18.45\n",
      "Episode : 80 \t\t Timestep : 880 \t\t Reward: -138 \t\t Average Reward : -18.74\n",
      "Episode : 81 \t\t Timestep : 890 \t\t Reward: -1 \t\t Average Reward : -6.06\n",
      "Episode : 82 \t\t Timestep : 900 \t\t Reward: -174 \t\t Average Reward : -17.44\n",
      "Episode : 83 \t\t Timestep : 910 \t\t Reward: -115 \t\t Average Reward : -16.65\n",
      "Episode : 84 \t\t Timestep : 920 \t\t Reward: -89 \t\t Average Reward : -19.25\n",
      "Episode : 85 \t\t Timestep : 930 \t\t Reward: -195 \t\t Average Reward : -19.95\n",
      "Episode : 86 \t\t Timestep : 940 \t\t Reward: -153 \t\t Average Reward : -4.75\n",
      "Episode : 87 \t\t Timestep : 950 \t\t Reward: -132 \t\t Average Reward : -20.14\n",
      "Episode : 88 \t\t Timestep : 960 \t\t Reward: -99 \t\t Average Reward : -13.56\n",
      "Episode : 89 \t\t Timestep : 970 \t\t Reward: -135 \t\t Average Reward : -16.47\n",
      "Episode : 90 \t\t Timestep : 980 \t\t Reward: -135 \t\t Average Reward : -19.97\n",
      "Episode : 91 \t\t Timestep : 990 \t\t Reward: -159 \t\t Average Reward : -18.41\n",
      "Episode : 92 \t\t Timestep : 1000 \t\t Reward: -139 \t\t Average Reward : -15.13\n",
      "Episode : 93 \t\t Timestep : 1010 \t\t Reward: -153 \t\t Average Reward : -14.84\n",
      "Episode : 94 \t\t Timestep : 1020 \t\t Reward: -104 \t\t Average Reward : -18.68\n",
      "Episode : 95 \t\t Timestep : 1030 \t\t Reward: -117 \t\t Average Reward : -28.24\n",
      "Episode : 96 \t\t Timestep : 1040 \t\t Reward: -133 \t\t Average Reward : -16.59\n",
      "Episode : 97 \t\t Timestep : 1050 \t\t Reward: -158 \t\t Average Reward : -1.61\n",
      "Episode : 98 \t\t Timestep : 1060 \t\t Reward: -144 \t\t Average Reward : -18.76\n",
      "Episode : 99 \t\t Timestep : 1070 \t\t Reward: -130 \t\t Average Reward : -19.73\n",
      "Episode : 100 \t\t Timestep : 1080 \t\t Reward: -139 \t\t Average Reward : -20.85\n",
      "Episode : 101 \t\t Timestep : 1090 \t\t Reward: -161 \t\t Average Reward : -9.21\n",
      "Episode : 102 \t\t Timestep : 1100 \t\t Reward: -137 \t\t Average Reward : -18.17\n",
      "Episode : 103 \t\t Timestep : 1110 \t\t Reward: -124 \t\t Average Reward : -23.79\n",
      "Episode : 104 \t\t Timestep : 1120 \t\t Reward: -153 \t\t Average Reward : -12.84\n",
      "Episode : 105 \t\t Timestep : 1130 \t\t Reward: -141 \t\t Average Reward : -12.42\n",
      "Episode : 106 \t\t Timestep : 1140 \t\t Reward: -143 \t\t Average Reward : -15.11\n",
      "Episode : 107 \t\t Timestep : 1150 \t\t Reward: -154 \t\t Average Reward : -14.75\n",
      "Episode : 108 \t\t Timestep : 1160 \t\t Reward: -130 \t\t Average Reward : -17.54\n",
      "Episode : 109 \t\t Timestep : 1170 \t\t Reward: -122 \t\t Average Reward : -18.9\n",
      "Episode : 110 \t\t Timestep : 1180 \t\t Reward: -178 \t\t Average Reward : -18.14\n",
      "Episode : 111 \t\t Timestep : 1190 \t\t Reward: -154 \t\t Average Reward : -19.95\n",
      "Episode : 112 \t\t Timestep : 1200 \t\t Reward: -146 \t\t Average Reward : -17.07\n",
      "Episode : 113 \t\t Timestep : 1210 \t\t Reward: -152 \t\t Average Reward : -18.85\n",
      "Episode : 114 \t\t Timestep : 1220 \t\t Reward: -182 \t\t Average Reward : -16.05\n",
      "Episode : 115 \t\t Timestep : 1230 \t\t Reward: -130 \t\t Average Reward : -16.93\n",
      "Episode : 116 \t\t Timestep : 1240 \t\t Reward: -160 \t\t Average Reward : -14.84\n",
      "Episode : 117 \t\t Timestep : 1250 \t\t Reward: -113 \t\t Average Reward : -24.49\n",
      "Episode : 118 \t\t Timestep : 1260 \t\t Reward: -140 \t\t Average Reward : -19.41\n",
      "Episode : 119 \t\t Timestep : 1270 \t\t Reward: -126 \t\t Average Reward : -22.0\n",
      "Episode : 120 \t\t Timestep : 1280 \t\t Reward: -139 \t\t Average Reward : -19.69\n",
      "Episode : 121 \t\t Timestep : 1290 \t\t Reward: -155 \t\t Average Reward : -14.92\n",
      "Episode : 122 \t\t Timestep : 1300 \t\t Reward: -201 \t\t Average Reward : -10.93\n",
      "Episode : 123 \t\t Timestep : 1310 \t\t Reward: -126 \t\t Average Reward : -17.12\n",
      "Episode : 124 \t\t Timestep : 1320 \t\t Reward: -147 \t\t Average Reward : -6.15\n",
      "Episode : 125 \t\t Timestep : 1330 \t\t Reward: -158 \t\t Average Reward : -11.48\n",
      "Episode : 126 \t\t Timestep : 1340 \t\t Reward: -134 \t\t Average Reward : -18.75\n",
      "Episode : 127 \t\t Timestep : 1350 \t\t Reward: -126 \t\t Average Reward : -17.6\n",
      "Episode : 128 \t\t Timestep : 1360 \t\t Reward: -150 \t\t Average Reward : -21.82\n",
      "Episode : 129 \t\t Timestep : 1370 \t\t Reward: -161 \t\t Average Reward : -17.38\n",
      "Episode : 130 \t\t Timestep : 1380 \t\t Reward: -131 \t\t Average Reward : -20.2\n",
      "Episode : 131 \t\t Timestep : 1390 \t\t Reward: -134 \t\t Average Reward : -12.85\n",
      "Episode : 132 \t\t Timestep : 1400 \t\t Reward: -109 \t\t Average Reward : -20.93\n",
      "Episode : 133 \t\t Timestep : 1410 \t\t Reward: -138 \t\t Average Reward : -15.8\n",
      "Episode : 134 \t\t Timestep : 1420 \t\t Reward: -115 \t\t Average Reward : -11.91\n",
      "Episode : 135 \t\t Timestep : 1430 \t\t Reward: -16 \t\t Average Reward : -18.32\n",
      "Episode : 136 \t\t Timestep : 1440 \t\t Reward: -124 \t\t Average Reward : -14.7\n",
      "Episode : 137 \t\t Timestep : 1450 \t\t Reward: -176 \t\t Average Reward : 1.27\n",
      "Episode : 138 \t\t Timestep : 1460 \t\t Reward: -143 \t\t Average Reward : -19.39\n",
      "Episode : 139 \t\t Timestep : 1470 \t\t Reward: -163 \t\t Average Reward : -20.38\n",
      "Episode : 140 \t\t Timestep : 1480 \t\t Reward: -128 \t\t Average Reward : -16.2\n",
      "Episode : 141 \t\t Timestep : 1490 \t\t Reward: -147 \t\t Average Reward : -21.85\n",
      "Episode : 142 \t\t Timestep : 1500 \t\t Reward: -135 \t\t Average Reward : -18.73\n",
      "Episode : 143 \t\t Timestep : 1510 \t\t Reward: -103 \t\t Average Reward : -24.18\n",
      "Episode : 144 \t\t Timestep : 1520 \t\t Reward: -134 \t\t Average Reward : -22.45\n",
      "Episode : 145 \t\t Timestep : 1530 \t\t Reward: -108 \t\t Average Reward : -17.54\n",
      "Episode : 146 \t\t Timestep : 1540 \t\t Reward: -129 \t\t Average Reward : -19.46\n",
      "Episode : 147 \t\t Timestep : 1550 \t\t Reward: -123 \t\t Average Reward : -20.44\n",
      "Episode : 148 \t\t Timestep : 1560 \t\t Reward: -114 \t\t Average Reward : -13.95\n",
      "Episode : 149 \t\t Timestep : 1570 \t\t Reward: -150 \t\t Average Reward : -1.51\n",
      "Episode : 150 \t\t Timestep : 1580 \t\t Reward: -144 \t\t Average Reward : -18.83\n",
      "Episode : 151 \t\t Timestep : 1590 \t\t Reward: -160 \t\t Average Reward : -18.66\n",
      "Episode : 152 \t\t Timestep : 1600 \t\t Reward: -122 \t\t Average Reward : -15.16\n",
      "Episode : 153 \t\t Timestep : 1610 \t\t Reward: -170 \t\t Average Reward : -18.57\n",
      "Episode : 154 \t\t Timestep : 1620 \t\t Reward: -154 \t\t Average Reward : -17.75\n",
      "Episode : 155 \t\t Timestep : 1630 \t\t Reward: -117 \t\t Average Reward : -15.53\n",
      "Episode : 156 \t\t Timestep : 1640 \t\t Reward: -144 \t\t Average Reward : -16.95\n",
      "Episode : 157 \t\t Timestep : 1650 \t\t Reward: -177 \t\t Average Reward : -17.26\n",
      "Episode : 158 \t\t Timestep : 1660 \t\t Reward: -131 \t\t Average Reward : -13.14\n",
      "Episode : 159 \t\t Timestep : 1670 \t\t Reward: -136 \t\t Average Reward : -16.6\n",
      "Episode : 160 \t\t Timestep : 1680 \t\t Reward: -97 \t\t Average Reward : -17.29\n",
      "Episode : 161 \t\t Timestep : 1690 \t\t Reward: -42 \t\t Average Reward : -6.39\n",
      "Updating the Model\n",
      "Episode : 162 \t\t Timestep : 2090 \t\t Reward: -141 \t\t Average Reward : 80.7\n",
      "Updating the Model\n",
      "Episode : 163 \t\t Timestep : 2100 \t\t Reward: -481 \t\t Average Reward : -17.17\n",
      "Episode : 164 \t\t Timestep : 2110 \t\t Reward: -463 \t\t Average Reward : -11.7\n",
      "Episode : 165 \t\t Timestep : 2120 \t\t Reward: -491 \t\t Average Reward : -32.99\n",
      "Episode : 166 \t\t Timestep : 2130 \t\t Reward: -550 \t\t Average Reward : -4.99\n",
      "Episode : 167 \t\t Timestep : 2140 \t\t Reward: -409 \t\t Average Reward : -35.43\n",
      "Updating the Model\n",
      "Episode : 168 \t\t Timestep : 2150 \t\t Reward: -525 \t\t Average Reward : -6.05\n",
      "Episode : 169 \t\t Timestep : 2160 \t\t Reward: -468 \t\t Average Reward : -17.36\n",
      "Episode : 170 \t\t Timestep : 2170 \t\t Reward: -432 \t\t Average Reward : -32.09\n",
      "Episode : 171 \t\t Timestep : 2180 \t\t Reward: -553 \t\t Average Reward : -4.91\n",
      "Episode : 172 \t\t Timestep : 2190 \t\t Reward: -458 \t\t Average Reward : -33.69\n",
      "Updating the Model\n",
      "Episode : 173 \t\t Timestep : 2200 \t\t Reward: -542 \t\t Average Reward : -35.07\n",
      "Episode : 174 \t\t Timestep : 2210 \t\t Reward: -569 \t\t Average Reward : -13.68\n",
      "Episode : 175 \t\t Timestep : 2220 \t\t Reward: -605 \t\t Average Reward : -19.36\n",
      "Episode : 176 \t\t Timestep : 2230 \t\t Reward: -490 \t\t Average Reward : 5.23\n",
      "Episode : 177 \t\t Timestep : 2240 \t\t Reward: -625 \t\t Average Reward : -15.45\n",
      "Updating the Model\n",
      "Episode : 178 \t\t Timestep : 2250 \t\t Reward: -586 \t\t Average Reward : -20.98\n",
      "Episode : 179 \t\t Timestep : 2260 \t\t Reward: -463 \t\t Average Reward : 1.82\n",
      "Episode : 180 \t\t Timestep : 2270 \t\t Reward: -481 \t\t Average Reward : -13.19\n",
      "Episode : 181 \t\t Timestep : 2280 \t\t Reward: -612 \t\t Average Reward : 1.22\n",
      "Episode : 182 \t\t Timestep : 2290 \t\t Reward: -570 \t\t Average Reward : -26.19\n",
      "Updating the Model\n",
      "Episode : 183 \t\t Timestep : 2300 \t\t Reward: -628 \t\t Average Reward : 1.9\n",
      "Episode : 184 \t\t Timestep : 2310 \t\t Reward: -772 \t\t Average Reward : -23.52\n",
      "Episode : 185 \t\t Timestep : 2320 \t\t Reward: -468 \t\t Average Reward : -34.4\n",
      "Episode : 186 \t\t Timestep : 2330 \t\t Reward: -652 \t\t Average Reward : -10.32\n",
      "Episode : 187 \t\t Timestep : 2340 \t\t Reward: -723 \t\t Average Reward : -34.06\n",
      "Updating the Model\n",
      "Episode : 188 \t\t Timestep : 2350 \t\t Reward: -570 \t\t Average Reward : -38.84\n",
      "Episode : 189 \t\t Timestep : 2360 \t\t Reward: -738 \t\t Average Reward : -4.99\n",
      "Episode : 190 \t\t Timestep : 2370 \t\t Reward: -437 \t\t Average Reward : -32.63\n",
      "Episode : 191 \t\t Timestep : 2380 \t\t Reward: -432 \t\t Average Reward : 2.45\n",
      "Episode : 192 \t\t Timestep : 2390 \t\t Reward: -804 \t\t Average Reward : -25.31\n",
      "Updating the Model\n",
      "Episode : 193 \t\t Timestep : 2400 \t\t Reward: -653 \t\t Average Reward : -31.3\n",
      "Episode : 194 \t\t Timestep : 2410 \t\t Reward: -439 \t\t Average Reward : -27.24\n",
      "Episode : 195 \t\t Timestep : 2420 \t\t Reward: -471 \t\t Average Reward : -39.15\n",
      "Episode : 196 \t\t Timestep : 2430 \t\t Reward: -471 \t\t Average Reward : -3.66\n",
      "Episode : 197 \t\t Timestep : 2440 \t\t Reward: -458 \t\t Average Reward : -10.24\n",
      "Updating the Model\n",
      "Episode : 198 \t\t Timestep : 2450 \t\t Reward: -541 \t\t Average Reward : -31.97\n",
      "Episode : 199 \t\t Timestep : 2460 \t\t Reward: -659 \t\t Average Reward : -32.75\n",
      "Episode : 200 \t\t Timestep : 2470 \t\t Reward: -562 \t\t Average Reward : -31.53\n",
      "Episode : 201 \t\t Timestep : 2480 \t\t Reward: -596 \t\t Average Reward : -8.59\n",
      "Episode : 202 \t\t Timestep : 2490 \t\t Reward: -618 \t\t Average Reward : -12.85\n",
      "Updating the Model\n",
      "Episode : 203 \t\t Timestep : 2500 \t\t Reward: -523 \t\t Average Reward : 1.25\n",
      "Episode : 204 \t\t Timestep : 2510 \t\t Reward: -666 \t\t Average Reward : -9.57\n",
      "Episode : 205 \t\t Timestep : 2520 \t\t Reward: -673 \t\t Average Reward : -30.33\n",
      "Episode : 206 \t\t Timestep : 2530 \t\t Reward: -652 \t\t Average Reward : -36.12\n",
      "Episode : 207 \t\t Timestep : 2540 \t\t Reward: -680 \t\t Average Reward : -5.04\n",
      "Updating the Model\n",
      "Episode : 208 \t\t Timestep : 2550 \t\t Reward: -651 \t\t Average Reward : -10.86\n",
      "Episode : 209 \t\t Timestep : 2560 \t\t Reward: -587 \t\t Average Reward : -35.82\n",
      "Episode : 210 \t\t Timestep : 2570 \t\t Reward: -407 \t\t Average Reward : -35.49\n",
      "Episode : 211 \t\t Timestep : 2580 \t\t Reward: -669 \t\t Average Reward : 1.08\n",
      "Episode : 212 \t\t Timestep : 2590 \t\t Reward: -557 \t\t Average Reward : 4.69\n",
      "Updating the Model\n",
      "Episode : 213 \t\t Timestep : 2600 \t\t Reward: -605 \t\t Average Reward : -5.97\n",
      "Episode : 214 \t\t Timestep : 2610 \t\t Reward: -565 \t\t Average Reward : -34.79\n",
      "Episode : 215 \t\t Timestep : 2620 \t\t Reward: -683 \t\t Average Reward : -4.35\n",
      "Episode : 216 \t\t Timestep : 2630 \t\t Reward: -656 \t\t Average Reward : -26.62\n",
      "Episode : 217 \t\t Timestep : 2640 \t\t Reward: -407 \t\t Average Reward : -35.3\n",
      "Updating the Model\n",
      "Episode : 218 \t\t Timestep : 2650 \t\t Reward: -350 \t\t Average Reward : -7.43\n",
      "Episode : 219 \t\t Timestep : 2660 \t\t Reward: -679 \t\t Average Reward : -2.31\n",
      "Episode : 220 \t\t Timestep : 2670 \t\t Reward: -528 \t\t Average Reward : -8.08\n",
      "Episode : 221 \t\t Timestep : 2680 \t\t Reward: -499 \t\t Average Reward : -5.34\n",
      "Episode : 222 \t\t Timestep : 2690 \t\t Reward: -437 \t\t Average Reward : -21.15\n",
      "Updating the Model\n",
      "Episode : 223 \t\t Timestep : 2700 \t\t Reward: -665 \t\t Average Reward : -16.19\n",
      "Episode : 224 \t\t Timestep : 2710 \t\t Reward: -488 \t\t Average Reward : -8.32\n",
      "Episode : 225 \t\t Timestep : 2720 \t\t Reward: -494 \t\t Average Reward : -33.0\n",
      "Episode : 226 \t\t Timestep : 2730 \t\t Reward: -479 \t\t Average Reward : -8.59\n",
      "Episode : 227 \t\t Timestep : 2740 \t\t Reward: -576 \t\t Average Reward : -37.73\n",
      "Updating the Model\n",
      "Episode : 228 \t\t Timestep : 2750 \t\t Reward: -511 \t\t Average Reward : -37.87\n",
      "Episode : 229 \t\t Timestep : 2760 \t\t Reward: -509 \t\t Average Reward : -35.94\n",
      "Episode : 230 \t\t Timestep : 2770 \t\t Reward: -630 \t\t Average Reward : -4.04\n",
      "Episode : 231 \t\t Timestep : 2780 \t\t Reward: -534 \t\t Average Reward : 1.02\n",
      "Episode : 232 \t\t Timestep : 2790 \t\t Reward: -618 \t\t Average Reward : -35.84\n",
      "Updating the Model\n",
      "Episode : 233 \t\t Timestep : 2800 \t\t Reward: -601 \t\t Average Reward : -26.56\n",
      "Episode : 234 \t\t Timestep : 2810 \t\t Reward: -409 \t\t Average Reward : -8.1\n",
      "Episode : 235 \t\t Timestep : 2820 \t\t Reward: -528 \t\t Average Reward : -33.94\n",
      "Episode : 236 \t\t Timestep : 2830 \t\t Reward: -607 \t\t Average Reward : -9.59\n",
      "Episode : 237 \t\t Timestep : 2840 \t\t Reward: -526 \t\t Average Reward : -8.1\n",
      "Updating the Model\n",
      "Episode : 238 \t\t Timestep : 2850 \t\t Reward: -549 \t\t Average Reward : -33.83\n",
      "Episode : 239 \t\t Timestep : 2860 \t\t Reward: -554 \t\t Average Reward : -28.34\n",
      "Episode : 240 \t\t Timestep : 2870 \t\t Reward: -465 \t\t Average Reward : -26.43\n",
      "Episode : 241 \t\t Timestep : 2880 \t\t Reward: -487 \t\t Average Reward : 2.36\n",
      "Episode : 242 \t\t Timestep : 2890 \t\t Reward: -410 \t\t Average Reward : -36.04\n",
      "Updating the Model\n",
      "Episode : 243 \t\t Timestep : 2900 \t\t Reward: -613 \t\t Average Reward : -3.62\n",
      "Episode : 244 \t\t Timestep : 2910 \t\t Reward: -621 \t\t Average Reward : 0.6\n",
      "Episode : 245 \t\t Timestep : 2920 \t\t Reward: -636 \t\t Average Reward : -36.35\n",
      "Episode : 246 \t\t Timestep : 2930 \t\t Reward: -491 \t\t Average Reward : -34.6\n",
      "Episode : 247 \t\t Timestep : 2940 \t\t Reward: -490 \t\t Average Reward : -5.11\n",
      "Updating the Model\n",
      "Episode : 248 \t\t Timestep : 2950 \t\t Reward: -416 \t\t Average Reward : -6.3\n",
      "Episode : 249 \t\t Timestep : 2960 \t\t Reward: -536 \t\t Average Reward : -31.02\n",
      "Episode : 250 \t\t Timestep : 2970 \t\t Reward: -460 \t\t Average Reward : -36.02\n",
      "Episode : 251 \t\t Timestep : 2980 \t\t Reward: -579 \t\t Average Reward : 1.41\n",
      "Episode : 252 \t\t Timestep : 2990 \t\t Reward: -556 \t\t Average Reward : -6.77\n",
      "Updating the Model\n",
      "Episode : 253 \t\t Timestep : 3000 \t\t Reward: -527 \t\t Average Reward : -23.73\n",
      "Episode : 254 \t\t Timestep : 3010 \t\t Reward: -781 \t\t Average Reward : -36.38\n",
      "Episode : 255 \t\t Timestep : 3020 \t\t Reward: -555 \t\t Average Reward : -33.13\n",
      "Episode : 256 \t\t Timestep : 3030 \t\t Reward: -682 \t\t Average Reward : -29.88\n",
      "Episode : 257 \t\t Timestep : 3040 \t\t Reward: -563 \t\t Average Reward : -35.28\n",
      "Updating the Model\n",
      "Episode : 258 \t\t Timestep : 3050 \t\t Reward: -858 \t\t Average Reward : 1.22\n",
      "Episode : 259 \t\t Timestep : 3060 \t\t Reward: -720 \t\t Average Reward : -17.26\n",
      "Episode : 260 \t\t Timestep : 3070 \t\t Reward: -528 \t\t Average Reward : -7.43\n",
      "Episode : 261 \t\t Timestep : 3080 \t\t Reward: -883 \t\t Average Reward : -4.46\n",
      "Episode : 262 \t\t Timestep : 3090 \t\t Reward: -501 \t\t Average Reward : -15.91\n",
      "Updating the Model\n",
      "Episode : 263 \t\t Timestep : 3100 \t\t Reward: -579 \t\t Average Reward : -0.3\n",
      "Episode : 264 \t\t Timestep : 3110 \t\t Reward: -643 \t\t Average Reward : 4.17\n",
      "Episode : 265 \t\t Timestep : 3120 \t\t Reward: -604 \t\t Average Reward : -34.26\n",
      "Episode : 266 \t\t Timestep : 3130 \t\t Reward: -672 \t\t Average Reward : 0.28\n",
      "Episode : 267 \t\t Timestep : 3140 \t\t Reward: -598 \t\t Average Reward : -34.23\n",
      "Updating the Model\n",
      "Episode : 268 \t\t Timestep : 3150 \t\t Reward: -515 \t\t Average Reward : -36.52\n",
      "Episode : 269 \t\t Timestep : 3160 \t\t Reward: -657 \t\t Average Reward : -30.26\n",
      "Episode : 270 \t\t Timestep : 3170 \t\t Reward: -733 \t\t Average Reward : -11.69\n",
      "Episode : 271 \t\t Timestep : 3180 \t\t Reward: -385 \t\t Average Reward : -37.76\n",
      "Episode : 272 \t\t Timestep : 3190 \t\t Reward: -660 \t\t Average Reward : 1.18\n",
      "Updating the Model\n",
      "Episode : 273 \t\t Timestep : 3200 \t\t Reward: -491 \t\t Average Reward : -30.83\n",
      "Episode : 274 \t\t Timestep : 3210 \t\t Reward: -650 \t\t Average Reward : -22.72\n",
      "Episode : 275 \t\t Timestep : 3220 \t\t Reward: -694 \t\t Average Reward : -21.23\n",
      "Episode : 276 \t\t Timestep : 3230 \t\t Reward: -615 \t\t Average Reward : -30.98\n",
      "Episode : 277 \t\t Timestep : 3240 \t\t Reward: -701 \t\t Average Reward : -8.93\n",
      "Updating the Model\n",
      "Episode : 278 \t\t Timestep : 3250 \t\t Reward: -485 \t\t Average Reward : -5.97\n",
      "Episode : 279 \t\t Timestep : 3260 \t\t Reward: -417 \t\t Average Reward : -33.31\n",
      "Episode : 280 \t\t Timestep : 3270 \t\t Reward: -716 \t\t Average Reward : -21.98\n",
      "Episode : 281 \t\t Timestep : 3280 \t\t Reward: -666 \t\t Average Reward : -19.14\n",
      "Episode : 282 \t\t Timestep : 3290 \t\t Reward: -654 \t\t Average Reward : -34.22\n",
      "Updating the Model\n",
      "Episode : 283 \t\t Timestep : 3300 \t\t Reward: -563 \t\t Average Reward : 0.44\n",
      "Episode : 284 \t\t Timestep : 3310 \t\t Reward: -642 \t\t Average Reward : -31.11\n",
      "Episode : 285 \t\t Timestep : 3320 \t\t Reward: -463 \t\t Average Reward : -12.31\n",
      "Episode : 286 \t\t Timestep : 3330 \t\t Reward: -470 \t\t Average Reward : -32.63\n",
      "Episode : 287 \t\t Timestep : 3340 \t\t Reward: -607 \t\t Average Reward : -14.55\n",
      "Updating the Model\n",
      "Episode : 288 \t\t Timestep : 3350 \t\t Reward: -788 \t\t Average Reward : 0.0\n",
      "Episode : 289 \t\t Timestep : 3360 \t\t Reward: -567 \t\t Average Reward : -26.16\n",
      "Episode : 290 \t\t Timestep : 3370 \t\t Reward: -541 \t\t Average Reward : -36.07\n",
      "Episode : 291 \t\t Timestep : 3380 \t\t Reward: -750 \t\t Average Reward : -37.98\n",
      "Episode : 292 \t\t Timestep : 3390 \t\t Reward: -510 \t\t Average Reward : -7.59\n",
      "Updating the Model\n",
      "Episode : 293 \t\t Timestep : 3400 \t\t Reward: -645 \t\t Average Reward : -35.43\n",
      "Episode : 294 \t\t Timestep : 3410 \t\t Reward: -636 \t\t Average Reward : -28.06\n",
      "Episode : 295 \t\t Timestep : 3420 \t\t Reward: -564 \t\t Average Reward : -25.01\n",
      "Episode : 296 \t\t Timestep : 3430 \t\t Reward: -531 \t\t Average Reward : -12.43\n",
      "Episode : 297 \t\t Timestep : 3440 \t\t Reward: -669 \t\t Average Reward : 0.84\n",
      "Updating the Model\n",
      "Episode : 298 \t\t Timestep : 3450 \t\t Reward: -520 \t\t Average Reward : -35.43\n",
      "Episode : 299 \t\t Timestep : 3460 \t\t Reward: -611 \t\t Average Reward : -36.86\n",
      "Episode : 300 \t\t Timestep : 3470 \t\t Reward: -556 \t\t Average Reward : -15.12\n",
      "Episode : 301 \t\t Timestep : 3480 \t\t Reward: -367 \t\t Average Reward : 2.32\n",
      "Episode : 302 \t\t Timestep : 3490 \t\t Reward: -610 \t\t Average Reward : -34.02\n",
      "Updating the Model\n",
      "Episode : 303 \t\t Timestep : 3500 \t\t Reward: -566 \t\t Average Reward : -6.85\n",
      "Episode : 304 \t\t Timestep : 3510 \t\t Reward: -491 \t\t Average Reward : -32.85\n",
      "Episode : 305 \t\t Timestep : 3520 \t\t Reward: -484 \t\t Average Reward : -37.14\n",
      "Episode : 306 \t\t Timestep : 3530 \t\t Reward: -613 \t\t Average Reward : -0.98\n",
      "Episode : 307 \t\t Timestep : 3540 \t\t Reward: -517 \t\t Average Reward : -38.24\n",
      "Updating the Model\n",
      "Episode : 308 \t\t Timestep : 3550 \t\t Reward: -573 \t\t Average Reward : -31.48\n",
      "Episode : 309 \t\t Timestep : 3560 \t\t Reward: -458 \t\t Average Reward : -38.28\n",
      "Episode : 310 \t\t Timestep : 3570 \t\t Reward: -408 \t\t Average Reward : -6.9\n",
      "Episode : 311 \t\t Timestep : 3580 \t\t Reward: -693 \t\t Average Reward : -7.91\n",
      "Episode : 312 \t\t Timestep : 3590 \t\t Reward: -666 \t\t Average Reward : -34.0\n",
      "Updating the Model\n",
      "Episode : 313 \t\t Timestep : 3600 \t\t Reward: -650 \t\t Average Reward : -32.29\n",
      "Episode : 314 \t\t Timestep : 3610 \t\t Reward: -582 \t\t Average Reward : -32.62\n",
      "Episode : 315 \t\t Timestep : 3620 \t\t Reward: -459 \t\t Average Reward : -25.93\n",
      "Episode : 316 \t\t Timestep : 3630 \t\t Reward: -463 \t\t Average Reward : -21.39\n",
      "Episode : 317 \t\t Timestep : 3640 \t\t Reward: -503 \t\t Average Reward : -31.77\n",
      "Updating the Model\n",
      "Episode : 318 \t\t Timestep : 3650 \t\t Reward: -668 \t\t Average Reward : -36.71\n",
      "Episode : 319 \t\t Timestep : 3660 \t\t Reward: -424 \t\t Average Reward : -30.46\n",
      "Episode : 320 \t\t Timestep : 3670 \t\t Reward: -552 \t\t Average Reward : -34.07\n",
      "Episode : 321 \t\t Timestep : 3680 \t\t Reward: -684 \t\t Average Reward : -15.99\n",
      "Episode : 322 \t\t Timestep : 3690 \t\t Reward: -482 \t\t Average Reward : -23.08\n",
      "Updating the Model\n",
      "Episode : 323 \t\t Timestep : 3700 \t\t Reward: -501 \t\t Average Reward : 2.81\n",
      "Episode : 324 \t\t Timestep : 3710 \t\t Reward: -633 \t\t Average Reward : -36.53\n",
      "Episode : 325 \t\t Timestep : 3720 \t\t Reward: -413 \t\t Average Reward : -36.73\n",
      "Episode : 326 \t\t Timestep : 3730 \t\t Reward: -579 \t\t Average Reward : -4.47\n",
      "Episode : 327 \t\t Timestep : 3740 \t\t Reward: -814 \t\t Average Reward : -36.24\n",
      "Updating the Model\n",
      "Episode : 328 \t\t Timestep : 3750 \t\t Reward: -511 \t\t Average Reward : 0.57\n",
      "Episode : 329 \t\t Timestep : 3760 \t\t Reward: -612 \t\t Average Reward : -39.37\n",
      "Episode : 330 \t\t Timestep : 3770 \t\t Reward: -669 \t\t Average Reward : -37.23\n",
      "Episode : 331 \t\t Timestep : 3780 \t\t Reward: -599 \t\t Average Reward : -0.91\n",
      "Episode : 332 \t\t Timestep : 3790 \t\t Reward: -551 \t\t Average Reward : -20.42\n",
      "Updating the Model\n",
      "Episode : 333 \t\t Timestep : 3800 \t\t Reward: -624 \t\t Average Reward : -29.75\n",
      "Episode : 334 \t\t Timestep : 3810 \t\t Reward: -655 \t\t Average Reward : -25.41\n",
      "Episode : 335 \t\t Timestep : 3820 \t\t Reward: -753 \t\t Average Reward : -3.55\n",
      "Episode : 336 \t\t Timestep : 3830 \t\t Reward: -466 \t\t Average Reward : -32.51\n",
      "Episode : 337 \t\t Timestep : 3840 \t\t Reward: -529 \t\t Average Reward : -27.22\n",
      "Updating the Model\n",
      "Episode : 338 \t\t Timestep : 3850 \t\t Reward: -769 \t\t Average Reward : -4.13\n",
      "Episode : 339 \t\t Timestep : 3860 \t\t Reward: -490 \t\t Average Reward : -32.73\n",
      "Episode : 340 \t\t Timestep : 3870 \t\t Reward: -651 \t\t Average Reward : -17.78\n",
      "Episode : 341 \t\t Timestep : 3880 \t\t Reward: -610 \t\t Average Reward : -30.18\n",
      "Episode : 342 \t\t Timestep : 3890 \t\t Reward: -544 \t\t Average Reward : -10.44\n",
      "Updating the Model\n",
      "Episode : 343 \t\t Timestep : 3900 \t\t Reward: -526 \t\t Average Reward : -36.28\n",
      "Episode : 344 \t\t Timestep : 3910 \t\t Reward: -634 \t\t Average Reward : -31.76\n",
      "Episode : 345 \t\t Timestep : 3920 \t\t Reward: -410 \t\t Average Reward : -11.87\n",
      "Episode : 346 \t\t Timestep : 3930 \t\t Reward: -638 \t\t Average Reward : -19.74\n",
      "Episode : 347 \t\t Timestep : 3940 \t\t Reward: -604 \t\t Average Reward : -40.28\n",
      "Updating the Model\n",
      "Episode : 348 \t\t Timestep : 3950 \t\t Reward: -600 \t\t Average Reward : -17.14\n",
      "Episode : 349 \t\t Timestep : 3960 \t\t Reward: -555 \t\t Average Reward : -10.72\n",
      "Episode : 350 \t\t Timestep : 3970 \t\t Reward: -555 \t\t Average Reward : -24.82\n",
      "Episode : 351 \t\t Timestep : 3980 \t\t Reward: -475 \t\t Average Reward : -29.19\n",
      "Episode : 352 \t\t Timestep : 3990 \t\t Reward: -566 \t\t Average Reward : -26.06\n",
      "Updating the Model\n",
      "Episode : 353 \t\t Timestep : 4000 \t\t Reward: -521 \t\t Average Reward : -32.84\n",
      "Episode : 354 \t\t Timestep : 4010 \t\t Reward: -611 \t\t Average Reward : -15.99\n",
      "Episode : 355 \t\t Timestep : 4020 \t\t Reward: -514 \t\t Average Reward : -36.57\n",
      "Episode : 356 \t\t Timestep : 4030 \t\t Reward: -677 \t\t Average Reward : -27.55\n",
      "Episode : 357 \t\t Timestep : 4040 \t\t Reward: -682 \t\t Average Reward : -37.32\n",
      "Updating the Model\n",
      "Episode : 358 \t\t Timestep : 4050 \t\t Reward: -565 \t\t Average Reward : -34.92\n",
      "Episode : 359 \t\t Timestep : 4060 \t\t Reward: -667 \t\t Average Reward : 1.3\n",
      "Episode : 360 \t\t Timestep : 4070 \t\t Reward: -550 \t\t Average Reward : -38.27\n",
      "Episode : 361 \t\t Timestep : 4080 \t\t Reward: -570 \t\t Average Reward : -0.7\n",
      "Episode : 362 \t\t Timestep : 4090 \t\t Reward: -750 \t\t Average Reward : -32.32\n",
      "Updating the Model\n",
      "Episode : 363 \t\t Timestep : 4100 \t\t Reward: -643 \t\t Average Reward : -34.57\n",
      "Episode : 364 \t\t Timestep : 4110 \t\t Reward: -534 \t\t Average Reward : -35.08\n",
      "Episode : 365 \t\t Timestep : 4120 \t\t Reward: -606 \t\t Average Reward : 1.65\n",
      "Episode : 366 \t\t Timestep : 4130 \t\t Reward: -406 \t\t Average Reward : -5.41\n",
      "Episode : 367 \t\t Timestep : 4140 \t\t Reward: -444 \t\t Average Reward : -34.23\n",
      "Updating the Model\n",
      "Episode : 368 \t\t Timestep : 4150 \t\t Reward: -631 \t\t Average Reward : -5.44\n",
      "Episode : 369 \t\t Timestep : 4160 \t\t Reward: -515 \t\t Average Reward : -5.77\n",
      "Episode : 370 \t\t Timestep : 4170 \t\t Reward: -709 \t\t Average Reward : -33.78\n",
      "Episode : 371 \t\t Timestep : 4180 \t\t Reward: -492 \t\t Average Reward : -6.83\n",
      "Episode : 372 \t\t Timestep : 4190 \t\t Reward: -419 \t\t Average Reward : -7.15\n",
      "Updating the Model\n",
      "Episode : 373 \t\t Timestep : 4200 \t\t Reward: -470 \t\t Average Reward : -29.85\n",
      "Episode : 374 \t\t Timestep : 4210 \t\t Reward: -618 \t\t Average Reward : 1.44\n",
      "Episode : 375 \t\t Timestep : 4220 \t\t Reward: -547 \t\t Average Reward : -35.68\n",
      "Episode : 376 \t\t Timestep : 4230 \t\t Reward: -698 \t\t Average Reward : -25.25\n",
      "Episode : 377 \t\t Timestep : 4240 \t\t Reward: -506 \t\t Average Reward : -7.07\n",
      "Updating the Model\n",
      "Episode : 378 \t\t Timestep : 4250 \t\t Reward: -580 \t\t Average Reward : 2.01\n",
      "Episode : 379 \t\t Timestep : 4260 \t\t Reward: -676 \t\t Average Reward : -37.72\n",
      "Episode : 380 \t\t Timestep : 4270 \t\t Reward: -572 \t\t Average Reward : -37.74\n",
      "Episode : 381 \t\t Timestep : 4280 \t\t Reward: -493 \t\t Average Reward : -8.46\n",
      "Episode : 382 \t\t Timestep : 4290 \t\t Reward: -696 \t\t Average Reward : 0.17\n",
      "Updating the Model\n",
      "Episode : 383 \t\t Timestep : 4300 \t\t Reward: -483 \t\t Average Reward : -32.53\n",
      "Episode : 384 \t\t Timestep : 4310 \t\t Reward: -572 \t\t Average Reward : 0.93\n",
      "Episode : 385 \t\t Timestep : 4320 \t\t Reward: -580 \t\t Average Reward : -36.22\n",
      "Episode : 386 \t\t Timestep : 4330 \t\t Reward: -471 \t\t Average Reward : -8.44\n",
      "Episode : 387 \t\t Timestep : 4340 \t\t Reward: -488 \t\t Average Reward : -15.07\n",
      "Updating the Model\n",
      "Episode : 388 \t\t Timestep : 4350 \t\t Reward: -530 \t\t Average Reward : -30.85\n",
      "Episode : 389 \t\t Timestep : 4360 \t\t Reward: -569 \t\t Average Reward : 1.67\n",
      "Episode : 390 \t\t Timestep : 4370 \t\t Reward: -649 \t\t Average Reward : -20.74\n",
      "Episode : 391 \t\t Timestep : 4380 \t\t Reward: -447 \t\t Average Reward : -26.23\n",
      "Episode : 392 \t\t Timestep : 4390 \t\t Reward: -477 \t\t Average Reward : -32.68\n",
      "Updating the Model\n",
      "Episode : 393 \t\t Timestep : 4400 \t\t Reward: -579 \t\t Average Reward : -36.45\n",
      "Episode : 394 \t\t Timestep : 4410 \t\t Reward: -641 \t\t Average Reward : -38.76\n",
      "Episode : 395 \t\t Timestep : 4420 \t\t Reward: -600 \t\t Average Reward : -1.34\n",
      "Episode : 396 \t\t Timestep : 4430 \t\t Reward: -411 \t\t Average Reward : -6.1\n",
      "Episode : 397 \t\t Timestep : 4440 \t\t Reward: -778 \t\t Average Reward : -35.59\n",
      "Updating the Model\n",
      "Episode : 398 \t\t Timestep : 4450 \t\t Reward: -613 \t\t Average Reward : -36.78\n",
      "Episode : 399 \t\t Timestep : 4460 \t\t Reward: -448 \t\t Average Reward : -30.08\n",
      "Episode : 400 \t\t Timestep : 4470 \t\t Reward: -542 \t\t Average Reward : -24.1\n",
      "Episode : 401 \t\t Timestep : 4480 \t\t Reward: -680 \t\t Average Reward : -32.14\n",
      "Episode : 402 \t\t Timestep : 4490 \t\t Reward: -611 \t\t Average Reward : -33.71\n",
      "Updating the Model\n",
      "Episode : 403 \t\t Timestep : 4500 \t\t Reward: -635 \t\t Average Reward : -22.93\n",
      "Episode : 404 \t\t Timestep : 4510 \t\t Reward: -480 \t\t Average Reward : -12.31\n",
      "Episode : 405 \t\t Timestep : 4520 \t\t Reward: -657 \t\t Average Reward : -14.71\n",
      "Episode : 406 \t\t Timestep : 4530 \t\t Reward: -480 \t\t Average Reward : -11.78\n",
      "Episode : 407 \t\t Timestep : 4540 \t\t Reward: -537 \t\t Average Reward : -10.62\n",
      "Updating the Model\n",
      "Episode : 408 \t\t Timestep : 4550 \t\t Reward: -603 \t\t Average Reward : -33.05\n",
      "Episode : 409 \t\t Timestep : 4560 \t\t Reward: -695 \t\t Average Reward : -10.54\n",
      "Episode : 410 \t\t Timestep : 4570 \t\t Reward: -580 \t\t Average Reward : -33.99\n",
      "Episode : 411 \t\t Timestep : 4580 \t\t Reward: -654 \t\t Average Reward : -13.9\n",
      "Episode : 412 \t\t Timestep : 4590 \t\t Reward: -651 \t\t Average Reward : -32.86\n",
      "Updating the Model\n",
      "Episode : 413 \t\t Timestep : 4600 \t\t Reward: -704 \t\t Average Reward : -39.77\n",
      "Episode : 414 \t\t Timestep : 4610 \t\t Reward: -533 \t\t Average Reward : -3.76\n",
      "Episode : 415 \t\t Timestep : 4620 \t\t Reward: -528 \t\t Average Reward : -26.34\n",
      "Episode : 416 \t\t Timestep : 4630 \t\t Reward: -544 \t\t Average Reward : -35.23\n",
      "Episode : 417 \t\t Timestep : 4640 \t\t Reward: -503 \t\t Average Reward : -33.73\n",
      "Updating the Model\n",
      "Episode : 418 \t\t Timestep : 4650 \t\t Reward: -543 \t\t Average Reward : -0.68\n",
      "Episode : 419 \t\t Timestep : 4660 \t\t Reward: -885 \t\t Average Reward : -6.04\n",
      "Episode : 420 \t\t Timestep : 4670 \t\t Reward: -628 \t\t Average Reward : 4.24\n",
      "Episode : 421 \t\t Timestep : 4680 \t\t Reward: -711 \t\t Average Reward : -0.21\n",
      "Episode : 422 \t\t Timestep : 4690 \t\t Reward: -505 \t\t Average Reward : -38.33\n",
      "Updating the Model\n",
      "Episode : 423 \t\t Timestep : 4700 \t\t Reward: -504 \t\t Average Reward : -4.65\n",
      "Episode : 424 \t\t Timestep : 4710 \t\t Reward: -600 \t\t Average Reward : -21.54\n",
      "Episode : 425 \t\t Timestep : 4720 \t\t Reward: -703 \t\t Average Reward : -20.57\n",
      "Episode : 426 \t\t Timestep : 4730 \t\t Reward: -460 \t\t Average Reward : -36.18\n",
      "Episode : 427 \t\t Timestep : 4740 \t\t Reward: -538 \t\t Average Reward : 1.61\n",
      "Updating the Model\n",
      "Episode : 428 \t\t Timestep : 4750 \t\t Reward: -477 \t\t Average Reward : -28.08\n",
      "Episode : 429 \t\t Timestep : 4760 \t\t Reward: -594 \t\t Average Reward : 0.82\n",
      "Episode : 430 \t\t Timestep : 4770 \t\t Reward: -523 \t\t Average Reward : -34.51\n",
      "Episode : 431 \t\t Timestep : 4780 \t\t Reward: -495 \t\t Average Reward : -11.15\n",
      "Episode : 432 \t\t Timestep : 4790 \t\t Reward: -429 \t\t Average Reward : -4.11\n",
      "Updating the Model\n",
      "Episode : 433 \t\t Timestep : 4800 \t\t Reward: -471 \t\t Average Reward : -19.89\n",
      "Episode : 434 \t\t Timestep : 4810 \t\t Reward: -415 \t\t Average Reward : -33.85\n",
      "Episode : 435 \t\t Timestep : 4820 \t\t Reward: -512 \t\t Average Reward : -3.48\n",
      "Episode : 436 \t\t Timestep : 4830 \t\t Reward: -620 \t\t Average Reward : -35.63\n",
      "Episode : 437 \t\t Timestep : 4840 \t\t Reward: -537 \t\t Average Reward : 1.37\n",
      "Updating the Model\n",
      "Episode : 438 \t\t Timestep : 4850 \t\t Reward: -774 \t\t Average Reward : -34.66\n",
      "Episode : 439 \t\t Timestep : 4860 \t\t Reward: -563 \t\t Average Reward : -35.47\n",
      "Episode : 440 \t\t Timestep : 4870 \t\t Reward: -599 \t\t Average Reward : -34.0\n",
      "Episode : 441 \t\t Timestep : 4880 \t\t Reward: -520 \t\t Average Reward : 1.65\n",
      "Episode : 442 \t\t Timestep : 4890 \t\t Reward: -629 \t\t Average Reward : -33.2\n",
      "Updating the Model\n",
      "Episode : 443 \t\t Timestep : 4900 \t\t Reward: -447 \t\t Average Reward : -6.64\n",
      "Episode : 444 \t\t Timestep : 4910 \t\t Reward: -530 \t\t Average Reward : -35.91\n",
      "Episode : 445 \t\t Timestep : 4920 \t\t Reward: -739 \t\t Average Reward : -39.83\n",
      "Episode : 446 \t\t Timestep : 4930 \t\t Reward: -435 \t\t Average Reward : -34.17\n",
      "Episode : 447 \t\t Timestep : 4940 \t\t Reward: -449 \t\t Average Reward : -8.79\n",
      "Updating the Model\n",
      "Episode : 448 \t\t Timestep : 4950 \t\t Reward: -716 \t\t Average Reward : 3.01\n",
      "Episode : 449 \t\t Timestep : 4960 \t\t Reward: -499 \t\t Average Reward : -34.49\n",
      "Episode : 450 \t\t Timestep : 4970 \t\t Reward: -445 \t\t Average Reward : 1.74\n",
      "Episode : 451 \t\t Timestep : 4980 \t\t Reward: -698 \t\t Average Reward : 1.01\n",
      "Episode : 452 \t\t Timestep : 4990 \t\t Reward: -615 \t\t Average Reward : -12.83\n",
      "Updating the Model\n",
      "Episode : 453 \t\t Timestep : 5000 \t\t Reward: -589 \t\t Average Reward : -32.67\n",
      "Episode : 454 \t\t Timestep : 5010 \t\t Reward: -552 \t\t Average Reward : 3.46\n",
      "Episode : 455 \t\t Timestep : 5020 \t\t Reward: -696 \t\t Average Reward : -3.68\n",
      "Episode : 456 \t\t Timestep : 5030 \t\t Reward: -584 \t\t Average Reward : -3.97\n",
      "Episode : 457 \t\t Timestep : 5040 \t\t Reward: -546 \t\t Average Reward : -8.79\n",
      "Updating the Model\n",
      "Episode : 458 \t\t Timestep : 5050 \t\t Reward: -609 \t\t Average Reward : -35.63\n",
      "Episode : 459 \t\t Timestep : 5060 \t\t Reward: -403 \t\t Average Reward : -33.64\n",
      "Episode : 460 \t\t Timestep : 5070 \t\t Reward: -630 \t\t Average Reward : -34.2\n",
      "Episode : 461 \t\t Timestep : 5080 \t\t Reward: -718 \t\t Average Reward : -28.06\n",
      "Episode : 462 \t\t Timestep : 5090 \t\t Reward: -443 \t\t Average Reward : -38.56\n",
      "Updating the Model\n",
      "Episode : 463 \t\t Timestep : 5100 \t\t Reward: -567 \t\t Average Reward : -3.38\n",
      "Episode : 464 \t\t Timestep : 5110 \t\t Reward: -772 \t\t Average Reward : -32.18\n",
      "Episode : 465 \t\t Timestep : 5120 \t\t Reward: -745 \t\t Average Reward : -24.44\n",
      "Episode : 466 \t\t Timestep : 5130 \t\t Reward: -730 \t\t Average Reward : -24.35\n",
      "Episode : 467 \t\t Timestep : 5140 \t\t Reward: -657 \t\t Average Reward : -13.45\n",
      "Updating the Model\n",
      "Episode : 468 \t\t Timestep : 5150 \t\t Reward: -583 \t\t Average Reward : 1.37\n",
      "Episode : 469 \t\t Timestep : 5160 \t\t Reward: -529 \t\t Average Reward : 1.36\n",
      "Episode : 470 \t\t Timestep : 5170 \t\t Reward: -470 \t\t Average Reward : -8.64\n",
      "Episode : 471 \t\t Timestep : 5180 \t\t Reward: -550 \t\t Average Reward : -33.81\n",
      "Episode : 472 \t\t Timestep : 5190 \t\t Reward: -451 \t\t Average Reward : -13.16\n",
      "Updating the Model\n",
      "Episode : 473 \t\t Timestep : 5200 \t\t Reward: -558 \t\t Average Reward : -0.4\n",
      "Episode : 474 \t\t Timestep : 5210 \t\t Reward: -650 \t\t Average Reward : -26.3\n",
      "Episode : 475 \t\t Timestep : 5220 \t\t Reward: -741 \t\t Average Reward : -33.16\n",
      "Episode : 476 \t\t Timestep : 5230 \t\t Reward: -591 \t\t Average Reward : -35.49\n",
      "Episode : 477 \t\t Timestep : 5240 \t\t Reward: -620 \t\t Average Reward : 1.9\n",
      "Updating the Model\n",
      "Episode : 478 \t\t Timestep : 5250 \t\t Reward: -670 \t\t Average Reward : -34.35\n",
      "Episode : 479 \t\t Timestep : 5260 \t\t Reward: -496 \t\t Average Reward : -12.98\n",
      "Episode : 480 \t\t Timestep : 5270 \t\t Reward: -465 \t\t Average Reward : -31.6\n",
      "Episode : 481 \t\t Timestep : 5280 \t\t Reward: -408 \t\t Average Reward : -34.54\n",
      "Episode : 482 \t\t Timestep : 5290 \t\t Reward: -635 \t\t Average Reward : -34.0\n",
      "Updating the Model\n",
      "Episode : 483 \t\t Timestep : 5300 \t\t Reward: -552 \t\t Average Reward : -22.65\n",
      "Episode : 484 \t\t Timestep : 5310 \t\t Reward: -554 \t\t Average Reward : -0.67\n",
      "Episode : 485 \t\t Timestep : 5320 \t\t Reward: -531 \t\t Average Reward : -22.28\n",
      "Episode : 486 \t\t Timestep : 5330 \t\t Reward: -451 \t\t Average Reward : -35.67\n",
      "Episode : 487 \t\t Timestep : 5340 \t\t Reward: -575 \t\t Average Reward : -34.06\n",
      "Updating the Model\n",
      "Episode : 488 \t\t Timestep : 5350 \t\t Reward: -611 \t\t Average Reward : -31.87\n",
      "Episode : 489 \t\t Timestep : 5360 \t\t Reward: -426 \t\t Average Reward : -36.71\n",
      "Episode : 490 \t\t Timestep : 5370 \t\t Reward: -620 \t\t Average Reward : -28.46\n",
      "Episode : 491 \t\t Timestep : 5380 \t\t Reward: -706 \t\t Average Reward : -35.82\n",
      "Episode : 492 \t\t Timestep : 5390 \t\t Reward: -794 \t\t Average Reward : -35.38\n",
      "Updating the Model\n",
      "Episode : 493 \t\t Timestep : 5400 \t\t Reward: -533 \t\t Average Reward : -15.63\n",
      "Episode : 494 \t\t Timestep : 5410 \t\t Reward: -588 \t\t Average Reward : -7.06\n",
      "Episode : 495 \t\t Timestep : 5420 \t\t Reward: -577 \t\t Average Reward : 2.01\n",
      "Episode : 496 \t\t Timestep : 5430 \t\t Reward: -558 \t\t Average Reward : -4.92\n",
      "Episode : 497 \t\t Timestep : 5440 \t\t Reward: -603 \t\t Average Reward : -19.02\n",
      "Updating the Model\n",
      "Episode : 498 \t\t Timestep : 5450 \t\t Reward: -560 \t\t Average Reward : -32.75\n",
      "Episode : 499 \t\t Timestep : 5460 \t\t Reward: -549 \t\t Average Reward : -23.93\n",
      "Episode : 500 \t\t Timestep : 5470 \t\t Reward: -565 \t\t Average Reward : -35.65\n",
      "Episode : 501 \t\t Timestep : 5480 \t\t Reward: -632 \t\t Average Reward : -30.85\n",
      "Episode : 502 \t\t Timestep : 5490 \t\t Reward: -540 \t\t Average Reward : -37.14\n",
      "Updating the Model\n",
      "Episode : 503 \t\t Timestep : 5500 \t\t Reward: -544 \t\t Average Reward : -36.67\n",
      "Episode : 504 \t\t Timestep : 5510 \t\t Reward: -600 \t\t Average Reward : 0.68\n",
      "Episode : 505 \t\t Timestep : 5520 \t\t Reward: -552 \t\t Average Reward : -14.98\n",
      "Episode : 506 \t\t Timestep : 5530 \t\t Reward: -505 \t\t Average Reward : -1.35\n",
      "Episode : 507 \t\t Timestep : 5540 \t\t Reward: -619 \t\t Average Reward : -4.6\n",
      "Updating the Model\n",
      "Episode : 508 \t\t Timestep : 5550 \t\t Reward: -424 \t\t Average Reward : -35.83\n",
      "Episode : 509 \t\t Timestep : 5560 \t\t Reward: -693 \t\t Average Reward : -37.31\n",
      "Episode : 510 \t\t Timestep : 5570 \t\t Reward: -514 \t\t Average Reward : -0.89\n",
      "Episode : 511 \t\t Timestep : 5580 \t\t Reward: -673 \t\t Average Reward : -22.89\n",
      "Episode : 512 \t\t Timestep : 5590 \t\t Reward: -656 \t\t Average Reward : -17.4\n",
      "Updating the Model\n",
      "Episode : 513 \t\t Timestep : 5600 \t\t Reward: -496 \t\t Average Reward : -28.12\n",
      "Episode : 514 \t\t Timestep : 5610 \t\t Reward: -592 \t\t Average Reward : -34.71\n",
      "Episode : 515 \t\t Timestep : 5620 \t\t Reward: -601 \t\t Average Reward : -33.84\n",
      "Episode : 516 \t\t Timestep : 5630 \t\t Reward: -615 \t\t Average Reward : -5.01\n",
      "Episode : 517 \t\t Timestep : 5640 \t\t Reward: -685 \t\t Average Reward : -6.42\n",
      "Updating the Model\n",
      "Episode : 518 \t\t Timestep : 5650 \t\t Reward: -693 \t\t Average Reward : -36.09\n",
      "Episode : 519 \t\t Timestep : 5660 \t\t Reward: -565 \t\t Average Reward : -28.96\n",
      "Episode : 520 \t\t Timestep : 5670 \t\t Reward: -529 \t\t Average Reward : -39.71\n",
      "Episode : 521 \t\t Timestep : 5680 \t\t Reward: -531 \t\t Average Reward : -15.11\n",
      "Episode : 522 \t\t Timestep : 5690 \t\t Reward: -557 \t\t Average Reward : -34.65\n",
      "Updating the Model\n",
      "Episode : 523 \t\t Timestep : 5700 \t\t Reward: -486 \t\t Average Reward : -36.22\n",
      "Episode : 524 \t\t Timestep : 5710 \t\t Reward: -571 \t\t Average Reward : -37.06\n",
      "Episode : 525 \t\t Timestep : 5720 \t\t Reward: -584 \t\t Average Reward : -35.48\n",
      "Episode : 526 \t\t Timestep : 5730 \t\t Reward: -621 \t\t Average Reward : -14.41\n",
      "Episode : 527 \t\t Timestep : 5740 \t\t Reward: -596 \t\t Average Reward : -24.32\n",
      "Updating the Model\n",
      "Episode : 528 \t\t Timestep : 5750 \t\t Reward: -698 \t\t Average Reward : -36.4\n",
      "Episode : 529 \t\t Timestep : 5760 \t\t Reward: -489 \t\t Average Reward : -37.13\n",
      "Episode : 530 \t\t Timestep : 5770 \t\t Reward: -740 \t\t Average Reward : -19.48\n",
      "Episode : 531 \t\t Timestep : 5780 \t\t Reward: -432 \t\t Average Reward : -34.12\n",
      "Episode : 532 \t\t Timestep : 5790 \t\t Reward: -578 \t\t Average Reward : -19.8\n",
      "Updating the Model\n",
      "Episode : 533 \t\t Timestep : 5800 \t\t Reward: -611 \t\t Average Reward : -15.16\n",
      "Episode : 534 \t\t Timestep : 5810 \t\t Reward: -754 \t\t Average Reward : -37.33\n",
      "Episode : 535 \t\t Timestep : 5820 \t\t Reward: -616 \t\t Average Reward : -36.42\n",
      "Episode : 536 \t\t Timestep : 5830 \t\t Reward: -719 \t\t Average Reward : -13.14\n",
      "Episode : 537 \t\t Timestep : 5840 \t\t Reward: -527 \t\t Average Reward : -27.57\n",
      "Updating the Model\n",
      "Episode : 538 \t\t Timestep : 5850 \t\t Reward: -412 \t\t Average Reward : -36.02\n",
      "Episode : 539 \t\t Timestep : 5860 \t\t Reward: -411 \t\t Average Reward : -25.58\n",
      "Episode : 540 \t\t Timestep : 5870 \t\t Reward: -573 \t\t Average Reward : -33.64\n",
      "Episode : 541 \t\t Timestep : 5880 \t\t Reward: -543 \t\t Average Reward : -11.18\n",
      "Episode : 542 \t\t Timestep : 5890 \t\t Reward: -827 \t\t Average Reward : -30.54\n",
      "Updating the Model\n",
      "Episode : 543 \t\t Timestep : 5900 \t\t Reward: -527 \t\t Average Reward : 0.55\n",
      "Episode : 544 \t\t Timestep : 5910 \t\t Reward: -499 \t\t Average Reward : -2.32\n",
      "Episode : 545 \t\t Timestep : 5920 \t\t Reward: -550 \t\t Average Reward : -10.29\n",
      "Episode : 546 \t\t Timestep : 5930 \t\t Reward: -447 \t\t Average Reward : -3.16\n",
      "Episode : 547 \t\t Timestep : 5940 \t\t Reward: -476 \t\t Average Reward : -39.7\n",
      "Updating the Model\n",
      "Episode : 548 \t\t Timestep : 5950 \t\t Reward: -461 \t\t Average Reward : -25.34\n",
      "Episode : 549 \t\t Timestep : 5960 \t\t Reward: -570 \t\t Average Reward : -34.18\n",
      "Episode : 550 \t\t Timestep : 5970 \t\t Reward: -572 \t\t Average Reward : -7.25\n",
      "Episode : 551 \t\t Timestep : 5980 \t\t Reward: -542 \t\t Average Reward : -2.04\n",
      "Episode : 552 \t\t Timestep : 5990 \t\t Reward: -519 \t\t Average Reward : -35.45\n",
      "Updating the Model\n",
      "Episode : 553 \t\t Timestep : 6000 \t\t Reward: -614 \t\t Average Reward : -25.88\n",
      "Episode : 554 \t\t Timestep : 6010 \t\t Reward: -717 \t\t Average Reward : -24.53\n",
      "Episode : 555 \t\t Timestep : 6020 \t\t Reward: -592 \t\t Average Reward : -3.19\n",
      "Episode : 556 \t\t Timestep : 6030 \t\t Reward: -552 \t\t Average Reward : -35.82\n",
      "Episode : 557 \t\t Timestep : 6040 \t\t Reward: -422 \t\t Average Reward : -39.75\n",
      "Updating the Model\n",
      "Episode : 558 \t\t Timestep : 6050 \t\t Reward: -612 \t\t Average Reward : -25.92\n",
      "Episode : 559 \t\t Timestep : 6060 \t\t Reward: -707 \t\t Average Reward : -35.85\n",
      "Episode : 560 \t\t Timestep : 6070 \t\t Reward: -562 \t\t Average Reward : -22.79\n",
      "Episode : 561 \t\t Timestep : 6080 \t\t Reward: -720 \t\t Average Reward : -5.78\n",
      "Episode : 562 \t\t Timestep : 6090 \t\t Reward: -742 \t\t Average Reward : -33.75\n",
      "Updating the Model\n",
      "Episode : 563 \t\t Timestep : 6100 \t\t Reward: -425 \t\t Average Reward : -0.35\n",
      "Episode : 564 \t\t Timestep : 6110 \t\t Reward: -488 \t\t Average Reward : -4.18\n",
      "Episode : 565 \t\t Timestep : 6120 \t\t Reward: -649 \t\t Average Reward : -19.94\n",
      "Episode : 566 \t\t Timestep : 6130 \t\t Reward: -832 \t\t Average Reward : -34.26\n",
      "Episode : 567 \t\t Timestep : 6140 \t\t Reward: -616 \t\t Average Reward : -32.46\n",
      "Updating the Model\n",
      "Episode : 568 \t\t Timestep : 6150 \t\t Reward: -610 \t\t Average Reward : -10.86\n",
      "Episode : 569 \t\t Timestep : 6160 \t\t Reward: -709 \t\t Average Reward : -37.42\n",
      "Episode : 570 \t\t Timestep : 6170 \t\t Reward: -611 \t\t Average Reward : -13.76\n",
      "Episode : 571 \t\t Timestep : 6180 \t\t Reward: -581 \t\t Average Reward : -7.55\n",
      "Episode : 572 \t\t Timestep : 6190 \t\t Reward: -458 \t\t Average Reward : -29.52\n",
      "Updating the Model\n",
      "Episode : 573 \t\t Timestep : 6200 \t\t Reward: -657 \t\t Average Reward : -33.66\n",
      "Episode : 574 \t\t Timestep : 6210 \t\t Reward: -737 \t\t Average Reward : -3.69\n",
      "Episode : 575 \t\t Timestep : 6220 \t\t Reward: -664 \t\t Average Reward : -30.6\n",
      "Episode : 576 \t\t Timestep : 6230 \t\t Reward: -490 \t\t Average Reward : 0.77\n",
      "Episode : 577 \t\t Timestep : 6240 \t\t Reward: -457 \t\t Average Reward : -2.96\n",
      "Updating the Model\n",
      "Episode : 578 \t\t Timestep : 6250 \t\t Reward: -404 \t\t Average Reward : -16.57\n",
      "Episode : 579 \t\t Timestep : 6260 \t\t Reward: -579 \t\t Average Reward : -26.5\n",
      "Episode : 580 \t\t Timestep : 6270 \t\t Reward: -521 \t\t Average Reward : -30.24\n",
      "Episode : 581 \t\t Timestep : 6280 \t\t Reward: -529 \t\t Average Reward : -8.14\n",
      "Episode : 582 \t\t Timestep : 6290 \t\t Reward: -614 \t\t Average Reward : 1.18\n",
      "Updating the Model\n",
      "Episode : 583 \t\t Timestep : 6300 \t\t Reward: -404 \t\t Average Reward : -8.0\n",
      "Episode : 584 \t\t Timestep : 6310 \t\t Reward: -650 \t\t Average Reward : -1.6\n",
      "Episode : 585 \t\t Timestep : 6320 \t\t Reward: -535 \t\t Average Reward : -34.96\n",
      "Episode : 586 \t\t Timestep : 6330 \t\t Reward: -644 \t\t Average Reward : -31.43\n",
      "Episode : 587 \t\t Timestep : 6340 \t\t Reward: -676 \t\t Average Reward : -25.3\n",
      "Updating the Model\n",
      "Episode : 588 \t\t Timestep : 6350 \t\t Reward: -566 \t\t Average Reward : -36.67\n",
      "Episode : 589 \t\t Timestep : 6360 \t\t Reward: -636 \t\t Average Reward : -0.84\n",
      "Episode : 590 \t\t Timestep : 6370 \t\t Reward: -429 \t\t Average Reward : -35.44\n",
      "Episode : 591 \t\t Timestep : 6380 \t\t Reward: -718 \t\t Average Reward : -0.48\n",
      "Episode : 592 \t\t Timestep : 6390 \t\t Reward: -616 \t\t Average Reward : -8.83\n",
      "Updating the Model\n",
      "Episode : 593 \t\t Timestep : 6400 \t\t Reward: -501 \t\t Average Reward : -4.67\n",
      "Episode : 594 \t\t Timestep : 6410 \t\t Reward: -614 \t\t Average Reward : 1.61\n",
      "Episode : 595 \t\t Timestep : 6420 \t\t Reward: -443 \t\t Average Reward : -9.34\n",
      "Episode : 596 \t\t Timestep : 6430 \t\t Reward: -657 \t\t Average Reward : -16.11\n",
      "Episode : 597 \t\t Timestep : 6440 \t\t Reward: -556 \t\t Average Reward : -29.8\n",
      "Updating the Model\n",
      "Episode : 598 \t\t Timestep : 6450 \t\t Reward: -492 \t\t Average Reward : -9.57\n",
      "Episode : 599 \t\t Timestep : 6460 \t\t Reward: -415 \t\t Average Reward : -29.66\n",
      "Episode : 600 \t\t Timestep : 6470 \t\t Reward: -526 \t\t Average Reward : -34.43\n",
      "Episode : 601 \t\t Timestep : 6480 \t\t Reward: -610 \t\t Average Reward : -37.24\n",
      "Episode : 602 \t\t Timestep : 6490 \t\t Reward: -594 \t\t Average Reward : -21.77\n",
      "Updating the Model\n",
      "Episode : 603 \t\t Timestep : 6500 \t\t Reward: -394 \t\t Average Reward : -31.92\n",
      "Episode : 604 \t\t Timestep : 6510 \t\t Reward: -418 \t\t Average Reward : -7.16\n",
      "Episode : 605 \t\t Timestep : 6520 \t\t Reward: -674 \t\t Average Reward : -29.62\n",
      "Episode : 606 \t\t Timestep : 6530 \t\t Reward: -564 \t\t Average Reward : -37.54\n",
      "Episode : 607 \t\t Timestep : 6540 \t\t Reward: -582 \t\t Average Reward : -29.94\n",
      "Updating the Model\n",
      "Episode : 608 \t\t Timestep : 6550 \t\t Reward: -619 \t\t Average Reward : -35.76\n",
      "Episode : 609 \t\t Timestep : 6560 \t\t Reward: -464 \t\t Average Reward : -33.81\n",
      "Episode : 610 \t\t Timestep : 6570 \t\t Reward: -559 \t\t Average Reward : -19.79\n",
      "Episode : 611 \t\t Timestep : 6580 \t\t Reward: -573 \t\t Average Reward : -34.59\n",
      "Episode : 612 \t\t Timestep : 6590 \t\t Reward: -498 \t\t Average Reward : -26.6\n",
      "Updating the Model\n",
      "Episode : 613 \t\t Timestep : 6600 \t\t Reward: -531 \t\t Average Reward : -9.96\n",
      "Episode : 614 \t\t Timestep : 6610 \t\t Reward: -759 \t\t Average Reward : -3.92\n",
      "Episode : 615 \t\t Timestep : 6620 \t\t Reward: -539 \t\t Average Reward : -9.69\n",
      "Episode : 616 \t\t Timestep : 6630 \t\t Reward: -602 \t\t Average Reward : -5.31\n",
      "Episode : 617 \t\t Timestep : 6640 \t\t Reward: -555 \t\t Average Reward : 2.93\n",
      "Updating the Model\n",
      "Episode : 618 \t\t Timestep : 6650 \t\t Reward: -478 \t\t Average Reward : -34.99\n",
      "Episode : 619 \t\t Timestep : 6660 \t\t Reward: -608 \t\t Average Reward : -34.28\n",
      "Episode : 620 \t\t Timestep : 6670 \t\t Reward: -549 \t\t Average Reward : -28.22\n",
      "Episode : 621 \t\t Timestep : 6680 \t\t Reward: -587 \t\t Average Reward : -12.68\n",
      "Episode : 622 \t\t Timestep : 6690 \t\t Reward: -735 \t\t Average Reward : -26.13\n",
      "Updating the Model\n",
      "Episode : 623 \t\t Timestep : 6700 \t\t Reward: -743 \t\t Average Reward : 2.39\n",
      "Episode : 624 \t\t Timestep : 6710 \t\t Reward: -429 \t\t Average Reward : -20.65\n",
      "Episode : 625 \t\t Timestep : 6720 \t\t Reward: -564 \t\t Average Reward : -32.25\n",
      "Episode : 626 \t\t Timestep : 6730 \t\t Reward: -518 \t\t Average Reward : 1.91\n",
      "Episode : 627 \t\t Timestep : 6740 \t\t Reward: -623 \t\t Average Reward : -16.01\n",
      "Updating the Model\n",
      "Episode : 628 \t\t Timestep : 6750 \t\t Reward: -597 \t\t Average Reward : -34.51\n",
      "Episode : 629 \t\t Timestep : 6760 \t\t Reward: -747 \t\t Average Reward : -3.11\n",
      "Episode : 630 \t\t Timestep : 6770 \t\t Reward: -509 \t\t Average Reward : 0.58\n",
      "Episode : 631 \t\t Timestep : 6780 \t\t Reward: -786 \t\t Average Reward : 2.22\n",
      "Episode : 632 \t\t Timestep : 6790 \t\t Reward: -504 \t\t Average Reward : -33.02\n",
      "Updating the Model\n",
      "Episode : 633 \t\t Timestep : 6800 \t\t Reward: -493 \t\t Average Reward : -36.75\n",
      "Episode : 634 \t\t Timestep : 6810 \t\t Reward: -543 \t\t Average Reward : -37.74\n",
      "Episode : 635 \t\t Timestep : 6820 \t\t Reward: -599 \t\t Average Reward : -24.23\n",
      "Episode : 636 \t\t Timestep : 6830 \t\t Reward: -477 \t\t Average Reward : 1.42\n",
      "Episode : 637 \t\t Timestep : 6840 \t\t Reward: -475 \t\t Average Reward : -14.01\n",
      "Updating the Model\n",
      "Episode : 638 \t\t Timestep : 6850 \t\t Reward: -645 \t\t Average Reward : -25.77\n",
      "Episode : 639 \t\t Timestep : 6860 \t\t Reward: -695 \t\t Average Reward : -13.45\n",
      "Episode : 640 \t\t Timestep : 6870 \t\t Reward: -686 \t\t Average Reward : -28.83\n",
      "Episode : 641 \t\t Timestep : 6880 \t\t Reward: -556 \t\t Average Reward : -14.26\n",
      "Episode : 642 \t\t Timestep : 6890 \t\t Reward: -544 \t\t Average Reward : -27.74\n",
      "Updating the Model\n",
      "Episode : 643 \t\t Timestep : 6900 \t\t Reward: -549 \t\t Average Reward : -35.91\n",
      "Episode : 644 \t\t Timestep : 6910 \t\t Reward: -420 \t\t Average Reward : -33.86\n",
      "Episode : 645 \t\t Timestep : 6920 \t\t Reward: -500 \t\t Average Reward : -28.12\n",
      "Episode : 646 \t\t Timestep : 6930 \t\t Reward: -540 \t\t Average Reward : -7.07\n",
      "Episode : 647 \t\t Timestep : 6940 \t\t Reward: -490 \t\t Average Reward : -9.13\n",
      "Updating the Model\n",
      "Episode : 648 \t\t Timestep : 6950 \t\t Reward: -597 \t\t Average Reward : -6.65\n",
      "Episode : 649 \t\t Timestep : 6960 \t\t Reward: -679 \t\t Average Reward : -16.26\n",
      "Episode : 650 \t\t Timestep : 6970 \t\t Reward: -453 \t\t Average Reward : -31.62\n",
      "Episode : 651 \t\t Timestep : 6980 \t\t Reward: -637 \t\t Average Reward : -27.19\n",
      "Episode : 652 \t\t Timestep : 6990 \t\t Reward: -601 \t\t Average Reward : -24.21\n",
      "Updating the Model\n",
      "Episode : 653 \t\t Timestep : 7000 \t\t Reward: -488 \t\t Average Reward : -31.18\n",
      "Episode : 654 \t\t Timestep : 7010 \t\t Reward: -526 \t\t Average Reward : -7.8\n",
      "Episode : 655 \t\t Timestep : 7020 \t\t Reward: -627 \t\t Average Reward : -33.68\n",
      "Episode : 656 \t\t Timestep : 7030 \t\t Reward: -459 \t\t Average Reward : -30.07\n",
      "Episode : 657 \t\t Timestep : 7040 \t\t Reward: -647 \t\t Average Reward : -36.32\n",
      "Updating the Model\n",
      "Episode : 658 \t\t Timestep : 7050 \t\t Reward: -774 \t\t Average Reward : -18.29\n",
      "Episode : 659 \t\t Timestep : 7060 \t\t Reward: -460 \t\t Average Reward : -25.65\n",
      "Episode : 660 \t\t Timestep : 7070 \t\t Reward: -652 \t\t Average Reward : -27.35\n",
      "Episode : 661 \t\t Timestep : 7080 \t\t Reward: -556 \t\t Average Reward : -32.97\n",
      "Episode : 662 \t\t Timestep : 7090 \t\t Reward: -720 \t\t Average Reward : -30.88\n",
      "Updating the Model\n",
      "Episode : 663 \t\t Timestep : 7100 \t\t Reward: -574 \t\t Average Reward : -6.29\n",
      "Episode : 664 \t\t Timestep : 7110 \t\t Reward: -755 \t\t Average Reward : -24.66\n",
      "Episode : 665 \t\t Timestep : 7120 \t\t Reward: -799 \t\t Average Reward : -26.64\n",
      "Episode : 666 \t\t Timestep : 7130 \t\t Reward: -574 \t\t Average Reward : 1.21\n",
      "Episode : 667 \t\t Timestep : 7140 \t\t Reward: -711 \t\t Average Reward : -17.17\n",
      "Updating the Model\n",
      "Episode : 668 \t\t Timestep : 7150 \t\t Reward: -575 \t\t Average Reward : -37.76\n",
      "Episode : 669 \t\t Timestep : 7160 \t\t Reward: -663 \t\t Average Reward : 2.8\n",
      "Episode : 670 \t\t Timestep : 7170 \t\t Reward: -732 \t\t Average Reward : -2.06\n",
      "Episode : 671 \t\t Timestep : 7180 \t\t Reward: -472 \t\t Average Reward : 2.06\n",
      "Episode : 672 \t\t Timestep : 7190 \t\t Reward: -498 \t\t Average Reward : -23.92\n",
      "Updating the Model\n",
      "Episode : 673 \t\t Timestep : 7200 \t\t Reward: -725 \t\t Average Reward : -27.4\n",
      "Episode : 674 \t\t Timestep : 7210 \t\t Reward: -585 \t\t Average Reward : -17.35\n",
      "Episode : 675 \t\t Timestep : 7220 \t\t Reward: -516 \t\t Average Reward : -2.42\n",
      "Episode : 676 \t\t Timestep : 7230 \t\t Reward: -567 \t\t Average Reward : -37.39\n",
      "Episode : 677 \t\t Timestep : 7240 \t\t Reward: -738 \t\t Average Reward : -1.3\n",
      "Updating the Model\n",
      "Episode : 678 \t\t Timestep : 7250 \t\t Reward: -608 \t\t Average Reward : -33.28\n",
      "Episode : 679 \t\t Timestep : 7260 \t\t Reward: -542 \t\t Average Reward : -11.87\n",
      "Episode : 680 \t\t Timestep : 7270 \t\t Reward: -580 \t\t Average Reward : -34.99\n",
      "Episode : 681 \t\t Timestep : 7280 \t\t Reward: -486 \t\t Average Reward : -12.11\n",
      "Episode : 682 \t\t Timestep : 7290 \t\t Reward: -542 \t\t Average Reward : -6.99\n",
      "Updating the Model\n",
      "Episode : 683 \t\t Timestep : 7300 \t\t Reward: -434 \t\t Average Reward : -37.09\n",
      "Episode : 684 \t\t Timestep : 7310 \t\t Reward: -664 \t\t Average Reward : -18.34\n",
      "Episode : 685 \t\t Timestep : 7320 \t\t Reward: -515 \t\t Average Reward : -21.74\n",
      "Episode : 686 \t\t Timestep : 7330 \t\t Reward: -512 \t\t Average Reward : -37.29\n",
      "Episode : 687 \t\t Timestep : 7340 \t\t Reward: -442 \t\t Average Reward : -31.32\n",
      "Updating the Model\n",
      "Episode : 688 \t\t Timestep : 7350 \t\t Reward: -552 \t\t Average Reward : -36.09\n",
      "Episode : 689 \t\t Timestep : 7360 \t\t Reward: -627 \t\t Average Reward : -35.4\n",
      "Episode : 690 \t\t Timestep : 7370 \t\t Reward: -647 \t\t Average Reward : -35.41\n",
      "Episode : 691 \t\t Timestep : 7380 \t\t Reward: -620 \t\t Average Reward : -36.9\n",
      "Episode : 692 \t\t Timestep : 7390 \t\t Reward: -481 \t\t Average Reward : 2.76\n",
      "Updating the Model\n",
      "Episode : 693 \t\t Timestep : 7400 \t\t Reward: -631 \t\t Average Reward : -0.53\n",
      "Episode : 694 \t\t Timestep : 7410 \t\t Reward: -448 \t\t Average Reward : -8.6\n",
      "Episode : 695 \t\t Timestep : 7420 \t\t Reward: -546 \t\t Average Reward : -20.15\n",
      "Episode : 696 \t\t Timestep : 7430 \t\t Reward: -914 \t\t Average Reward : -10.87\n",
      "Episode : 697 \t\t Timestep : 7440 \t\t Reward: -628 \t\t Average Reward : 1.18\n",
      "Updating the Model\n",
      "Episode : 698 \t\t Timestep : 7450 \t\t Reward: -650 \t\t Average Reward : -3.71\n",
      "Episode : 699 \t\t Timestep : 7460 \t\t Reward: -694 \t\t Average Reward : -33.37\n",
      "Episode : 700 \t\t Timestep : 7470 \t\t Reward: -648 \t\t Average Reward : -36.15\n",
      "Episode : 701 \t\t Timestep : 7480 \t\t Reward: -684 \t\t Average Reward : -34.7\n",
      "Episode : 702 \t\t Timestep : 7490 \t\t Reward: -547 \t\t Average Reward : -37.38\n",
      "Updating the Model\n",
      "Episode : 703 \t\t Timestep : 7500 \t\t Reward: -644 \t\t Average Reward : -35.89\n",
      "Episode : 704 \t\t Timestep : 7510 \t\t Reward: -578 \t\t Average Reward : -1.57\n",
      "Episode : 705 \t\t Timestep : 7520 \t\t Reward: -523 \t\t Average Reward : -34.33\n",
      "Episode : 706 \t\t Timestep : 7530 \t\t Reward: -569 \t\t Average Reward : -36.69\n",
      "Episode : 707 \t\t Timestep : 7540 \t\t Reward: -628 \t\t Average Reward : -9.88\n",
      "Updating the Model\n",
      "Episode : 708 \t\t Timestep : 7550 \t\t Reward: -575 \t\t Average Reward : -4.22\n",
      "Episode : 709 \t\t Timestep : 7560 \t\t Reward: -428 \t\t Average Reward : -34.71\n",
      "Episode : 710 \t\t Timestep : 7570 \t\t Reward: -606 \t\t Average Reward : -37.09\n",
      "Episode : 711 \t\t Timestep : 7580 \t\t Reward: -696 \t\t Average Reward : -1.26\n",
      "Episode : 712 \t\t Timestep : 7590 \t\t Reward: -759 \t\t Average Reward : -6.68\n",
      "Updating the Model\n",
      "Episode : 713 \t\t Timestep : 7600 \t\t Reward: -461 \t\t Average Reward : -2.01\n",
      "Episode : 714 \t\t Timestep : 7610 \t\t Reward: -555 \t\t Average Reward : 1.4\n",
      "Episode : 715 \t\t Timestep : 7620 \t\t Reward: -491 \t\t Average Reward : -20.14\n",
      "Episode : 716 \t\t Timestep : 7630 \t\t Reward: -553 \t\t Average Reward : -2.66\n",
      "Episode : 717 \t\t Timestep : 7640 \t\t Reward: -602 \t\t Average Reward : -16.88\n",
      "Updating the Model\n",
      "Episode : 718 \t\t Timestep : 7650 \t\t Reward: -564 \t\t Average Reward : -9.58\n",
      "Episode : 719 \t\t Timestep : 7660 \t\t Reward: -447 \t\t Average Reward : -5.77\n",
      "Episode : 720 \t\t Timestep : 7670 \t\t Reward: -626 \t\t Average Reward : -16.56\n",
      "Episode : 721 \t\t Timestep : 7680 \t\t Reward: -638 \t\t Average Reward : -21.02\n",
      "Episode : 722 \t\t Timestep : 7690 \t\t Reward: -591 \t\t Average Reward : -3.53\n",
      "Updating the Model\n",
      "Episode : 723 \t\t Timestep : 7700 \t\t Reward: -512 \t\t Average Reward : -27.49\n",
      "Episode : 724 \t\t Timestep : 7710 \t\t Reward: -509 \t\t Average Reward : -17.08\n",
      "Episode : 725 \t\t Timestep : 7720 \t\t Reward: -495 \t\t Average Reward : -6.72\n",
      "Episode : 726 \t\t Timestep : 7730 \t\t Reward: -576 \t\t Average Reward : 2.41\n",
      "Episode : 727 \t\t Timestep : 7740 \t\t Reward: -698 \t\t Average Reward : -35.59\n",
      "Updating the Model\n",
      "Episode : 728 \t\t Timestep : 7750 \t\t Reward: -537 \t\t Average Reward : -17.8\n",
      "Episode : 729 \t\t Timestep : 7760 \t\t Reward: -556 \t\t Average Reward : -40.89\n",
      "Episode : 730 \t\t Timestep : 7770 \t\t Reward: -567 \t\t Average Reward : -11.34\n",
      "Episode : 731 \t\t Timestep : 7780 \t\t Reward: -581 \t\t Average Reward : -36.02\n",
      "Episode : 732 \t\t Timestep : 7790 \t\t Reward: -540 \t\t Average Reward : -21.94\n",
      "Updating the Model\n",
      "Episode : 733 \t\t Timestep : 7800 \t\t Reward: -606 \t\t Average Reward : -14.77\n",
      "Episode : 734 \t\t Timestep : 7810 \t\t Reward: -707 \t\t Average Reward : 1.64\n",
      "Episode : 735 \t\t Timestep : 7820 \t\t Reward: -523 \t\t Average Reward : -7.1\n",
      "Episode : 736 \t\t Timestep : 7830 \t\t Reward: -406 \t\t Average Reward : -1.2\n",
      "Episode : 737 \t\t Timestep : 7840 \t\t Reward: -659 \t\t Average Reward : -33.6\n",
      "Updating the Model\n",
      "Episode : 738 \t\t Timestep : 7850 \t\t Reward: -681 \t\t Average Reward : -38.04\n",
      "Episode : 739 \t\t Timestep : 7860 \t\t Reward: -679 \t\t Average Reward : -37.78\n",
      "Episode : 740 \t\t Timestep : 7870 \t\t Reward: -451 \t\t Average Reward : -22.28\n",
      "Episode : 741 \t\t Timestep : 7880 \t\t Reward: -451 \t\t Average Reward : -19.88\n",
      "Episode : 742 \t\t Timestep : 7890 \t\t Reward: -690 \t\t Average Reward : -0.83\n",
      "Updating the Model\n",
      "Episode : 743 \t\t Timestep : 7900 \t\t Reward: -458 \t\t Average Reward : -27.83\n",
      "Episode : 744 \t\t Timestep : 7910 \t\t Reward: -567 \t\t Average Reward : 4.76\n",
      "Episode : 745 \t\t Timestep : 7920 \t\t Reward: -467 \t\t Average Reward : -0.38\n",
      "Episode : 746 \t\t Timestep : 7930 \t\t Reward: -545 \t\t Average Reward : 1.99\n",
      "Episode : 747 \t\t Timestep : 7940 \t\t Reward: -668 \t\t Average Reward : -33.63\n",
      "Updating the Model\n",
      "Episode : 748 \t\t Timestep : 7950 \t\t Reward: -487 \t\t Average Reward : -36.98\n",
      "Episode : 749 \t\t Timestep : 7960 \t\t Reward: -557 \t\t Average Reward : -36.8\n",
      "Episode : 750 \t\t Timestep : 7970 \t\t Reward: -753 \t\t Average Reward : -10.87\n",
      "Episode : 751 \t\t Timestep : 7980 \t\t Reward: -651 \t\t Average Reward : -16.65\n",
      "Episode : 752 \t\t Timestep : 7990 \t\t Reward: -602 \t\t Average Reward : -4.99\n",
      "Updating the Model\n",
      "Episode : 753 \t\t Timestep : 8000 \t\t Reward: -799 \t\t Average Reward : -36.49\n",
      "Episode : 754 \t\t Timestep : 8010 \t\t Reward: -639 \t\t Average Reward : -36.76\n",
      "Episode : 755 \t\t Timestep : 8020 \t\t Reward: -605 \t\t Average Reward : -33.37\n",
      "Episode : 756 \t\t Timestep : 8030 \t\t Reward: -608 \t\t Average Reward : -35.37\n",
      "Episode : 757 \t\t Timestep : 8040 \t\t Reward: -517 \t\t Average Reward : -25.15\n",
      "Updating the Model\n",
      "Episode : 758 \t\t Timestep : 8050 \t\t Reward: -524 \t\t Average Reward : -36.24\n",
      "Episode : 759 \t\t Timestep : 8060 \t\t Reward: -530 \t\t Average Reward : -34.4\n",
      "Episode : 760 \t\t Timestep : 8070 \t\t Reward: -475 \t\t Average Reward : -12.92\n",
      "Episode : 761 \t\t Timestep : 8080 \t\t Reward: -503 \t\t Average Reward : -37.64\n",
      "Episode : 762 \t\t Timestep : 8090 \t\t Reward: -543 \t\t Average Reward : -15.08\n",
      "Updating the Model\n",
      "Episode : 763 \t\t Timestep : 8100 \t\t Reward: -414 \t\t Average Reward : -20.45\n",
      "Episode : 764 \t\t Timestep : 8110 \t\t Reward: -605 \t\t Average Reward : -21.12\n",
      "Episode : 765 \t\t Timestep : 8120 \t\t Reward: -527 \t\t Average Reward : -37.85\n",
      "Episode : 766 \t\t Timestep : 8130 \t\t Reward: -473 \t\t Average Reward : -34.4\n",
      "Episode : 767 \t\t Timestep : 8140 \t\t Reward: -591 \t\t Average Reward : 1.22\n",
      "Updating the Model\n",
      "Episode : 768 \t\t Timestep : 8150 \t\t Reward: -675 \t\t Average Reward : -28.01\n",
      "Episode : 769 \t\t Timestep : 8160 \t\t Reward: -673 \t\t Average Reward : -16.52\n",
      "Episode : 770 \t\t Timestep : 8170 \t\t Reward: -643 \t\t Average Reward : -23.86\n",
      "Episode : 771 \t\t Timestep : 8180 \t\t Reward: -632 \t\t Average Reward : -4.1\n",
      "Episode : 772 \t\t Timestep : 8190 \t\t Reward: -527 \t\t Average Reward : -32.81\n",
      "Updating the Model\n",
      "Episode : 773 \t\t Timestep : 8200 \t\t Reward: -582 \t\t Average Reward : -36.78\n",
      "Episode : 774 \t\t Timestep : 8210 \t\t Reward: -536 \t\t Average Reward : -34.79\n",
      "Episode : 775 \t\t Timestep : 8220 \t\t Reward: -564 \t\t Average Reward : 0.01\n",
      "Episode : 776 \t\t Timestep : 8230 \t\t Reward: -442 \t\t Average Reward : -10.22\n",
      "Episode : 777 \t\t Timestep : 8240 \t\t Reward: -576 \t\t Average Reward : -35.32\n",
      "Updating the Model\n",
      "Episode : 778 \t\t Timestep : 8250 \t\t Reward: -624 \t\t Average Reward : -16.25\n",
      "Episode : 779 \t\t Timestep : 8260 \t\t Reward: -538 \t\t Average Reward : -19.57\n",
      "Episode : 780 \t\t Timestep : 8270 \t\t Reward: -578 \t\t Average Reward : -30.69\n",
      "Episode : 781 \t\t Timestep : 8280 \t\t Reward: -437 \t\t Average Reward : -23.99\n",
      "Episode : 782 \t\t Timestep : 8290 \t\t Reward: -652 \t\t Average Reward : 0.79\n",
      "Updating the Model\n",
      "Episode : 783 \t\t Timestep : 8300 \t\t Reward: -563 \t\t Average Reward : -1.1\n",
      "Episode : 784 \t\t Timestep : 8310 \t\t Reward: -689 \t\t Average Reward : -32.44\n",
      "Episode : 785 \t\t Timestep : 8320 \t\t Reward: -568 \t\t Average Reward : -34.12\n",
      "Episode : 786 \t\t Timestep : 8330 \t\t Reward: -642 \t\t Average Reward : -38.68\n",
      "Episode : 787 \t\t Timestep : 8340 \t\t Reward: -617 \t\t Average Reward : 3.32\n",
      "Updating the Model\n",
      "Episode : 788 \t\t Timestep : 8350 \t\t Reward: -547 \t\t Average Reward : -36.34\n",
      "Episode : 789 \t\t Timestep : 8360 \t\t Reward: -647 \t\t Average Reward : -29.04\n",
      "Episode : 790 \t\t Timestep : 8370 \t\t Reward: -653 \t\t Average Reward : -2.38\n",
      "Episode : 791 \t\t Timestep : 8380 \t\t Reward: -675 \t\t Average Reward : -30.59\n",
      "Episode : 792 \t\t Timestep : 8390 \t\t Reward: -482 \t\t Average Reward : -9.54\n",
      "Updating the Model\n",
      "Episode : 793 \t\t Timestep : 8400 \t\t Reward: -650 \t\t Average Reward : -3.72\n",
      "Episode : 794 \t\t Timestep : 8410 \t\t Reward: -435 \t\t Average Reward : 1.44\n",
      "Episode : 795 \t\t Timestep : 8420 \t\t Reward: -509 \t\t Average Reward : -4.92\n",
      "Episode : 796 \t\t Timestep : 8430 \t\t Reward: -599 \t\t Average Reward : -20.01\n",
      "Episode : 797 \t\t Timestep : 8440 \t\t Reward: -591 \t\t Average Reward : -32.77\n",
      "Updating the Model\n",
      "Episode : 798 \t\t Timestep : 8450 \t\t Reward: -532 \t\t Average Reward : -34.21\n",
      "Episode : 799 \t\t Timestep : 8460 \t\t Reward: -551 \t\t Average Reward : -26.96\n",
      "Episode : 800 \t\t Timestep : 8470 \t\t Reward: -666 \t\t Average Reward : -15.6\n",
      "Episode : 801 \t\t Timestep : 8480 \t\t Reward: -413 \t\t Average Reward : -34.77\n",
      "Episode : 802 \t\t Timestep : 8490 \t\t Reward: -582 \t\t Average Reward : -16.81\n",
      "Updating the Model\n",
      "Episode : 803 \t\t Timestep : 8500 \t\t Reward: -612 \t\t Average Reward : -36.95\n",
      "Episode : 804 \t\t Timestep : 8510 \t\t Reward: -482 \t\t Average Reward : -38.3\n",
      "Episode : 805 \t\t Timestep : 8520 \t\t Reward: -630 \t\t Average Reward : -10.27\n",
      "Episode : 806 \t\t Timestep : 8530 \t\t Reward: -696 \t\t Average Reward : -36.41\n",
      "Episode : 807 \t\t Timestep : 8540 \t\t Reward: -760 \t\t Average Reward : -2.53\n",
      "Updating the Model\n",
      "Episode : 808 \t\t Timestep : 8550 \t\t Reward: -413 \t\t Average Reward : -19.47\n",
      "Episode : 809 \t\t Timestep : 8560 \t\t Reward: -765 \t\t Average Reward : -25.13\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 102\u001b[0m\n\u001b[1;32m     99\u001b[0m print_avg_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(print_avg_reward, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    101\u001b[0m ddpg_agent\u001b[38;5;241m.\u001b[39mexplore_noise \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9998\u001b[39m\n\u001b[0;32m--> 102\u001b[0m ep_r \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddpg_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mturns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Timestep : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Reward: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Average Reward : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i_episode, time_step, ep_r, print_avg_reward))\n\u001b[1;32m    106\u001b[0m print_running_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[26], line 66\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(env, agent, turns)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Take deterministic actions at test time\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     a \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(s, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 66\u001b[0m     s_next, r, dw, tr, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     done \u001b[38;5;241m=\u001b[39m (dw \u001b[38;5;129;01mor\u001b[39;00m tr)\n\u001b[1;32m     69\u001b[0m     total_scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n",
      "File \u001b[0;32m/mnt/sobhan/cpi/lib/python3.12/site-packages/gym/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m/mnt/sobhan/cpi/lib/python3.12/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/sobhan/cpi/lib/python3.12/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/sobhan/cpi/lib/python3.12/site-packages/gym/envs/box2d/lunar_lander.py:544\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    537\u001b[0m oy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtip[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m dispersion[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m side[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m dispersion[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m direction \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_AWAY \u001b[38;5;241m/\u001b[39m SCALE\n\u001b[1;32m    539\u001b[0m )\n\u001b[1;32m    540\u001b[0m impulse_pos \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mposition[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m ox \u001b[38;5;241m-\u001b[39m tip[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m17\u001b[39m \u001b[38;5;241m/\u001b[39m SCALE,\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mposition[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m oy \u001b[38;5;241m+\u001b[39m tip[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_HEIGHT \u001b[38;5;241m/\u001b[39m SCALE,\n\u001b[1;32m    543\u001b[0m )\n\u001b[0;32m--> 544\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_particle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimpulse_pos\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimpulse_pos\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_power\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    545\u001b[0m p\u001b[38;5;241m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    546\u001b[0m     (ox \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power, oy \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power),\n\u001b[1;32m    547\u001b[0m     impulse_pos,\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    549\u001b[0m )\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    551\u001b[0m     (\u001b[38;5;241m-\u001b[39mox \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power, \u001b[38;5;241m-\u001b[39moy \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power),\n\u001b[1;32m    552\u001b[0m     impulse_pos,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    554\u001b[0m )\n",
      "File \u001b[0;32m/mnt/sobhan/cpi/lib/python3.12/site-packages/gym/envs/box2d/lunar_lander.py:422\u001b[0m, in \u001b[0;36mLunarLander._create_particle\u001b[0;34m(self, mass, x, y, ttl)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontinuous \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m], {}\n\u001b[0;32m--> 422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_particle\u001b[39m(\u001b[38;5;28mself\u001b[39m, mass, x, y, ttl):\n\u001b[1;32m    423\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld\u001b[38;5;241m.\u001b[39mCreateDynamicBody(\n\u001b[1;32m    424\u001b[0m         position\u001b[38;5;241m=\u001b[39m(x, y),\n\u001b[1;32m    425\u001b[0m         angle\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    433\u001b[0m         ),\n\u001b[1;32m    434\u001b[0m     )\n\u001b[1;32m    435\u001b[0m     p\u001b[38;5;241m.\u001b[39mttl \u001b[38;5;241m=\u001b[39m ttl\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gym\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from ppo import PPO\n",
    "from ddpg import DDPG\n",
    "from td import TD3\n",
    "import os\n",
    "\n",
    "env_name = \"LunarLanderContinuous-v2\"\n",
    "max_ep_len = 1000\n",
    "max_training_timesteps = 5e6\n",
    "\n",
    "print_freq = max_ep_len * 2\n",
    "log_freq = 10\n",
    "save_model_freq = int(1e5)\n",
    "\n",
    "action_std = None\n",
    "K_epochs = 200\n",
    "eps_clip = 0.1\n",
    "gamma = 0.99\n",
    "\n",
    "lr_actor = 0.0001  \n",
    "lr_critic = 0.0001 \n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "env.observation_space.shape[0]\n",
    "env.action_space.shape[0]\n",
    "float(env.action_space.high[0]) \n",
    "\n",
    "state_dim = 8\n",
    "action_dim = 2\n",
    "\n",
    "ddpg_agent = TD3(state_dim, action_dim, max_action=1)\n",
    "\n",
    "log_dir = \"PPO_logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_f_name = log_dir + '/' + env_name + \"_PPO_log_\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".csv\"\n",
    "\n",
    "print(f\"Training environment: {env_name}\")\n",
    "\n",
    "log_f = open(log_f_name, \"w+\")\n",
    "log_f.write('episode,timestep,reward\\n')\n",
    "\n",
    "start_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT):\", start_time)\n",
    "\n",
    "time_step = 0\n",
    "i_episode = 0\n",
    "\n",
    "\n",
    "# printing and logging variables\n",
    "print_running_reward = 0\n",
    "print_running_episodes = 0\n",
    "\n",
    "def evaluate_policy(env, agent, turns = 3):\n",
    "    total_scores = 0\n",
    "    for j in range(turns):\n",
    "        s, info = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Take deterministic actions at test time\n",
    "            a = agent.select_action(s, deterministic=True)\n",
    "            s_next, r, dw, tr, info = env.step(a)\n",
    "            done = (dw or tr)\n",
    "\n",
    "            total_scores += r\n",
    "            s = s_next\n",
    "    return int(total_scores/turns)\n",
    "\n",
    "while time_step <= max_training_timesteps:\n",
    "    state = env.reset()[0]\n",
    "    current_ep_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = ddpg_agent.select_action(state, False)\n",
    "\n",
    "        state_, reward, done, truncated, info = env.step(action)\n",
    "        done = (done or truncated)\n",
    "\n",
    "        reward = Reward_adapter(reward, 1)\n",
    "\n",
    "        ddpg_agent.buffer.store(state, action, reward, state_, done)\n",
    "\n",
    "        time_step += 1\n",
    "        current_ep_reward += reward\n",
    "\n",
    "        if time_step > 2000 and time_step % 50 == 0:\n",
    "            print(\"Updating the Model\")\n",
    "            ddpg_agent.update()\n",
    "\n",
    "        if time_step % log_freq == 0 and print_running_episodes > 0:\n",
    "            log_f.write(f'{i_episode},{time_step},{current_ep_reward}\\n')\n",
    "            # print average reward till last episode\n",
    "            print_avg_reward = print_running_reward / print_running_episodes\n",
    "            print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "            ddpg_agent.explore_noise *= 0.9998\n",
    "            ep_r = evaluate_policy(env, ddpg_agent, turns=3)\n",
    "\n",
    "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Reward: {} \\t\\t Average Reward : {}\".format(i_episode, time_step, ep_r, print_avg_reward))\n",
    "\n",
    "            print_running_reward = 0\n",
    "            print_running_episodes = 0\n",
    "\n",
    "        state = state_\n",
    "\n",
    "    print_running_reward += current_ep_reward\n",
    "    print_running_episodes += 1\n",
    "\n",
    "    i_episode += 1\n",
    "\n",
    "log_f.close()\n",
    "end_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Finished training at (GMT):\", end_time)\n",
    "print(\"Total training time:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just ignore this function~\n",
    "def str2bool(v):\n",
    "    '''transfer str to bool for argparse'''\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'True','true','TRUE', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'False','false','FALSE', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "#reward engineering for better training\n",
    "def Reward_adapter(r, EnvIdex):\n",
    "    # For Pendulum-v0\n",
    "    if EnvIdex == 0:\n",
    "        r = (r + 8) / 8\n",
    "\n",
    "    # For LunarLander\n",
    "    elif EnvIdex == 1:\n",
    "        if r <= -100: r = -10\n",
    "\n",
    "    # For BipedalWalker\n",
    "    elif EnvIdex == 4 or EnvIdex == 5:\n",
    "        if r <= -100: r = -1\n",
    "    return r\n",
    "\n",
    "def evaluate_policy(env, agent, turns = 3):\n",
    "    total_scores = 0\n",
    "    for j in range(turns):\n",
    "        s, info = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Take deterministic actions at test time\n",
    "            a = agent.select_action(s, deterministic=True)\n",
    "            s_next, r, dw, tr, info = env.step(a)\n",
    "            done = (dw or tr)\n",
    "\n",
    "            total_scores += r\n",
    "            s = s_next\n",
    "    return int(total_scores/turns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Namespace(dvc=device(type='cuda'), EnvIdex=1, write=False, render=False, Loadmodel=False, ModelIdex=30, seed=0, update_every=50, Max_train_steps=5000000, save_interval=100000, eval_interval=2000, delay_freq=1, gamma=0.99, net_width=256, a_lr=0.0001, c_lr=0.0001, batch_size=256, explore_noise=0.15, explore_noise_decay=0.998)\n",
      "Env:LunarLanderContinuous-v2  state_dim:8  action_dim:2  max_a:1.0  min_a:-1.0  max_e_steps:1000\n",
      "Random Seed: 0\n",
      "EnvName:LLdV2, Steps: 2k, Episode Reward:-1166\n",
      "EnvName:LLdV2, Steps: 4k, Episode Reward:-452\n",
      "EnvName:LLdV2, Steps: 6k, Episode Reward:-208\n",
      "EnvName:LLdV2, Steps: 8k, Episode Reward:-92\n",
      "EnvName:LLdV2, Steps: 10k, Episode Reward:-128\n",
      "EnvName:LLdV2, Steps: 12k, Episode Reward:-72\n",
      "EnvName:LLdV2, Steps: 14k, Episode Reward:19\n",
      "EnvName:LLdV2, Steps: 16k, Episode Reward:-85\n",
      "EnvName:LLdV2, Steps: 18k, Episode Reward:-17\n",
      "EnvName:LLdV2, Steps: 20k, Episode Reward:-32\n",
      "EnvName:LLdV2, Steps: 22k, Episode Reward:-46\n",
      "EnvName:LLdV2, Steps: 24k, Episode Reward:-32\n",
      "EnvName:LLdV2, Steps: 26k, Episode Reward:-204\n",
      "EnvName:LLdV2, Steps: 28k, Episode Reward:-273\n",
      "EnvName:LLdV2, Steps: 30k, Episode Reward:-36\n",
      "EnvName:LLdV2, Steps: 32k, Episode Reward:-174\n",
      "EnvName:LLdV2, Steps: 34k, Episode Reward:-80\n",
      "EnvName:LLdV2, Steps: 36k, Episode Reward:-4\n",
      "EnvName:LLdV2, Steps: 38k, Episode Reward:-126\n",
      "EnvName:LLdV2, Steps: 40k, Episode Reward:-83\n",
      "EnvName:LLdV2, Steps: 42k, Episode Reward:-61\n",
      "EnvName:LLdV2, Steps: 44k, Episode Reward:6\n",
      "EnvName:LLdV2, Steps: 46k, Episode Reward:-25\n",
      "EnvName:LLdV2, Steps: 48k, Episode Reward:46\n",
      "EnvName:LLdV2, Steps: 50k, Episode Reward:-9\n",
      "EnvName:LLdV2, Steps: 52k, Episode Reward:13\n",
      "EnvName:LLdV2, Steps: 54k, Episode Reward:1\n",
      "EnvName:LLdV2, Steps: 56k, Episode Reward:3\n",
      "EnvName:LLdV2, Steps: 58k, Episode Reward:0\n",
      "EnvName:LLdV2, Steps: 60k, Episode Reward:-14\n",
      "EnvName:LLdV2, Steps: 62k, Episode Reward:-9\n",
      "EnvName:LLdV2, Steps: 64k, Episode Reward:-8\n",
      "EnvName:LLdV2, Steps: 66k, Episode Reward:9\n",
      "EnvName:LLdV2, Steps: 68k, Episode Reward:-5\n",
      "EnvName:LLdV2, Steps: 70k, Episode Reward:-6\n",
      "EnvName:LLdV2, Steps: 72k, Episode Reward:8\n",
      "EnvName:LLdV2, Steps: 74k, Episode Reward:3\n",
      "EnvName:LLdV2, Steps: 76k, Episode Reward:0\n",
      "EnvName:LLdV2, Steps: 78k, Episode Reward:-13\n",
      "EnvName:LLdV2, Steps: 80k, Episode Reward:12\n",
      "EnvName:LLdV2, Steps: 82k, Episode Reward:-8\n",
      "EnvName:LLdV2, Steps: 84k, Episode Reward:4\n",
      "EnvName:LLdV2, Steps: 86k, Episode Reward:7\n",
      "EnvName:LLdV2, Steps: 88k, Episode Reward:18\n",
      "EnvName:LLdV2, Steps: 90k, Episode Reward:6\n",
      "EnvName:LLdV2, Steps: 92k, Episode Reward:-1\n",
      "EnvName:LLdV2, Steps: 94k, Episode Reward:1\n",
      "EnvName:LLdV2, Steps: 96k, Episode Reward:10\n",
      "EnvName:LLdV2, Steps: 98k, Episode Reward:-6\n",
      "EnvName:LLdV2, Steps: 100k, Episode Reward:6\n",
      "EnvName:LLdV2, Steps: 102k, Episode Reward:17\n",
      "EnvName:LLdV2, Steps: 104k, Episode Reward:-2\n",
      "EnvName:LLdV2, Steps: 106k, Episode Reward:2\n",
      "EnvName:LLdV2, Steps: 108k, Episode Reward:-2\n",
      "EnvName:LLdV2, Steps: 110k, Episode Reward:11\n",
      "EnvName:LLdV2, Steps: 112k, Episode Reward:8\n",
      "EnvName:LLdV2, Steps: 114k, Episode Reward:7\n",
      "EnvName:LLdV2, Steps: 116k, Episode Reward:-43\n",
      "EnvName:LLdV2, Steps: 118k, Episode Reward:-22\n",
      "EnvName:LLdV2, Steps: 120k, Episode Reward:-25\n",
      "EnvName:LLdV2, Steps: 122k, Episode Reward:-10\n",
      "EnvName:LLdV2, Steps: 124k, Episode Reward:-31\n",
      "EnvName:LLdV2, Steps: 126k, Episode Reward:6\n",
      "EnvName:LLdV2, Steps: 128k, Episode Reward:-46\n",
      "EnvName:LLdV2, Steps: 130k, Episode Reward:-29\n",
      "EnvName:LLdV2, Steps: 132k, Episode Reward:-59\n",
      "EnvName:LLdV2, Steps: 134k, Episode Reward:-13\n",
      "EnvName:LLdV2, Steps: 136k, Episode Reward:-12\n",
      "EnvName:LLdV2, Steps: 138k, Episode Reward:-2\n",
      "EnvName:LLdV2, Steps: 140k, Episode Reward:-15\n",
      "EnvName:LLdV2, Steps: 142k, Episode Reward:-2\n",
      "EnvName:LLdV2, Steps: 144k, Episode Reward:-7\n",
      "EnvName:LLdV2, Steps: 146k, Episode Reward:-3\n",
      "EnvName:LLdV2, Steps: 148k, Episode Reward:-8\n",
      "EnvName:LLdV2, Steps: 150k, Episode Reward:-16\n",
      "EnvName:LLdV2, Steps: 152k, Episode Reward:-49\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 121\u001b[0m\n\u001b[1;32m    117\u001b[0m         eval_env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 121\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 104\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# train 50 times every 50 steps rather than 1 training per step. Better!\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (total_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mopt\u001b[38;5;241m.\u001b[39mmax_e_steps) \u001b[38;5;129;01mand\u001b[39;00m (total_steps \u001b[38;5;241m%\u001b[39m opt\u001b[38;5;241m.\u001b[39mupdate_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 104\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''record & log'''\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_steps \u001b[38;5;241m%\u001b[39m opt\u001b[38;5;241m.\u001b[39meval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/mnt/sobhan/Beacons/td.py:156\u001b[0m, in \u001b[0;36mTD3.update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_critic_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    155\u001b[0m q_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_critic_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m''' Clipped Double Q-learning '''\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelay_counter \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelay_freq:\n\u001b[1;32m    160\u001b[0m \t\u001b[38;5;66;03m# Update the Actor\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/sobhan/cpi/lib/python3.12/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/sobhan/cpi/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/mnt/sobhan/cpi/lib/python3.12/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/mnt/sobhan/cpi/lib/python3.12/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/sobhan/cpi/lib/python3.12/site-packages/torch/optim/adam.py:509\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    506\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_lerp_(device_exp_avgs, device_grads, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m    508\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_mul_(device_exp_avg_sqs, beta2)\n\u001b[0;32m--> 509\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_addcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# Delete the local intermediate since it won't be used anymore to save on peak memory\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m device_grads\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gymnasium as gym\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import os, shutil\n",
    "import argparse\n",
    "import torch\n",
    "from td import TD3\n",
    "\n",
    "\n",
    "'''Hyperparameter Setting'''\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dvc', type=str, default='cuda', help='running device: cuda or cpu')\n",
    "parser.add_argument('--EnvIdex', type=int, default=1, help='PV1, Lch_Cv2, Humanv4, HCv4, BWv3, BWHv3')\n",
    "parser.add_argument('--write', type=str2bool, default=False, help='Use SummaryWriter to record the training')\n",
    "parser.add_argument('--render', type=str2bool, default=False, help='Render or Not')\n",
    "parser.add_argument('--Loadmodel', type=str2bool, default=False, help='Load pretrained model or Not')\n",
    "parser.add_argument('--ModelIdex', type=int, default=30, help='which model to load')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=0, help='random seed')\n",
    "parser.add_argument('--update_every', type=int, default=50, help='training frequency')\n",
    "parser.add_argument('--Max_train_steps', type=int, default=int(5e6), help='Max training steps')\n",
    "parser.add_argument('--save_interval', type=int, default=int(1e5), help='Model saving interval, in steps.')\n",
    "parser.add_argument('--eval_interval', type=int, default=int(2e3), help='Model evaluating interval, in steps.')\n",
    "\n",
    "parser.add_argument('--delay_freq', type=int, default=1, help='Delayed frequency for Actor and Target Net')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, help='Discounted Factor')\n",
    "parser.add_argument('--net_width', type=int, default=256, help='Hidden net width, s_dim-400-300-a_dim')\n",
    "parser.add_argument('--a_lr', type=float, default=1e-4, help='Learning rate of actor')\n",
    "parser.add_argument('--c_lr', type=float, default=1e-4, help='Learning rate of critic')\n",
    "parser.add_argument('--batch_size', type=int, default=256, help='batch_size of training')\n",
    "parser.add_argument('--explore_noise', type=float, default=0.15, help='exploring noise when interacting')\n",
    "parser.add_argument('--explore_noise_decay', type=float, default=0.998, help='Decay rate of explore noise')\n",
    "opt = parser.parse_args('')\n",
    "opt.dvc = torch.device(opt.dvc) # from str to torch.device\n",
    "print(opt)\n",
    "\n",
    "\n",
    "def main():\n",
    "    EnvName = ['Pendulum-v1','LunarLanderContinuous-v2','Humanoid-v4','HalfCheetah-v4','BipedalWalker-v3','BipedalWalkerHardcore-v3']\n",
    "    BrifEnvName = ['PV1', 'LLdV2', 'Humanv4', 'HCv4','BWv3', 'BWHv3']\n",
    "\n",
    "    # Build Env\n",
    "    env = gym.make(EnvName[opt.EnvIdex], render_mode = \"human\" if opt.render else None)\n",
    "    eval_env = gym.make(EnvName[opt.EnvIdex])\n",
    "    opt.state_dim = env.observation_space.shape[0]\n",
    "    opt.action_dim = env.action_space.shape[0]\n",
    "    opt.max_action = float(env.action_space.high[0])   #remark: action space-max,max\n",
    "    opt.max_e_steps = env._max_episode_steps\n",
    "    print(f'Env:{EnvName[opt.EnvIdex]}  state_dim:{opt.state_dim}  action_dim:{opt.action_dim}  '\n",
    "          f'max_a:{opt.max_action}  min_a:{env.action_space.low[0]}  max_e_steps:{opt.max_e_steps}')\n",
    "\n",
    "    # Seed Everything\n",
    "    env_seed = opt.seed\n",
    "    np.random.seed(opt.seed)\n",
    "    torch.manual_seed(opt.seed)\n",
    "    torch.cuda.manual_seed(opt.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(\"Random Seed: {}\".format(opt.seed))\n",
    "\n",
    "    # Build SummaryWriter to record training curves\n",
    "    if opt.write:\n",
    "        from torch.utils.tensorboard import SummaryWriter\n",
    "        timenow = str(datetime.now())[0:-10]\n",
    "        timenow = ' ' + timenow[0:13] + '_' + timenow[-2::]\n",
    "        writepath = 'runs/{}'.format(BrifEnvName[opt.EnvIdex]) + timenow\n",
    "        if os.path.exists(writepath): shutil.rmtree(writepath)\n",
    "        writer = SummaryWriter(log_dir=writepath)\n",
    "\n",
    "\n",
    "    # Build DRL model\n",
    "    if not os.path.exists('model'): os.mkdir('model')\n",
    "    agent = TD3(state_dim=opt.state_dim, action_dim=opt.action_dim, max_action=1) # var: transfer argparse to dictionary\n",
    "    if opt.Loadmodel: agent.load(BrifEnvName[opt.EnvIdex], opt.ModelIdex)\n",
    "\n",
    "    if opt.render:\n",
    "        while True:\n",
    "            score = evaluate_policy(env, agent, turns=1)\n",
    "            print('EnvName:', BrifEnvName[opt.EnvIdex], 'score:', score)\n",
    "    else:\n",
    "        total_steps = 0\n",
    "        while total_steps < opt.Max_train_steps:\n",
    "            s, info = env.reset(seed=env_seed)  # Do not use opt.seed directly, or it can overfit to opt.seed\n",
    "            env_seed += 1\n",
    "            done = False\n",
    "\n",
    "            '''Interact & trian'''\n",
    "            while not done:\n",
    "                if total_steps < (10*opt.max_e_steps): a = env.action_space.sample() # warm up\n",
    "                else: a = agent.select_action(s, deterministic=False)\n",
    "                s_next, r, dw, tr, info = env.step(a) # dw: dead&win; tr: truncated\n",
    "                # r = Reward_adapter(r, opt.EnvIdex)\n",
    "                done = (dw or tr)\n",
    "\n",
    "                agent.buffer.store(s, a, r, s_next, dw)\n",
    "                s = s_next\n",
    "                total_steps += 1\n",
    "\n",
    "                '''train if its time'''\n",
    "                # train 50 times every 50 steps rather than 1 training per step. Better!\n",
    "                if (total_steps >= 2*opt.max_e_steps) and (total_steps % opt.update_every == 0):\n",
    "                    agent.update()\n",
    "\n",
    "                '''record & log'''\n",
    "                if total_steps % opt.eval_interval == 0:\n",
    "                    agent.explore_noise *= opt.explore_noise_decay\n",
    "                    ep_r = evaluate_policy(eval_env, agent, turns=3)\n",
    "                    if opt.write: writer.add_scalar('ep_r', ep_r, global_step=total_steps)\n",
    "                    print(f'EnvName:{BrifEnvName[opt.EnvIdex]}, Steps: {int(total_steps/1000)}k, Episode Reward:{ep_r}')\n",
    "\n",
    "                '''save model'''\n",
    "                if total_steps % opt.save_interval == 0:\n",
    "                    agent.save(BrifEnvName[opt.EnvIdex], int(total_steps/1000))\n",
    "        env.close()\n",
    "        eval_env.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sobhan/cpi/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1000, Episode Reward:-133.70651539273302\n",
      "Steps: 2000, Episode Reward:-386.2711131134291\n",
      "Steps: 3000, Episode Reward:-1079.5909419748411\n",
      "Steps: 4000, Episode Reward:-531.8640370024275\n",
      "Steps: 5000, Episode Reward:-357.3779769169705\n",
      "Steps: 6000, Episode Reward:-393.44752598834987\n",
      "Steps: 7000, Episode Reward:-298.6332106530454\n",
      "Steps: 8000, Episode Reward:-390.49976887155395\n",
      "Steps: 9000, Episode Reward:-494.52058582956926\n",
      "Steps: 10000, Episode Reward:-71.08657570679081\n",
      "Steps: 11000, Episode Reward:72.63717296339482\n",
      "Steps: 12000, Episode Reward:-103.05285536330696\n",
      "Steps: 13000, Episode Reward:-69.8877710872083\n",
      "Steps: 14000, Episode Reward:-64.58550788672065\n",
      "Steps: 15000, Episode Reward:-24.684381887021196\n",
      "Steps: 16000, Episode Reward:-45.573699303399465\n",
      "Steps: 17000, Episode Reward:7.338005654791584\n",
      "Steps: 18000, Episode Reward:77.81901165201396\n",
      "Steps: 19000, Episode Reward:156.68480132240438\n",
      "Steps: 20000, Episode Reward:189.0323423219489\n",
      "Steps: 21000, Episode Reward:176.36793290757566\n",
      "Steps: 22000, Episode Reward:150.59185113511987\n",
      "Steps: 23000, Episode Reward:218.8721725749994\n",
      "Steps: 24000, Episode Reward:39.54187481417912\n",
      "Steps: 25000, Episode Reward:134.86900450665283\n",
      "Steps: 26000, Episode Reward:195.7338707550847\n",
      "Steps: 27000, Episode Reward:130.1743285215986\n",
      "Steps: 28000, Episode Reward:108.911790567164\n",
      "Steps: 29000, Episode Reward:183.34490893109734\n",
      "Steps: 30000, Episode Reward:130.36664708064603\n",
      "Steps: 31000, Episode Reward:97.76739184346005\n",
      "Steps: 32000, Episode Reward:57.37688431281806\n",
      "Steps: 33000, Episode Reward:69.4844192545527\n",
      "Steps: 34000, Episode Reward:79.51257959238015\n",
      "Steps: 35000, Episode Reward:116.56865925965785\n",
      "Steps: 36000, Episode Reward:0.9056728040831643\n",
      "Steps: 37000, Episode Reward:-7.3962281226597355\n",
      "Steps: 38000, Episode Reward:36.122823026660726\n",
      "Steps: 39000, Episode Reward:-70.43042394574988\n",
      "Steps: 40000, Episode Reward:-84.05096330801909\n",
      "Steps: 41000, Episode Reward:-6.291985261985007\n",
      "Steps: 42000, Episode Reward:30.66077072424871\n",
      "Steps: 43000, Episode Reward:33.44843727036789\n",
      "Steps: 44000, Episode Reward:145.97715828500355\n",
      "Steps: 45000, Episode Reward:-71.38425844201761\n",
      "Steps: 46000, Episode Reward:71.92991702787673\n",
      "Steps: 47000, Episode Reward:-67.58901161842228\n",
      "Steps: 48000, Episode Reward:-48.477502223807825\n",
      "Steps: 49000, Episode Reward:7.097049154994119\n",
      "Steps: 50000, Episode Reward:20.29561284815572\n",
      "Steps: 51000, Episode Reward:-6.574368924878897\n",
      "Steps: 52000, Episode Reward:4.297902018663228\n",
      "Steps: 53000, Episode Reward:59.5229503898139\n",
      "Steps: 54000, Episode Reward:-63.78659633288557\n",
      "Steps: 55000, Episode Reward:-21.29155282229236\n",
      "Steps: 56000, Episode Reward:-16.92055805489551\n",
      "Steps: 57000, Episode Reward:23.19487466734454\n",
      "Steps: 58000, Episode Reward:-130.3340921380647\n",
      "Steps: 59000, Episode Reward:-65.3892708727943\n",
      "Steps: 60000, Episode Reward:-124.91035548248564\n",
      "Steps: 61000, Episode Reward:-59.80486365331869\n",
      "Steps: 62000, Episode Reward:-71.31864630804108\n",
      "Steps: 63000, Episode Reward:-64.05221872845331\n",
      "Steps: 64000, Episode Reward:-86.53229844252274\n",
      "Steps: 65000, Episode Reward:-24.864949966754875\n",
      "Steps: 66000, Episode Reward:40.657432819579654\n",
      "Steps: 67000, Episode Reward:-71.85693210737043\n",
      "Steps: 68000, Episode Reward:122.40684063046561\n",
      "Steps: 69000, Episode Reward:-90.08904096165324\n",
      "Steps: 70000, Episode Reward:-17.922610218898214\n",
      "Steps: 71000, Episode Reward:-55.692045282062494\n",
      "Steps: 72000, Episode Reward:-87.03851918828859\n",
      "Steps: 73000, Episode Reward:-17.662553017907815\n",
      "Steps: 74000, Episode Reward:11.583460144139291\n",
      "Steps: 75000, Episode Reward:118.37841297221416\n",
      "Steps: 76000, Episode Reward:54.56527135236523\n",
      "Steps: 77000, Episode Reward:26.68198157363522\n",
      "Steps: 78000, Episode Reward:11.108707918393941\n",
      "Steps: 79000, Episode Reward:121.56116187749664\n",
      "Steps: 80000, Episode Reward:-19.10134895227956\n",
      "Steps: 81000, Episode Reward:-44.577465764475185\n",
      "Steps: 82000, Episode Reward:49.752418375476644\n",
      "Steps: 83000, Episode Reward:-7.702187049621384\n",
      "Steps: 84000, Episode Reward:49.49897472384451\n",
      "Steps: 85000, Episode Reward:-17.6718599493117\n",
      "Steps: 86000, Episode Reward:12.553738528748216\n",
      "Steps: 87000, Episode Reward:-2.2842318889064046\n",
      "Steps: 88000, Episode Reward:3.786670160319068\n",
      "Steps: 89000, Episode Reward:-6.391656344156232\n",
      "Steps: 90000, Episode Reward:-10.151120677065157\n",
      "Steps: 91000, Episode Reward:-74.46759649749704\n",
      "Steps: 92000, Episode Reward:-39.23245365379614\n",
      "Steps: 93000, Episode Reward:-70.49919473556137\n",
      "Steps: 94000, Episode Reward:-20.82720657071234\n",
      "Steps: 95000, Episode Reward:-66.91791175520102\n",
      "Steps: 96000, Episode Reward:-7.087951666141703\n",
      "Steps: 97000, Episode Reward:51.21035257256934\n",
      "Steps: 98000, Episode Reward:-78.54611562294387\n",
      "Steps: 99000, Episode Reward:-79.72416479400647\n",
      "Steps: 100000, Episode Reward:50.259497276968645\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from ppo import PPO\n",
    "from ddpg import DDPG\n",
    "from td import TD3\n",
    "import os\n",
    "\n",
    "\n",
    "env_name = \"LunarLanderContinuous-v2\"\n",
    "\n",
    "env = gym.make(env_name)\n",
    "eval_env = gym.make(env_name)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])   #remark: action space-max,max\n",
    "max_e_steps = env._max_episode_steps\n",
    "\n",
    "reproducibility(3)\n",
    "agent = TD3(state_dim=opt.state_dim, action_dim=opt.action_dim, max_action=1) \n",
    "\n",
    "total_steps = 0\n",
    "env_seed = 3\n",
    "\n",
    "\n",
    "while total_steps < 100000:\n",
    "\n",
    "    state, info = env.reset(seed=3)\n",
    "    env_seed += 1\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if total_steps < 10*max_e_steps: \n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = agent.select_action(state, deterministic=False)\n",
    "        \n",
    "        state_, reward, done, tr, info = env.step(action)\n",
    "        reward = Reward_adapter(reward, 1)\n",
    "        done = (done or tr)\n",
    "\n",
    "        agent.buffer.store(state, action, reward, state_, done)\n",
    "        state = state_\n",
    "        total_steps +=1\n",
    "\n",
    "        if (total_steps >= 2*max_e_steps) and (total_steps % 50 == 0):\n",
    "            agent.update()\n",
    "\n",
    "        if total_steps % 1000 == 0:\n",
    "            agent.explore_noise *= 0.988\n",
    "\n",
    "            total_scores = 0\n",
    "            for j in range(5):\n",
    "                s, info = eval_env.reset()\n",
    "                done = False\n",
    "                while not done:\n",
    "                    # Take deterministic actions at test time\n",
    "                    a = agent.select_action(s, deterministic=True)\n",
    "                    s_next, r, dw, tr, info = eval_env.step(a)\n",
    "                    done = (dw or tr)\n",
    "\n",
    "                    total_scores += r\n",
    "                    s = s_next\n",
    "            eval_s = total_scores / 5\n",
    "            print(f'Steps: {int(total_steps)}, Episode Reward:{eval_s}')\n",
    "\n",
    "    env.close()\n",
    "    eval_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
