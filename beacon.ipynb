{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to : NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import argparse\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from environment import BeaconEnv\n",
    "from ppo import PPO, RolloutBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to : NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "# set device to cpu or cuda\n",
    "device = torch.device('cpu')\n",
    "\n",
    "if(torch.cuda.is_available()):\n",
    "    device = torch.device('cuda:4')\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    print(\"Device set to : cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data='/mnt/kerem/CEU', epochs=300, episodes=100, lr=0.005, wd=0.0001, seed=3, model_name='PPO', gamma=0.99, latent1=256, latent2=256, control_size=20, gene_size=5, beacon_size=1270, victim_prob=0.8, pop_reset_freq=10, max_queries=10, state_dim=(4,), n_actions=1, resume='', save_dir='./results', results_dir='./results', device=device(type='cuda'))\n"
     ]
    }
   ],
   "source": [
    "# @title Arguments\n",
    "parser = argparse.ArgumentParser(description='Actor Critic')\n",
    "\n",
    "parser.add_argument('--data', default=\"/mnt/kerem/CEU\", type=str, help='Dataset Path')\n",
    "parser.add_argument('--epochs', default=300, type=int, metavar='N', help='Number of epochs for training agent.')\n",
    "parser.add_argument('--episodes', default=100, type=int, metavar='N', help='Number of episodes for training agent.')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.005, type=float, metavar='LR', help='initial learning rate', dest='lr')\n",
    "parser.add_argument('--wd', default=0.0001, type=float, help='Weight decay for training optimizer')\n",
    "parser.add_argument('--seed', default=3, type=int, help='Seed for reproducibility')\n",
    "parser.add_argument('--model-name', default=\"PPO\", type=str, help='Model name for saving model.')\n",
    "parser.add_argument('--gamma', default=0.99, type=float, metavar='N', help='The discount factor as mentioned in the previous section')\n",
    "\n",
    "# Model\n",
    "parser.add_argument(\"--latent1\", default=256, required=False, help=\"Latent Space Size for first layer of network.\")\n",
    "parser.add_argument(\"--latent2\", default=256, required=False, help=\"Latent Space Size for second layer of network.\")\n",
    "\n",
    "# Env Properties\n",
    "parser.add_argument('--control_size', default=20, type=int, help='Beacon and Attacker Control group size')\n",
    "parser.add_argument('--gene_size', default=5, type=int, help='States gene size')\n",
    "parser.add_argument('--beacon_size', default=1270, type=int, help='Beacon population size')\n",
    "parser.add_argument('--victim_prob', default=0.8, type=float, help='Victim inside beacon or not!')\n",
    "parser.add_argument('--pop_reset_freq', default=10, type=int, help='Reset Population Frequency (Epochs)')\n",
    "parser.add_argument('--max_queries', default=10, type=int, help='Maximum queries per episode')\n",
    "\n",
    "\n",
    "parser.add_argument(\"--state_dim\", default=(4,), required=False, help=\"State Dimension\")\n",
    "parser.add_argument(\"--n-actions\", default=1, required=False, help=\"Actions Count for each state\")\n",
    "\n",
    "\n",
    "# utils\n",
    "parser.add_argument('--resume', default=\"\", type=str, metavar='PATH', help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--save-dir', default='./results', type=str, metavar='PATH', help='path to cache (default: none)')\n",
    "\n",
    "# args = parser.parse_args()  # running in command line\n",
    "args = parser.parse_args('')  # running in ipynb\n",
    "\n",
    "# set command line arguments here when running in ipynb\n",
    "if args.save_dir == '':\n",
    "    args.save_dir = \"./\"\n",
    "\n",
    "args.results_dir = args.save_dir\n",
    "\n",
    "if not os.path.exists(args.results_dir):\n",
    "      os.makedirs(args.results_dir)\n",
    "\n",
    "args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â CEU Beacon - it contains 164 people in total which we will divide into groups to experiment\n",
    "beacon = pd.read_csv(os.path.join(args.data, \"Beacon_164.txt\"), index_col=0, delim_whitespace=True)\n",
    "# Reference genome, i.e. the genome that has no SNPs, all major allele pairs for each position\n",
    "reference = pickle.load(open(os.path.join(args.data, \"reference.pickle\"),\"rb\"))\n",
    "# Binary representation of the beacon; 0: no SNP (i.e. no mutation) 1: SNP (i.e. mutation)\n",
    "binary = np.logical_and(beacon.values != reference, beacon.values != \"NN\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table that contains MAF (minor allele frequency) values for each position. \n",
    "maf = pd.read_csv(os.path.join(args.data, \"MAF.txt\"), index_col=0, delim_whitespace=True)\n",
    "maf.rename(columns = {'referenceAllele':'major', 'referenceAlleleFrequency':'major_freq', \n",
    "                      'otherAllele':'minor', 'otherAlleleFrequency':'minor_freq'}, inplace = True)\n",
    "maf[\"maf\"] = np.round(maf[\"maf\"].values, 3)\n",
    "# Same variable with sorted maf values\n",
    "sorted_maf = maf.sort_values(by='maf')\n",
    "# Extracting column to an array for future use\n",
    "maf_values = maf[\"maf\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4029840, 164), (4029840, 1), (4029840, 164), (4029840,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beacon.shape, reference.shape, binary.shape, maf_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_continuous_action_space = True                \n",
    "max_training_timesteps = int(1e10)   # break training loop if timeteps > max_training_timesteps\n",
    "\n",
    "action_std = 0.4                    # starting std for action distribution (Multivariate Normal)\n",
    "action_std_decay_rate = 0.05        # linearly decay action_std (action_std = action_std - action_std_decay_rate)\n",
    "min_action_std = 0.1                # minimum action_std (stop decay after action_std <= min_action_std)\n",
    "action_std_decay_freq = int(2.5e5)\n",
    "\n",
    "################ PPO hyperparameters ################\n",
    "update_timestep = args.max_queries * 4      # update policy every n timesteps\n",
    "K_epochs = 1             # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003       # learning rate for actor network\n",
    "lr_critic = 0.001       # learning rate for critic network\n",
    "\n",
    "random_seed = 0         # set random seed if required (0 = no random seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Size of the population in too low!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m============================================================================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m################# training procedure ################\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mBeaconEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeacon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaf_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m state_dim \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mbeacon_size \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mgene_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m      6\u001b[0m action_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/mnt/sobhan/Beacons/environment.py:22\u001b[0m, in \u001b[0;36mBeaconEnv.__init__\u001b[0;34m(self, args, beacon, maf, binary)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary\u001b[38;5;241m=\u001b[39mbinary\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Randomly set populations and genes\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms_beacon, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms_control, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_control, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvictim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmafs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_populations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Initialize the agents\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/sobhan/Beacons/environment.py:122\u001b[0m, in \u001b[0;36mBeaconEnv.get_populations\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_populations\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mcontrol_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbeacon_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeacon\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 122\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize of the population in too low!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m# Prepare index arrays for future use\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     genes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeacon\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgene_size] \u001b[38;5;66;03m# Randomly select gene indexes\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: Size of the population in too low!"
     ]
    }
   ],
   "source": [
    "print(\"============================================================================================\")\n",
    "\n",
    "################# training procedure ################\n",
    "env = BeaconEnv(args, beacon, maf_values, binary)\n",
    "state_dim = args.beacon_size * args.gene_size * 4\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "# initialize a PPO agent\n",
    "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "\n",
    "\n",
    "# track total training time\n",
    "start_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "################### Logging ###################\n",
    "run_num = 0\n",
    "current_num_files = next(os.walk(args.results_dir))[2]\n",
    "run_num = len(current_num_files)\n",
    "log_f_name = args.results_dir + '/PPO_' + \"_log_\" + str(run_num) + \".csv\"\n",
    "\n",
    "print(\"current logging run number for \" + \" : \", run_num)\n",
    "print(\"logging at : \" + log_f_name)\n",
    "\n",
    "################### checkpointing ###################\n",
    "run_num_pretrained = 0\n",
    "directory = args.results_dir + \"/weights\"\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "checkpoint_path = directory + \"/PPO_{}_{}.pth\".format(random_seed, run_num_pretrained)\n",
    "print(\"save checkpoint path : \" + checkpoint_path)\n",
    "\n",
    "log_f = open(log_f_name,\"w+\")\n",
    "log_f.write('episode,timestep,reward\\n')\n",
    "\n",
    "\n",
    "# printing and logging variables\n",
    "print_running_reward = 0\n",
    "print_running_episodes = 0\n",
    "\n",
    "log_running_reward = 0\n",
    "log_running_episodes = 0\n",
    "\n",
    "time_step = 0\n",
    "i_episode = 0\n",
    "\n",
    "\n",
    "# training loop\n",
    "while time_step <= max_training_timesteps:\n",
    "\n",
    "    state = env.reset()[1]\n",
    "    # print(state.size())\n",
    "    current_ep_reward = 0\n",
    "\n",
    "    for t in range(1, args.max_queries+1):\n",
    "\n",
    "        # select action with policy\n",
    "        state = torch.flatten(state)\n",
    "        action = ppo_agent.select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # saving reward and is_terminals\n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "        time_step +=1\n",
    "        current_ep_reward += reward\n",
    "\n",
    "        # update PPO agent\n",
    "        if time_step % update_timestep == 0:\n",
    "            print(\"updating the agent\")\n",
    "            ppo_agent.update()\n",
    "\n",
    "            # log average reward till last episode\n",
    "            log_avg_reward = log_running_reward / log_running_episodes\n",
    "            log_avg_reward = log_avg_reward\n",
    "\n",
    "            log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
    "            log_f.flush()\n",
    "\n",
    "            log_running_reward = 0\n",
    "            log_running_episodes = 0\n",
    "\n",
    "            # print average reward till last episode\n",
    "            print_avg_reward = print_running_reward / print_running_episodes\n",
    "            print_avg_reward = print_avg_reward\n",
    "\n",
    "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
    "\n",
    "            print_running_reward = 0\n",
    "            print_running_episodes = 0\n",
    "\n",
    "            # save model weights\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"saving model at : \" + checkpoint_path)\n",
    "            ppo_agent.save(checkpoint_path)\n",
    "            print(\"model saved\")\n",
    "            print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "        # if continuous action space; then decay action std of ouput action distribution\n",
    "        if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
    "            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "\n",
    "        # break; if the episode is over\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print_running_reward += current_ep_reward\n",
    "    print_running_episodes += 1\n",
    "\n",
    "    log_running_reward += current_ep_reward\n",
    "    log_running_episodes += 1\n",
    "\n",
    "    i_episode += 1\n",
    "\n",
    "\n",
    "log_f.close()\n",
    "env.close()\n",
    "\n",
    "\n",
    "# print total training time\n",
    "print(\"============================================================================================\")\n",
    "end_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "print(\"Finished training at (GMT) : \", end_time)\n",
    "print(\"Total training time  : \", end_time - start_time)\n",
    "print(\"============================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
