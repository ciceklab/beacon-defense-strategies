{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import argparse\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to : NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "# set device to cpu or cuda\n",
    "device = torch.device('cpu')\n",
    "\n",
    "if(torch.cuda.is_available()):\n",
    "    device = torch.device('cuda:4')\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    print(\"Device set to : cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data='/mnt/kerem/CEU', epochs=300, episodes=100, lr=0.005, wd=0.0001, seed=3, model_name='PPO', gamma=0.99, latent1=256, latent2=256, control_size=20, gene_size=100, beacon_size=60, victim_prob=0.8, pop_reset_freq=10, max_queries=10, state_dim=(4,), n_actions=1, resume='', save_dir='./results', results_dir='./results', device=device(type='cuda'))\n"
     ]
    }
   ],
   "source": [
    "# @title Arguments\n",
    "parser = argparse.ArgumentParser(description='Actor Critic')\n",
    "\n",
    "parser.add_argument('--data', default=\"/mnt/kerem/CEU\", type=str, help='Dataset Path')\n",
    "parser.add_argument('--epochs', default=300, type=int, metavar='N', help='Number of epochs for training agent.')\n",
    "parser.add_argument('--episodes', default=100, type=int, metavar='N', help='Number of episodes for training agent.')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.005, type=float, metavar='LR', help='initial learning rate', dest='lr')\n",
    "parser.add_argument('--wd', default=0.0001, type=float, help='Weight decay for training optimizer')\n",
    "parser.add_argument('--seed', default=3, type=int, help='Seed for reproducibility')\n",
    "parser.add_argument('--model-name', default=\"PPO\", type=str, help='Model name for saving model.')\n",
    "parser.add_argument('--gamma', default=0.99, type=float, metavar='N', help='The discount factor as mentioned in the previous section')\n",
    "\n",
    "# Model\n",
    "parser.add_argument(\"--latent1\", default=256, required=False, help=\"Latent Space Size for first layer of network.\")\n",
    "parser.add_argument(\"--latent2\", default=256, required=False, help=\"Latent Space Size for second layer of network.\")\n",
    "\n",
    "# Env Properties\n",
    "parser.add_argument('--control_size', default=20, type=int, help='Beacon and Attacker Control group size')\n",
    "parser.add_argument('--gene_size', default=100, type=int, help='States gene size')\n",
    "parser.add_argument('--beacon_size', default=60, type=int, help='Beacon population size')\n",
    "parser.add_argument('--victim_prob', default=0.8, type=float, help='Victim inside beacon or not!')\n",
    "parser.add_argument('--pop_reset_freq', default=10, type=int, help='Reset Population Frequency (Epochs)')\n",
    "parser.add_argument('--max_queries', default=10, type=int, help='Maximum queries per episode')\n",
    "\n",
    "\n",
    "parser.add_argument(\"--state_dim\", default=(4,), required=False, help=\"State Dimension\")\n",
    "parser.add_argument(\"--n-actions\", default=1, required=False, help=\"Actions Count for each state\")\n",
    "\n",
    "\n",
    "# utils\n",
    "parser.add_argument('--resume', default=\"\", type=str, metavar='PATH', help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--save-dir', default='./results', type=str, metavar='PATH', help='path to cache (default: none)')\n",
    "\n",
    "# args = parser.parse_args()  # running in command line\n",
    "args = parser.parse_args('')  # running in ipynb\n",
    "\n",
    "# set command line arguments here when running in ipynb\n",
    "if args.save_dir == '':\n",
    "    args.save_dir = \"./\"\n",
    "\n",
    "args.results_dir = args.save_dir\n",
    "\n",
    "if not os.path.exists(args.results_dir):\n",
    "      os.makedirs(args.results_dir)\n",
    "\n",
    "args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â CEU Beacon - it contains 164 people in total which we will divide into groups to experiment\n",
    "beacon = pd.read_csv(os.path.join(args.data, \"Beacon_164.txt\"), index_col=0, delim_whitespace=True)\n",
    "# Reference genome, i.e. the genome that has no SNPs, all major allele pairs for each position\n",
    "reference = pickle.load(open(os.path.join(args.data, \"reference.pickle\"),\"rb\"))\n",
    "# Binary representation of the beacon; 0: no SNP (i.e. no mutation) 1: SNP (i.e. mutation)\n",
    "binary = np.logical_and(beacon.values != reference, beacon.values != \"NN\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table that contains MAF (minor allele frequency) values for each position. \n",
    "maf = pd.read_csv(os.path.join(args.data, \"MAF.txt\"), index_col=0, delim_whitespace=True)\n",
    "maf.rename(columns = {'referenceAllele':'major', 'referenceAlleleFrequency':'major_freq', \n",
    "                      'otherAllele':'minor', 'otherAlleleFrequency':'minor_freq'}, inplace = True)\n",
    "maf[\"maf\"] = np.round(maf[\"maf\"].values, 3)\n",
    "# Same variable with sorted maf values\n",
    "sorted_maf = maf.sort_values(by='maf')\n",
    "# Extracting column to an array for future use\n",
    "maf_values = maf[\"maf\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4029840, 164), (4029840, 1), (4029840, 164), (4029840,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beacon.shape, reference.shape, binary.shape, maf_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_continuous_action_space = True                \n",
    "max_training_timesteps = int(1e10)   # break training loop if timeteps > max_training_timesteps\n",
    "\n",
    "action_std = 0.4                    # starting std for action distribution (Multivariate Normal)\n",
    "action_std_decay_rate = 0.05        # linearly decay action_std (action_std = action_std - action_std_decay_rate)\n",
    "min_action_std = 0.1                # minimum action_std (stop decay after action_std <= min_action_std)\n",
    "action_std_decay_freq = int(2.5e5)\n",
    "\n",
    "################ PPO hyperparameters ################\n",
    "update_timestep = args.max_queries * 4      # update policy every n timesteps\n",
    "K_epochs = 64           # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003       # learning rate for actor network\n",
    "lr_critic = 0.001       # learning rate for critic network\n",
    "\n",
    "random_seed = 0         # set random seed if required (0 = no random seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "============================================================================================\n",
      "Victim is NOT inside the Beacon!\n",
      "Started training at (GMT) :  2024-04-13 09:22:30\n",
      "============================================================================================\n",
      "current logging run number for  :  3\n",
      "logging at : ./results/PPO__log_3.csv\n",
      "save checkpoint path : ./results/weights/PPO_0_0.pth\n",
      "updating the agent\n",
      "Episode : 3 \t\t Timestep : 40 \t\t Average Reward : tensor([2.5656])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:02\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 7 \t\t Timestep : 80 \t\t Average Reward : tensor([11.0061])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:03\n",
      "--------------------------------------------------------------------------------------------\n",
      "Reseting the Populations\n",
      "Victim is NOT inside the Beacon!\n",
      "updating the agent\n",
      "Episode : 11 \t\t Timestep : 120 \t\t Average Reward : tensor([14.8444])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:05\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 15 \t\t Timestep : 160 \t\t Average Reward : tensor([51.9349])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:06\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 19 \t\t Timestep : 200 \t\t Average Reward : tensor([171.5287])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:08\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 23 \t\t Timestep : 240 \t\t Average Reward : tensor([301.6535])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:09\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 27 \t\t Timestep : 280 \t\t Average Reward : tensor([403.5572])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:10\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 31 \t\t Timestep : 320 \t\t Average Reward : tensor([474.7986])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:11\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 35 \t\t Timestep : 360 \t\t Average Reward : tensor([622.2997])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:12\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 39 \t\t Timestep : 400 \t\t Average Reward : tensor([756.7789])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:14\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 43 \t\t Timestep : 440 \t\t Average Reward : tensor([879.9988])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:15\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 47 \t\t Timestep : 480 \t\t Average Reward : tensor([1010.2454])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:16\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 51 \t\t Timestep : 520 \t\t Average Reward : tensor([1208.8042])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:17\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 55 \t\t Timestep : 560 \t\t Average Reward : tensor([1443.1178])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:18\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 59 \t\t Timestep : 600 \t\t Average Reward : tensor([1683.7991])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:19\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 63 \t\t Timestep : 640 \t\t Average Reward : tensor([1855.6843])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:20\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 67 \t\t Timestep : 680 \t\t Average Reward : tensor([2053.0347])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:20\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 71 \t\t Timestep : 720 \t\t Average Reward : tensor([2229.7148])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:21\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 75 \t\t Timestep : 760 \t\t Average Reward : tensor([2411.3943])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:22\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 79 \t\t Timestep : 800 \t\t Average Reward : tensor([2677.9890])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:23\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 83 \t\t Timestep : 840 \t\t Average Reward : tensor([2902.6724])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:24\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 87 \t\t Timestep : 880 \t\t Average Reward : tensor([3083.8877])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:25\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 91 \t\t Timestep : 920 \t\t Average Reward : tensor([3371.4856])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:27\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 95 \t\t Timestep : 960 \t\t Average Reward : tensor([3648.6560])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:28\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 99 \t\t Timestep : 1000 \t\t Average Reward : tensor([3946.6968])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:30\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 103 \t\t Timestep : 1040 \t\t Average Reward : tensor([4299.4209])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:32\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 107 \t\t Timestep : 1080 \t\t Average Reward : tensor([4574.9043])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:33\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 111 \t\t Timestep : 1120 \t\t Average Reward : tensor([4832.1797])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:34\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 115 \t\t Timestep : 1160 \t\t Average Reward : tensor([5154.1982])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:35\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 119 \t\t Timestep : 1200 \t\t Average Reward : tensor([5550.6436])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:36\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 123 \t\t Timestep : 1240 \t\t Average Reward : tensor([5958.3853])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:38\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 127 \t\t Timestep : 1280 \t\t Average Reward : tensor([6337.4502])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:39\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 131 \t\t Timestep : 1320 \t\t Average Reward : tensor([6726.1753])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:40\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 135 \t\t Timestep : 1360 \t\t Average Reward : tensor([7124.9302])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:41\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 139 \t\t Timestep : 1400 \t\t Average Reward : tensor([7502.7031])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:42\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 143 \t\t Timestep : 1440 \t\t Average Reward : tensor([7871.1660])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:43\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 147 \t\t Timestep : 1480 \t\t Average Reward : tensor([8220.9160])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:45\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 151 \t\t Timestep : 1520 \t\t Average Reward : tensor([8585.1025])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:46\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 155 \t\t Timestep : 1560 \t\t Average Reward : tensor([8872.9082])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:47\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 159 \t\t Timestep : 1600 \t\t Average Reward : tensor([9189.7109])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:48\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 163 \t\t Timestep : 1640 \t\t Average Reward : tensor([9473.4072])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:50\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 167 \t\t Timestep : 1680 \t\t Average Reward : tensor([9747.1787])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:50\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 171 \t\t Timestep : 1720 \t\t Average Reward : tensor([9983.0479])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:51\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 175 \t\t Timestep : 1760 \t\t Average Reward : tensor([10235.3379])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:52\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 179 \t\t Timestep : 1800 \t\t Average Reward : tensor([10516.8340])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:53\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 183 \t\t Timestep : 1840 \t\t Average Reward : tensor([10768.7305])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:54\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 187 \t\t Timestep : 1880 \t\t Average Reward : tensor([11041.3945])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:55\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 191 \t\t Timestep : 1920 \t\t Average Reward : tensor([11340.5029])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:56\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 195 \t\t Timestep : 1960 \t\t Average Reward : tensor([11545.3320])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:57\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 199 \t\t Timestep : 2000 \t\t Average Reward : tensor([11736.4121])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:59\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 203 \t\t Timestep : 2040 \t\t Average Reward : tensor([11985.5127])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:00\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 207 \t\t Timestep : 2080 \t\t Average Reward : tensor([12235.9492])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:01\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 211 \t\t Timestep : 2120 \t\t Average Reward : tensor([12438.4600])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:02\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 215 \t\t Timestep : 2160 \t\t Average Reward : tensor([12557.8496])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:03\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 219 \t\t Timestep : 2200 \t\t Average Reward : tensor([12751.1172])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:04\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 223 \t\t Timestep : 2240 \t\t Average Reward : tensor([12890.8975])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:05\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 227 \t\t Timestep : 2280 \t\t Average Reward : tensor([13015.4590])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:07\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 231 \t\t Timestep : 2320 \t\t Average Reward : tensor([13141.0752])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:08\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 235 \t\t Timestep : 2360 \t\t Average Reward : tensor([13324.8145])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:11\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 239 \t\t Timestep : 2400 \t\t Average Reward : tensor([13496.3711])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:12\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 243 \t\t Timestep : 2440 \t\t Average Reward : tensor([13612.0723])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:14\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 247 \t\t Timestep : 2480 \t\t Average Reward : tensor([13728.2842])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:15\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 251 \t\t Timestep : 2520 \t\t Average Reward : tensor([13930.8027])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:16\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 255 \t\t Timestep : 2560 \t\t Average Reward : tensor([14134.1914])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:17\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 259 \t\t Timestep : 2600 \t\t Average Reward : tensor([14299.7949])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:18\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 263 \t\t Timestep : 2640 \t\t Average Reward : tensor([14507.9287])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:20\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 267 \t\t Timestep : 2680 \t\t Average Reward : tensor([14753.1035])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:21\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 271 \t\t Timestep : 2720 \t\t Average Reward : tensor([14895.6211])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:22\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 275 \t\t Timestep : 2760 \t\t Average Reward : tensor([14939.1436])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:23\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 279 \t\t Timestep : 2800 \t\t Average Reward : tensor([15057.8809])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:25\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 283 \t\t Timestep : 2840 \t\t Average Reward : tensor([15224.6123])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:26\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 287 \t\t Timestep : 2880 \t\t Average Reward : tensor([15394.8271])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:27\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 291 \t\t Timestep : 2920 \t\t Average Reward : tensor([15463.5537])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:28\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 295 \t\t Timestep : 2960 \t\t Average Reward : tensor([15480.9980])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:29\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 299 \t\t Timestep : 3000 \t\t Average Reward : tensor([15678.7480])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:30\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 303 \t\t Timestep : 3040 \t\t Average Reward : tensor([15847.4434])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:31\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 307 \t\t Timestep : 3080 \t\t Average Reward : tensor([15981.8447])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:32\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 311 \t\t Timestep : 3120 \t\t Average Reward : tensor([16040.5361])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:33\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 315 \t\t Timestep : 3160 \t\t Average Reward : tensor([16129.0498])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:34\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 319 \t\t Timestep : 3200 \t\t Average Reward : tensor([16289.0674])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:35\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 323 \t\t Timestep : 3240 \t\t Average Reward : tensor([16362.2197])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:36\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 327 \t\t Timestep : 3280 \t\t Average Reward : tensor([16334.9785])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:37\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 331 \t\t Timestep : 3320 \t\t Average Reward : tensor([16354.3340])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:38\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 335 \t\t Timestep : 3360 \t\t Average Reward : tensor([16410.7012])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:38\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 339 \t\t Timestep : 3400 \t\t Average Reward : tensor([16560.4023])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:39\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 343 \t\t Timestep : 3440 \t\t Average Reward : tensor([16694.4961])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:40\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 347 \t\t Timestep : 3480 \t\t Average Reward : tensor([16877.4688])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:42\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 351 \t\t Timestep : 3520 \t\t Average Reward : tensor([17093.9590])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:44\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 355 \t\t Timestep : 3560 \t\t Average Reward : tensor([17358.9336])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:45\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 359 \t\t Timestep : 3600 \t\t Average Reward : tensor([17629.1465])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:46\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 363 \t\t Timestep : 3640 \t\t Average Reward : tensor([17933.8945])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:47\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 367 \t\t Timestep : 3680 \t\t Average Reward : tensor([18245.9023])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:48\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 371 \t\t Timestep : 3720 \t\t Average Reward : tensor([18538.5000])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:49\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 375 \t\t Timestep : 3760 \t\t Average Reward : tensor([18769.1641])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:51\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 379 \t\t Timestep : 3800 \t\t Average Reward : tensor([18972.9512])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:52\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 383 \t\t Timestep : 3840 \t\t Average Reward : tensor([19158.9531])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:53\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 387 \t\t Timestep : 3880 \t\t Average Reward : tensor([19377.3906])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:54\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 391 \t\t Timestep : 3920 \t\t Average Reward : tensor([19605.9102])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:55\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 395 \t\t Timestep : 3960 \t\t Average Reward : tensor([19894.4648])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:57\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 399 \t\t Timestep : 4000 \t\t Average Reward : tensor([20127.0859])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:58\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 403 \t\t Timestep : 4040 \t\t Average Reward : tensor([20365.4707])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:58\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 407 \t\t Timestep : 4080 \t\t Average Reward : tensor([20500.0078])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:59\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 411 \t\t Timestep : 4120 \t\t Average Reward : tensor([20627.3594])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:02:00\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 415 \t\t Timestep : 4160 \t\t Average Reward : tensor([20774.7910])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:02:01\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 419 \t\t Timestep : 4200 \t\t Average Reward : tensor([20914.5898])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:02:03\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 423 \t\t Timestep : 4240 \t\t Average Reward : tensor([21041.3203])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:02:04\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 427 \t\t Timestep : 4280 \t\t Average Reward : tensor([21204.4609])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:02:05\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 431 \t\t Timestep : 4320 \t\t Average Reward : tensor([21414.1406])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:02:06\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 435 \t\t Timestep : 4360 \t\t Average Reward : tensor([21520.9629])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:02:08\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 439 \t\t Timestep : 4400 \t\t Average Reward : tensor([21688.7773])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:02:09\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 443 \t\t Timestep : 4440 \t\t Average Reward : tensor([21918.4453])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:02:10\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 447 \t\t Timestep : 4480 \t\t Average Reward : tensor([22104.8477])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:02:11\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 451 \t\t Timestep : 4520 \t\t Average Reward : tensor([22221.0547])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:02:13\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n",
      "Episode : 455 \t\t Timestep : 4560 \t\t Average Reward : tensor([22315.0254])\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : ./results/weights/PPO_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:02:14\n",
      "--------------------------------------------------------------------------------------------\n",
      "updating the agent\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (40, 1)) of distribution MultivariateNormal(loc: torch.Size([40, 1]), covariance_matrix: torch.Size([40, 1, 1])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([[nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan]], device='cuda:4', grad_fn=<ExpandBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 81\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_step \u001b[38;5;241m%\u001b[39m update_timestep \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdating the agent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 81\u001b[0m     \u001b[43mppo_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# log average reward till last episode\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     log_avg_reward \u001b[38;5;241m=\u001b[39m log_running_reward \u001b[38;5;241m/\u001b[39m log_running_episodes\n",
      "File \u001b[0;32m/mnt/sobhan/Beacons/ppo.py:253\u001b[0m, in \u001b[0;36mPPO.update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Optimize policy for K epochs\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK_epochs):\n\u001b[1;32m    251\u001b[0m \n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# Evaluating old actions and values\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m     logprobs, state_values, dist_entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# match state_values tensor dimensions with rewards tensor\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     state_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(state_values)\n",
      "File \u001b[0;32m/mnt/sobhan/Beacons/ppo.py:121\u001b[0m, in \u001b[0;36mActorCritic.evaluate\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m    119\u001b[0m cov_mat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag_embed(action_var)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# print(\"action_mean: \", action_mean, \"cov_mat: \", cov_mat)\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[43mMultivariateNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov_mat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# for single action continuous environments\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/mnt/sobhan/cpi/lib/python3.12/site-packages/torch/distributions/multivariate_normal.py:177\u001b[0m, in \u001b[0;36mMultivariateNormal.__init__\u001b[0;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc \u001b[38;5;241m=\u001b[39m loc\u001b[38;5;241m.\u001b[39mexpand(batch_shape \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,))\n\u001b[1;32m    176\u001b[0m event_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scale_tril \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbroadcasted_scale_tril \u001b[38;5;241m=\u001b[39m scale_tril\n",
      "File \u001b[0;32m/mnt/sobhan/cpi/lib/python3.12/site-packages/torch/distributions/distribution.py:68\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     66\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m---> 68\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m             )\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (40, 1)) of distribution MultivariateNormal(loc: torch.Size([40, 1]), covariance_matrix: torch.Size([40, 1, 1])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([[nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan]], device='cuda:4', grad_fn=<ExpandBackward0>)"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from environment import BeaconEnv\n",
    "from ppo import PPO\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "################# training procedure ################\n",
    "env = BeaconEnv(args, beacon, maf_values, binary)\n",
    "state_dim = args.beacon_size * args.gene_size * 4\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "# initialize a PPO agent\n",
    "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "\n",
    "\n",
    "# track total training time\n",
    "start_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "################### Logging ###################\n",
    "run_num = 0\n",
    "current_num_files = next(os.walk(args.results_dir))[2]\n",
    "run_num = len(current_num_files)\n",
    "log_f_name = args.results_dir + '/PPO_' + \"_log_\" + str(run_num) + \".csv\"\n",
    "\n",
    "print(\"current logging run number for \" + \" : \", run_num)\n",
    "print(\"logging at : \" + log_f_name)\n",
    "\n",
    "################### checkpointing ###################\n",
    "run_num_pretrained = 0\n",
    "directory = args.results_dir + \"/weights\"\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "checkpoint_path = directory + \"/PPO_{}_{}.pth\".format(random_seed, run_num_pretrained)\n",
    "print(\"save checkpoint path : \" + checkpoint_path)\n",
    "\n",
    "log_f = open(log_f_name,\"w+\")\n",
    "log_f.write('episode,timestep,reward\\n')\n",
    "\n",
    "\n",
    "# printing and logging variables\n",
    "print_running_reward = 0\n",
    "print_running_episodes = 0\n",
    "\n",
    "log_running_reward = 0\n",
    "log_running_episodes = 0\n",
    "\n",
    "time_step = 0\n",
    "i_episode = 0\n",
    "\n",
    "\n",
    "# training loop\n",
    "while time_step <= max_training_timesteps:\n",
    "\n",
    "    state = env.reset()[1]\n",
    "    # print(state.size())\n",
    "    current_ep_reward = 0\n",
    "\n",
    "    for t in range(1, args.max_queries+1):\n",
    "\n",
    "        # select action with policy\n",
    "        state = torch.flatten(state)\n",
    "        action = ppo_agent.select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # saving reward and is_terminals\n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "        time_step +=1\n",
    "        current_ep_reward += reward\n",
    "\n",
    "        # update PPO agent\n",
    "        if time_step % update_timestep == 0:\n",
    "            print(\"updating the agent\")\n",
    "            ppo_agent.update()\n",
    "\n",
    "            # log average reward till last episode\n",
    "            log_avg_reward = log_running_reward / log_running_episodes\n",
    "            log_avg_reward = log_avg_reward\n",
    "\n",
    "            log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
    "            log_f.flush()\n",
    "\n",
    "            log_running_reward = 0\n",
    "            log_running_episodes = 0\n",
    "\n",
    "            # print average reward till last episode\n",
    "            print_avg_reward = print_running_reward / print_running_episodes\n",
    "            print_avg_reward = print_avg_reward\n",
    "\n",
    "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
    "\n",
    "            print_running_reward = 0\n",
    "            print_running_episodes = 0\n",
    "\n",
    "            # save model weights\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"saving model at : \" + checkpoint_path)\n",
    "            ppo_agent.save(checkpoint_path)\n",
    "            print(\"model saved\")\n",
    "            print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "        # if continuous action space; then decay action std of ouput action distribution\n",
    "        if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
    "            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "\n",
    "        # break; if the episode is over\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print_running_reward += current_ep_reward\n",
    "    print_running_episodes += 1\n",
    "\n",
    "    log_running_reward += current_ep_reward\n",
    "    log_running_episodes += 1\n",
    "\n",
    "    i_episode += 1\n",
    "\n",
    "\n",
    "log_f.close()\n",
    "env.close()\n",
    "\n",
    "\n",
    "# print total training time\n",
    "print(\"============================================================================================\")\n",
    "end_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "print(\"Finished training at (GMT) : \", end_time)\n",
    "print(\"Total training time  : \", end_time - start_time)\n",
    "print(\"============================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
