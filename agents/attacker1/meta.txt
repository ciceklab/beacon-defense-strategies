parser.add_argument('--data', default="/mnt/kerem/CEU", type=str, help='Dataset Path')
parser.add_argument('--epochs', default=64, type=int, metavar='N', help='Number of epochs for training agent.')
parser.add_argument('--episodes', default=10000, type=int, metavar='N', help='Number of episodes for training agent.')
parser.add_argument('--lr', '--learning-rate', default=0.005, type=float, metavar='LR', help='initial learning rate', dest='lr')
parser.add_argument('--wd', default=0.0001, type=float, help='Weight decay for training optimizer')
parser.add_argument('--seed', default=3, type=int, help='Seed for reproducibility')
parser.add_argument('--model-name', default="PPO", type=str, help='Model name for saving model.')
parser.add_argument('--gamma', default=0.99, type=float, metavar='N', help='The discount factor as mentioned in the previous section')

# Model
parser.add_argument("--latent1", default=256, required=False, help="Latent Space Size for first layer of network.")
parser.add_argument("--latent2", default=256, required=False, help="Latent Space Size for second layer of network.")

# Env Properties
parser.add_argument('--a_control_size', default=50, type=int, help='Attack Control group size')
parser.add_argument('--b_control_size', default=50, type=int, help='Beacon Control group size')
parser.add_argument('--gene_size', default=100, type=int, help='States gene size')
parser.add_argument('--beacon_size', default=10, type=int, help='Beacon population size')
parser.add_argument('--victim_prob', default=1, type=float, help='Victim inside beacon or not!')
parser.add_argument('--max_queries', default=5, type=int, help='Maximum queries per episode')


parser.add_argument('--attacker_type', default="agent", choices=["random", "optimal", "agent"], type=str, help='Type of the attacker')
parser.add_argument('--beacon_type', default="truth", choices=["random", "agent", "truth"], type=str, help='Type of the beacon')


parser.add_argument('--pop_reset_freq', default=100000000, type=int, help='Reset Population Frequency (Epochs)')
parser.add_argument('--update_freq', default=20, type=int, help='Train Agent model frequency')
parser.add_argument('--plot-freq', default=10, type=int, metavar='N', help='Plot Frequencies')
parser.add_argument('--val-freq', default=20, type=int, metavar='N', help='Validation frequencies')
parser.add_argument('--control-lrts', default=None, type=str, help='Control groups LRTS path')



parser.add_argument("--state_dim", default=(4,), required=False, help="State Dimension")
parser.add_argument("--n-actions", default=1, required=False, help="Actions Count for each state")


# utils
parser.add_argument('--resume', default="", type=str, metavar='PATH', help='path to latest checkpoint (default: none)')
parser.add_argument('--results-dir', default='./results/train', type=str, metavar='PATH', help='path to cache (default: none)')

def calc_reward(self, beacon_action, step)->float:
    preward1 = -1
    if pvalue < 0.05:
        total += 40

    duplicate -= 6

def get_state(self)->float:
    return self.victim_info




attacker_state_dim = 400
attacker_action_dim = 10

################ PPO hyperparameters ################
K_epochs = 300         # update policy for K epochs
eps_clip = 0.2           # clip parameter for PPO
gamma = 0.99                # discount factor

lr_actor = 0.001       # learning rate for actor network
lr_critic = 0.001        # learning rate for critic network